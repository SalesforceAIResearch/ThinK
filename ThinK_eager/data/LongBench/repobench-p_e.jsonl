{"input": "import waterwave.net.aio.define.AioServerDataDealer;\r\nimport waterwave.proxy.router.ProxyRouter;\r\nimport java.io.IOException;\r\nimport java.net.SocketAddress;\r\nimport java.nio.ByteBuffer;\r\nimport java.nio.channels.WritePendingException;\r\nimport java.util.LinkedList;\r\nimport shui.common.buffer.BufferTools;\r\nimport shui.common.log.Logger;\r\nimport shui.common.service.ThreadSharedService;\r\nimport shui.common.util.CommonUtil;\r\nimport waterwave.net.aio.AioClient;\r\nimport waterwave.net.aio.AioClientChannel;\r\nimport waterwave.net.aio.AioServerChannel;\r\nimport waterwave.net.aio.define.AioClientDataDealer;\r\n/*\r\n * Licensed to waterwave under one or more contributor\r\n *  \r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *  \r\n *      http://www.apache.org/licenses/LICENSE-2.0\r\n *  \r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n\r\npackage waterwave.proxy.aio;\r\n\r\n\r\n\r\n/**\r\n * \r\n * @author vv\r\n * \r\n * \r\n *         server:in -> queue \r\n *         client:in -> queue\r\n * \r\n *\r\n */\r\npublic class ProxyAioDataDealer extends ThreadSharedService implements AioServerDataDealer, AioClientDataDealer {\r\n\r\n\tAioClientChannel cc;\r\n\tAioServerChannel sc;\r\n\r\n\tAioClient client;\r\n\tSocketAddress remote;\r\n\r\n\tprivate boolean clientIniting;\r\n\r\n\tpublic ProxyAioDataDealer() {\r\n\r\n\t}\r\n\t\r\n\tfinal static boolean debug = false;\r\n\t\r\n\r\n\tpublic AioClient getClient() {\r\n\t\treturn client;\r\n\t}\r\n\tpublic void setClient(AioClient client) {\r\n\t\tthis.client = client;\r\n\t}\r\n\r\n\r\n\r\n\tpublic static void main(String[] args) {\r\n\t\t// TODO Auto-generated method stub\r\n\r\n\t}\r\n\t\r\n\r\n\r\n\t@Override\r\n\tpublic void serverOnConnect(AioServerChannel channel) {\r\n\t\tthis.sc = channel;\r\n\t\t\r\n\t\tclientIniting = true;\r\n\t\ttry {\r\n\t\t\tremote = ProxyRouter.getStaticRemote();\r\n\t\t\tclient.connect(remote, this);\r\n\t\t} catch (IOException e) {\r\n\t\t\tthis.clientOnError(null, e, null);\r\n\t\t\te.printStackTrace();\r\n\t\t}\r\n\t}\r\n\r\n\t@Override\r\n\tpublic void serverBeforeRead(AioServerChannel channel) {\r\n\t}\r\n\r\n\t@Override\r\n\tpublic void serverOnData(AioServerChannel channel, ByteBuffer b, int bytes) {\r\n\t\t//log.log(1, \"serverOnData...\", b);\r\n\t\t\r\n\t\t//TODO\r\n\t\t//Logger.log(new String(BufferTools.getBuffer2Byte(b)));\r\n\t\t\r\n\t\t//init client\r\n\t\tif (cc == null) {\r\n\t\t\tif (clientIniting) {\r\n\t\t\t\tlog.log(9, \"serverOnData clientIniting long ...............\");\r\n\t\t\t}\r\n\r\n//\t\t\tclientIniting = true;\r\n//\t\t\ttry {\r\n//\t\t\t\tremote = ProxyAioRouter.getStaticRemote();\r\n//\t\t\t\tclient.connect(remote, this);\r\n//\t\t\t} catch (IOException e) {\r\n//\t\t\t\tthis.clientOnError(null, e, null);\r\n//\t\t\t\te.printStackTrace();\r\n//\t\t\t}\r\n\t\t\twaitingToIniting(b);\r\n\t\t\t\r\n\t\t} else {\r\n\t\t\t//normal\r\n\t\t\twriteToClient(b);\r\n\t\t\tchekcServerQueue();\r\n\t\t}\r\n\r\n\t}\r\n\t\r\n\r\n\t@Override\r\n\tpublic void serverAfterWrite(AioServerChannel channel, ByteBuffer buffer, int bytes) {\r\n\t\tchekcServerQueue();\r\n\t}\r\n\t\r\n\tprivate final LinkedList<ByteBuffer> cq = new LinkedList<>();\r\n\tprivate final LinkedList<ByteBuffer> sq = new LinkedList<>();\r\n\t\r\n\tprivate void waitingToIniting(ByteBuffer buffer) {\r\n\t\tsynchronized (cq) {\r\n\t\t\tcq.add(buffer);\r\n\t\t}\r\n\t}\r\n\r\n\r\n\r\n\tprivate final void writeToClient(ByteBuffer buffer) {\r\n\t\tsynchronized (cq) {\r\n\t\t\tif (cq.size() == 0) {\r\n\t\t\t\twriteToClient0(buffer);\r\n\t\t\t} else {\r\n\t\t\t\tcq.add(buffer);\r\n\t\t\t\twriteClientFromQueue();\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\t\r\n\tprivate final void writeToServer(ByteBuffer buffer) {\r\n\t\tsynchronized (sq) {\r\n\t\t\tif (sq.size() == 0) {\r\n\t\t\t\twriteToServer0(buffer);\r\n\t\t\t} else {\r\n\t\t\t\tsq.add(buffer);\r\n\t\t\t\twriteServerFromQueue();\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\t\r\n\tprivate final void writeToServer0(ByteBuffer buffer) {\r\n\t\ttry {\r\n\t\t\tbuffer.flip();\r\n\t\t\tsc.write(buffer);\r\n\t\t} catch (WritePendingException e) {\r\n\t\t\te.printStackTrace();\r\n\t\t\tbuffer.position(buffer.limit());\r\n\t\t\tsq.add(buffer);\r\n\t\t\twriteServerFromQueue();\r\n\t\t}\r\n\t}\r\n\t\r\n\tprivate final void writeToClient0(ByteBuffer buffer) {\r\n\t\ttry {\r\n\t\t\tbuffer.flip();\r\n\t\t\tcc.write(buffer);\r\n\t\t} catch (WritePendingException e) {\r\n\t\t\te.printStackTrace();\r\n\t\t\tbuffer.position(buffer.limit());\r\n\t\t\tcq.add(buffer);\r\n\t\t\twriteClientFromQueue();\r\n\t\t}\r\n\t}\r\n\r\n\t/**\r\n\t * client writeQueue and merge Buffer;\r\n\t * \r\n\t */\r\n\tprivate final void writeClientFromQueue() {\r\n\t\t//log.log(1, \"cq\", cq, cq.size());\r\n\t\tByteBuffer b0 = cq.poll();\r\n\t\t\r\n\t\tfor (;;) {\r\n\t\t\tByteBuffer b1 = cq.peek();\r\n\t\t\tif (b1 == null) {\r\n\t\t\t\tbreak;\r\n\t\t\t}\r\n\t\t\tint r0 = b0.remaining();\r\n\t\t\tint s1 = b1.position();\r\n\t\t\tif (s1 > r0) {\r\n\t\t\t\tbreak;\r\n\t\t\t} else {\r\n\t\t\t\tif(b0 != b1) {\r\n\t\t\t\t\t\r\n\t\t\t\t}\r\n\t\t\t\tb0.put(b1);\r\n\t\t\t\tcq.poll();\r", "context": "src/main/java/waterwave/proxy/router/ProxyRouter.java\npublic class ProxyRouter {\r\n\r\n\tpublic static void main(String[] args) {\r\n\t\t// TODO Auto-generated method stub\r\n\r\n\t}\r\n\r\n\tpublic static SocketAddress staticRemote;\r\n\tpublic static InetAddress staticRemoteIp;\r\n\tpublic static int staticRemotePort;\r\n\tpublic static SocketAddress getStaticRemote() {\r\n\t\t// TODO Auto-generated method stub\r\n\t\treturn staticRemote;\r\n\t}\r\n\r\n}\r\nsrc/main/java/shui/common/util/CommonUtil.java\npublic class CommonUtil {\r\n\tpublic static String getLocalIp() {\r\n\t\tInetAddress addr;\r\n\t\ttry {\r\n\t\t\taddr = InetAddress.getLocalHost();\r\n\t\t\tString ip = addr.getHostAddress().toString();\r\n\t\t\treturn ip;\r\n\t\t} catch (UnknownHostException e) {\r\n\t\t\te.printStackTrace();\r\n\t\t}\r\n\t\treturn null;\r\n\r\n\t}\r\n\t\r\n\tpublic final static String getStringFromException(Throwable e) {\r\n\r\n\t\tStringWriter sw = new StringWriter();\r\n\t\tPrintWriter ps = new PrintWriter(sw);\r\n\r\n\t\te.printStackTrace(ps);\r\n\r\n\t\treturn sw.toString();\r\n\t}\r\n}\r\nsrc/main/java/shui/common/service/ThreadSharedService.java\npublic class ThreadSharedService {\r\n\tprotected final Logger log = new SimpleLogger(true);\r\n\t\r\n\tpublic static void main(String[] args) {\r\n\r\n\t}\r\n\r\n}\r\nsrc/main/java/waterwave/net/aio/AioClient.java\npublic final class AioClient extends AioService implements Runnable {\r\n\r\n\t// private final AsynchronousSocketChannel listener;\r\n\tprivate final AsynchronousChannelGroup channelGroup;\r\n\tprivate final AioDataDealerFactory aioDataDealerFactory;\r\n\t\r\n\tprivate final Map<Integer, AioServerChannel> connections = new ConcurrentHashMap<Integer, AioServerChannel>();\r\n\t\r\n\r\n\tpublic AioClient(ExecutorService channelWorkers, AioDataDealerFactory aioDataDealerFactory) throws IOException {\r\n\t\t// ExecutorService channelWorkers = Executors.newFixedThreadPool(Runtime.getRuntime().availableProcessors(), Executors.defaultThreadFactory());\r\n\t\tchannelGroup = AsynchronousChannelGroup.withThreadPool(channelWorkers);\r\n\t\tthis.aioDataDealerFactory = aioDataDealerFactory;\r\n\t}\r\n\r\n\tclass AcceptHandler implements CompletionHandler<Void, Void> {\r\n\t\tprivate final AsynchronousSocketChannel listener;\r\n\t\tprivate final AioClientDataDealer aioClientDataDealer;\r\n\r\n\t\tpublic AcceptHandler(AsynchronousSocketChannel listener, AioClientDataDealer aioClientDataDealer) {\r\n\t\t\tsuper();\r\n\t\t\tthis.listener = listener;\r\n\t\t\tthis.aioClientDataDealer = aioClientDataDealer;\r\n\t\t}\r\n\r\n\t\t@Override\r\n\t\tpublic void completed(Void result, Void attachment) {\r\n\t\t\t//log.log(1, \"Client AcceptHandler connected \");\r\n\t\t\thandleNewConnection(listener, aioClientDataDealer);\r\n\t\t}\r\n\r\n\t\t@Override\r\n\t\tpublic void failed(Throwable exc, Void attachment) {\r\n\t\t\t//log.log(1, \"Client AcceptHandler failed result: \" + exc);\r\n\t\t}\r\n\r\n\t}\r\n\t\r\n\tpublic AsynchronousSocketChannel connect(SocketAddress remote) throws IOException {\r\n\t\treturn connect(remote, null);\r\n\t}\r\n\tpublic AsynchronousSocketChannel connect(SocketAddress remote, AioClientDataDealer aioClientDataDealer) throws IOException {\r\n\t\tAsynchronousSocketChannel listener = createListener(channelGroup);\r\n\t\t//log.log(1, \"client start connect\");\r\n\r\n\t\tAcceptHandler acceptHandler = new AcceptHandler(listener, aioClientDataDealer);\r\n\r\n\t\tlistener.connect(remote, null, acceptHandler);\r\n\t\treturn listener;\r\n\t}\r\n\r\n\tprotected void handleNewConnection(AsynchronousSocketChannel channel, AioClientDataDealer aioClientDataDealer) {\r\n\t\tif (!channel.isOpen()) {\r\n\t\t\t//log.log(1, \"handleNewConnection closed.. \");\r\n\t\t\treturn;\r\n\t\t}\r\n\t\t\r\n\t\tAioClientDataDealer dealer = null;\r\n\t\tif (aioClientDataDealer != null) {\r\n\t\t\tdealer = aioClientDataDealer;\r\n\t\t} else {\r\n\t\t\taioDataDealerFactory.getAioClientDataDealer();\r\n\t\t}\r\n\r\n\t\tint channelId = getChannelId();\r\n\t\tAioClientChannel aioChannel = new AioClientChannel(channelId, channel, dealer, this);\r\n\t\t\r\n\t\t// connections.add(aioChannel);\r\n\t\taioChannel.run(null);\r\n\t\t\r\n\t\tdealer.clientOnConnect(aioChannel);\r\n\r\n\t\t// String w = \"GET / HTTP/1.1 \\n\\n\";\r\n\t\t// ByteBuffer buffer = ByteBuffer.wrap(w.getBytes());\r\n\t\t// System.out.println(\"set write \");\r\n\t\t// channel.write(buffer, buffer, new CompletionHandler<Integer, ByteBuffer>() {\r\n\t\t// @Override\r\n\t\t// public void completed(Integer result, ByteBuffer buffer) {\r\n\t\t// if (buffer.hasRemaining()) {\r\n\t\t// System.out.println(\"write... \");\r\n\t\t// channel.write(buffer, buffer, this);\r\n\t\t// } else {\r\n\t\t// // Go back and check if there is new data to write\r\n\t\t// // writeFromQueue();\r\n\t\t// System.out.println(\"write complete \" + result);\r\n\t\t// }\r\n\t\t// }\r\n\t\t//\r\n\t\t// @Override\r\n\t\t// public void failed(Throwable exc, ByteBuffer attachment) {\r\n\t\t// }\r\n\t\t// });\r\n\r\n\t}\r\n\r\n\t/**\r\n\t * \r\n\t * \r\n\t * @param channelGroup\r\n\t * @return\r\n\t * @throws IOException\r\n\t * \r\n\t * \r\n\t *             SO_SNDBUF The size of the socket send buffer .\r\n\t *             SO_RCVBUF The size of the socket receive buffer. \r\n\t *             SO_KEEPALIVE Keep connection alive. \r\n\t *             SO_REUSEADDR Re-use address .\r\n\t *             TCP_NODELAY Disable the Nagle algorithm.\r\n\t * \r\n\t * \r\n\t */\r\n\tprivate AsynchronousSocketChannel createListener(AsynchronousChannelGroup channelGroup) throws IOException {\r\n\t\tfinal AsynchronousSocketChannel listener = AsynchronousSocketChannel.open(channelGroup);\r\n\t\t//TODO\r\n\t\t//listener.setOption(StandardSocketOptions.TCP_NODELAY, true);\r\n\t\tlistener.setOption(StandardSocketOptions.SO_REUSEADDR, true);\r\n\t\tlistener.setOption(StandardSocketOptions.SO_RCVBUF, 16 * 1024);\r\n\t\tlistener.setOption(StandardSocketOptions.SO_SNDBUF, 16 * 1024);\r\n\t\treturn listener;\r\n\t}\r\n\r\n\t//for test\r\n\t@Override\r\n\tpublic void run() {\r\n\t\tSystem.out.println(Thread.currentThread().getName() + \"---run\");\r\n\r\n\t\tInetSocketAddress r = new InetSocketAddress(\"10.213.33.176\", 11200);\r\n\t\ttry {\r\n\t\t\tAsynchronousSocketChannel channel = connect(r);\r\n\t\t\tSystem.out.println(channel);\r\n\t\t} catch (IOException e) {\r\n\t\t\te.printStackTrace();\r\n\t\t}\r\n\t}\r\n\t\r\n\t\r\n\t/**\r\n\t * \r\n\t */\r\n\tprivate AtomicInteger channelId = new AtomicInteger();\r\n\tprivate int getChannelId() {\r\n\t\treturn channelId.incrementAndGet();\r\n\t}\r\n\r\n\tpublic void removeAioChannel(AioClientChannel client) {\r\n       connections.remove(client.getChannelId());\r\n\t}\r\n\t\r\n\r\n\tpublic static void main(String[] args) {\r\n\t\ttry {\r\n\t\t\tint threadCap = 4;\r\n\t\t\tExecutorService channelWorkers = Executors.newFixedThreadPool(threadCap, Executors.defaultThreadFactory());\r\n\t\t\tAioClient c = new AioClient(channelWorkers, null);\r\n\t\t\tc.run();\r\n\r\n\t\t\tThread.sleep(100000);\r\n\r\n\t\t} catch (Exception e) {\r\n\t\t\te.printStackTrace();\r\n\t\t}\r\n\r\n\t}\r\n\r\n}\r\nsrc/main/java/waterwave/net/aio/AioClientChannel.java\npublic final class AioClientChannel extends AioChannel{\r\n\t\r\n\tprivate final AsynchronousSocketChannel channel;\r\n\tprivate final AioClientDataDealer dealer;\r\n\t\r\n\tprivate final AioClient aioClient;\r\n\tprivate final int channelId;\r\n\t\r\n\t\r\n\t\r\n\r\n\tpublic AsynchronousSocketChannel getChannel() {\r\n\t\treturn channel;\r\n\t}\r\n\r\n\tpublic int getChannelId() {\r\n\t\treturn channelId;\r\n\t}\t\r\n\r\n\tpublic AioClientChannel(int channelId, AsynchronousSocketChannel channel, AioClientDataDealer dealer, AioClient aioClient) {\r\n\t\tthis.channelId = channelId;\r\n\t\tthis.channel = channel;\r\n\t\tthis.dealer = dealer;\r\n\t\tthis.aioClient = aioClient;\r\n\t}\r\n\r\n\tpublic static void main(String[] args) {\r\n\t\t// TODO Auto-generated method stub\r\n\r\n\t}\r\n\r\n\tpublic void run(ReadHandler reader) {\r\n\t\tdealer.clientBeforeRead(this);\r\n\t\tif(reader == null){\r\n\t\t\t//new\r\n\t\t\treader = new ReadHandler(this);\r\n\t\t}\r\n\t\tread(reader);\r\n\t\t\r\n\t}\r\n\t\r\n\t/**\r\n     * Runs a cycle of doing a beforeRead action and then enqueuing a new\r\n     * read on the client. Handles closed channels and errors while reading.\r\n     * If the client is still connected a new round of actions are called.\r\n     */\r\n\tclass ReadHandler implements CompletionHandler<Integer, ByteBuffer>{\r\n\t\tAioClientChannel clientChannel;\r\n\t\t\r\n        public ReadHandler(AioClientChannel clientChannel) {\r\n\t\t\tsuper();\r\n\t\t\tthis.clientChannel = clientChannel;\r\n\t\t}\r\n\r\n        @Override\r\n\t\tpublic void completed(Integer result, ByteBuffer buffer) {\r\n\t\t\t// if result is negative or zero the connection has been closed or something gone wrong\r\n\t\t\t//log.log(1, \"client ReadHandler read success: \");\r\n\t\t\tif (result < 1) {\r\n\t\t\t\tclientChannel.close();\r\n\t\t\t\tdealer.clientOnClose(clientChannel);\r\n\t\t\t\t\r\n\t\t\t\t//log.log(1, \"result < 1 ReadHandler Closing connection to \" + channel);\r\n\t\t\t} else {\r\n//\t\t\t\tSystem.out.println(\"result: \" + result);\r\n//\t\t\t\t// callback.onData(client, buffer, result);\r\n//\t\t\t\tbuffer.flip();\r\n//\t\t\t\tString name;\r\n//\t\t\t\tname = new String(buffer.array(), 0, result);\r\n//\t\t\t\tSystem.out.println(\"data: \" + name);\r\n//\t\t\t\t// enqueue next round of actions\r\n//\t\t\t\t// client.run();\r\n\t\t\t\t\r\n\t\t\t\tdealer.clientOnData(clientChannel, buffer, result);\r\n                // enqueue next round of actions\r\n\t\t\t\tclientChannel.run(this);\r\n\t\t\t\t\r\n\t\t\t}\r\n\t\t\t\r\n\t\t}\r\n\r\n\t\t@Override\r\n\t\tpublic void failed(Throwable e, ByteBuffer buffer) {\r\n\t\t\tlog.log(9, \"client ReadHandler read fail: \" + e);\r\n\t\t\t\r\n\t\t\tclientChannel.close();\r\n\t\t\tdealer.clientOnError(clientChannel, e, buffer);\r\n\t\t}\r\n\t\t\r\n//\t\tpublic void onData(AioServerChannel channel, ByteBuffer buffer, int bytes) {\r\n//\t        buffer.flip();\r\n//\t        // Just append the message on the buffer\r\n//\t        //AioServerChannel.appendMessage(new String(buffer.array(), 0, bytes));\r\n//\t    }\r\n\t\t\r\n\t}\r\n\t\r\n\tclass WriteHandler implements CompletionHandler<Integer, ByteBuffer>{\r\n\t\tAioClientChannel clientChannel;\r\n\t\t\r\n        public WriteHandler(AioClientChannel clientChannel) {\r\n\t\t\tsuper();\r\n\t\t\tthis.clientChannel = clientChannel;\r\n\t\t}\r\n\r\n\t\t@Override\r\n\t\tpublic void completed(Integer result, ByteBuffer buffer) {\r\n\t\t\t// if result is negative or zero the connection has been closed or something gone wrong\r\n\t\t\t//log.log(1, \"client WriteHandler write success: \");\r\n\t\t\tif (buffer.hasRemaining()) {\r\n\t\t\t\t//log.log(1, \"client WriteHandler write...  hasRemaining \");\r\n\t\t\t\tchannel.write(buffer, buffer, this);\r\n\t\t\t} else {\r\n\t\t\t\t// Go back and check if there is new data to write\r\n\t\t\t\t// writeFromQueue();\r\n\t\t\t\t//log.log(1, \"client write complete \" + result);\r\n\r\n\t\t\t}\r\n\t\t\tdealer.clientAfterWrite(clientChannel, buffer, result);\r\n\t\t\t\r\n\t\t\tBufferTools.returnBuffer(buffer);\r\n\t\t}\r\n\r\n\t\t@Override\r\n\t\tpublic void failed(Throwable e, ByteBuffer buffer) {\r\n\t\t\tlog.log(9, \"client WriteHandler write fail: \" + e);\r\n\t\t\tclientChannel.close();\r\n\t\t}\r\n\t\t\r\n//\t\tpublic void onData(AioServerChannel channel, ByteBuffer buffer, int bytes) {\r\n//\t        buffer.flip();\r\n//\t        // Just append the message on the buffer\r\n//\t        //AioServerChannel.appendMessage(new String(buffer.array(), 0, bytes));\r\n//\t    }\r\n\t\t\r\n\t}\r\n\t\r\n    /**\r\n     * Enqueue a read\r\n     * @param completionHandler callback on completed read\r\n     */\r\n    public final void read(CompletionHandler<Integer, ? super ByteBuffer> completionHandler) {\r\n    \t//log.log(1, \"Cliet: start client read \");\r\n    \t\r\n        ByteBuffer input = BufferTools.getBuffer();\r\n        if (!channel.isOpen()) {\r\n            return;\r\n        }\r\n        channel.read(input, input, completionHandler);\r\n    }\r\n    \r\n    \r\n    private CompletionHandler<Integer, ? super ByteBuffer> writerHandler = null;\r\n    public final void write(final ByteBuffer input) {\r\n    \tif(this.writerHandler == null) {\r\n    \t\twriterHandler = new WriteHandler(this);\r\n    \t}\r\n    \twrite(input, writerHandler);\r\n    }\r\n    \r\n    /**\r\n     * Enqueue a read\r\n     * @param completionHandler callback on completed read\r\n     */\r\n    protected final void write(ByteBuffer b ,CompletionHandler<Integer, ? super ByteBuffer> completionHandler) {\r\n    \t//log.log(1, \"start client write \");\r\n    \t\r\n        if (!channel.isOpen()) {\r\n            return;\r\n        }\r\n        channel.write(b, b, completionHandler);\r\n    }\r\n    \r\n\r\n\tpublic void close() {\r\n        try {\r\n            channel.close();\r\n        } catch (IOException e) {\r\n            e.printStackTrace();\r\n        }\r\n        aioClient.removeAioChannel(this);\r\n\t\t\r\n\t\t\r\n\t}\r\n\r\n}\r\nsrc/main/java/waterwave/net/aio/AioServerChannel.java\npublic final class AioServerChannel extends AioChannel{\r\n\t\r\n\tprivate final AsynchronousSocketChannel channel;\r\n\tprivate final AioServerDataDealer dealer;\r\n\tprivate final AioServer aioServer;\r\n\tprivate final int channelId;\r\n    //private final Queue<ByteBuffer> queue = new LinkedList<ByteBuffer>();\r\n\tprivate final Queue<ByteBuffer> queue = null;\r\n    \r\n    private boolean writing = false;\r\n\t\r\n    \r\n\r\n\tpublic int getChannelId() {\r\n\t\treturn channelId;\r\n\t}\r\n\r\n\r\n\tpublic AioServerChannel(int channelId, AsynchronousSocketChannel channel, AioServerDataDealer dealer, AioServer aioServer) {\r\n\t\tdealer.serverOnConnect(this);\r\n\t\tthis.channel = channel;\r\n\t\tthis.dealer = dealer;\r\n\t\tthis.aioServer = aioServer;\r\n\t\tthis.channelId = channelId;\r\n\t}\r\n\r\n\r\n\tpublic void run(ReadHandler reader) {\r\n\t\tdealer.serverBeforeRead(this);\r\n\t\tif (reader == null) {\r\n\t\t\treader = new ReadHandler(this);\r\n\t\t}\r\n\t\tread(reader);\r\n\r\n\t}\r\n\t\r\n    /**\r\n     * Runs a cycle of doing a beforeRead action and then enqueuing a new\r\n     * read on the client. Handles closed channels and errors while reading.\r\n     * If the client is still connected a new round of actions are called.\r\n     */\r\n\tclass ReadHandler implements CompletionHandler<Integer, ByteBuffer>{\r\n\t\tAioServerChannel serverChannel;\r\n\t\t\r\n        public ReadHandler(AioServerChannel serverChannel) {\r\n\t\t\tsuper();\r\n\t\t\tthis.serverChannel = serverChannel;\r\n\t\t}\r\n\r\n\t\t@Override\r\n        public void completed(Integer result, ByteBuffer buffer) {\r\n            // if result is negative or zero the connection has been closed or something gone wrong\r\n            if (result < 1) {\r\n            \t//log.log(1, \"server:ReadHandler result < 1 Closing connection to \" + serverChannel);\r\n            \t\r\n            \tserverChannel.close();\r\n            \tdealer.serverOnClose(serverChannel);\r\n            \t\r\n            } else {\r\n            \tdealer.serverOnData(serverChannel, buffer, result);\r\n                // enqueue next round of actions\r\n                serverChannel.run(this);\r\n            }\r\n        }\r\n\r\n\t\t@Override\r\n\t\tpublic void failed(Throwable exc, ByteBuffer attachment) {\r\n\t\t\t// TODO Auto-generated method stub\r\n\t\t\tserverChannel.close();\r\n\t\t\t\r\n\t\t\tdealer.serverOnError(serverChannel, exc, attachment);\r\n\t\t}\r\n\t\t\r\n//\t\tpublic void onData(AioServerChannel channel, ByteBuffer buffer, int bytes) {\r\n//\t        buffer.flip();\r\n//\t        // Just append the message on the buffer\r\n//\t        //AioServerChannel.appendMessage(new String(buffer.array(), 0, bytes));\r\n//\t    }\r\n\t\t\r\n\t}\r\n\t\r\n    /**\r\n     * Enqueue a read\r\n     * @param completionHandler callback on completed read\r\n     */\r\n    public final void read(CompletionHandler<Integer, ? super ByteBuffer> completionHandler) {\r\n        ByteBuffer input = BufferTools.getBuffer();\r\n        if (!channel.isOpen()) {\r\n            return;\r\n        }\r\n        channel.read(input, input, completionHandler);\r\n    }\r\n    \r\n\r\n    \r\n\tclass WriteHandler implements CompletionHandler<Integer, ByteBuffer>{\r\n\t\tAioServerChannel serverChannel;\r\n\t\t\r\n        public WriteHandler(AioServerChannel serverChannel) {\r\n\t\t\tsuper();\r\n\t\t\tthis.serverChannel = serverChannel;\r\n\t\t}\r\n\r\n        @Override\r\n\t\tpublic void completed(Integer result, ByteBuffer buffer) {\r\n\t\t\t// if result is negative or zero the connection has been closed or something gone wrong\r\n\t\t\t//System.out.println(\"read success: \");\r\n\t\t\tif (buffer.hasRemaining()) {\r\n\t\t\t\t//log.log(1, \"server write... \");\r\n\t\t\t\tchannel.write(buffer, buffer, this);\r\n\t\t\t} else {\r\n\t\t\t\t// Go back and check if there is new data to write\r\n\t\t\t\t// writeFromQueue();\r\n\t\t\t\t//log.log(1, \"server write complete \" + result);\r\n\t\t\t\t\r\n\t\t\t}\r\n\t\t\tdealer.serverAfterWrite(serverChannel, buffer, result);\r\n\t\t\tBufferTools.returnBuffer(buffer);\r\n\t\t}\r\n\r\n\t\t@Override\r\n\t\tpublic void failed(Throwable exc, ByteBuffer buffer) {\r\n\t\t\tlog.log(9, \"server WriteHandler write fail: \" + exc);\r\n\t\t\tserverChannel.close();\r\n\t\t}\r\n\t\t\r\n//\t\tpublic void onData(AioServerChannel channel, ByteBuffer buffer, int bytes) {\r\n//\t        buffer.flip();\r\n//\t        // Just append the message on the buffer\r\n//\t        //AioServerChannel.appendMessage(new String(buffer.array(), 0, bytes));\r\n//\t    }\r\n\t\t\r\n\t}\r\n    \r\n    \r\n\r\n    \r\n    private CompletionHandler<Integer, ? super ByteBuffer> writerHandler = null;\r\n    public final void write(final ByteBuffer input) {\r\n    \tif(this.writerHandler == null) {\r\n    \t\twriterHandler = new WriteHandler(this);\r\n    \t}\r\n    \twrite(input, writerHandler);\r\n    }\r\n    \r\n    /**\r\n     * Enqueue a read\r\n     * @param completionHandler callback on completed read\r\n     */\r\n    protected final void write(ByteBuffer b ,CompletionHandler<Integer, ? super ByteBuffer> completionHandler) {\r\n    \t//log.log(1, \"start server write \");\r\n    \t\r\n        if (!channel.isOpen()) {\r\n            return;\r\n        }\r\n        channel.write(b, b, completionHandler);\r\n    }\r\n    \r\n\r\n\r\n\t\r\n\t/**\r\n     * Closes the channel\r\n     */\r\n    public void close() {\r\n        try {\r\n            channel.close();\r\n        } catch (IOException e) {\r\n            e.printStackTrace();\r\n        }\r\n        aioServer.removeAioChannel(this);\r\n    }\r\n\r\n\r\n    \r\n    /**\r\n     * Enqueues a write of the buffer to the channel.\r\n     * The call is asynchronous so the buffer is not safe to modify after\r\n     * passing the buffer here.\r\n     *\r\n     * @param buffer the buffer to send to the channel\r\n     */\r\n    @Deprecated\r\n    public void writeOld(final ByteBuffer buffer) {\r\n        boolean threadShouldWrite = false;\r\n\r\n        synchronized(queue) {\r\n            queue.add(buffer);\r\n            // Currently no thread writing, make this thread dispatch a write\r\n            if (!writing) {\r\n                writing = true;\r\n                threadShouldWrite = true;\r\n            }\r\n        }\r\n\r\n        if (threadShouldWrite) {\r\n        \twriteFromQueueOld();\r\n        }\r\n    }\r\n    \r\n    @Deprecated\r\n    private void writeFromQueueOld() {\r\n        ByteBuffer buffer;\r\n\r\n        synchronized (queue) {\r\n            buffer = queue.poll();\r\n            if (buffer == null) {\r\n                writing = false;\r\n            }\r\n        }\r\n\r\n        // No new data in buffer to write\r\n        if (writing) {\r\n        \twriteBufferOld(buffer);\r\n        }\r\n    }\r\n    \r\n    @Deprecated\r\n    private void writeBufferOld(ByteBuffer buffer) {\r\n        channel.write(buffer, buffer, new CompletionHandler<Integer, ByteBuffer>() {\r\n            @Override\r\n            public void completed(Integer result, ByteBuffer buffer) {\r\n                if (buffer.hasRemaining()) {\r\n                    channel.write(buffer, buffer, this);\r\n                } else {\r\n                    // Go back and check if there is new data to write\r\n                \twriteFromQueueOld();\r\n                }\r\n            }\r\n\r\n            @Override\r\n            public void failed(Throwable exc, ByteBuffer attachment) {\r\n            }\r\n        });\r\n    }\r\n    \r\n    \r\n\r\n    /**\r\n     * Sends a message\r\n     * @param string the message\r\n     */\r\n    @Deprecated\r\n    public void writeStringMessageOld(String string) {\r\n    \twriteOld(ByteBuffer.wrap(string.getBytes()));\r\n    }\r\n\r\n    /**\r\n     * Send a message from a specific client\r\n     * @param client the message is sent from\r\n     * @param message to send\r\n     */\r\n    @Deprecated\r\n    public void writeMessageFromOld(AioServerChannel client, String message) {\r\n        if (dealer.serverAcceptsMessages()) {\r\n            //writeStringMessage(client.getUserName() + \": \" + message);\r\n        }\r\n    }\r\n    \r\n\r\n\t\r\n\r\n}\r\nsrc/main/java/waterwave/net/aio/define/AioServerDataDealer.java\npublic interface AioServerDataDealer {\r\n\t\r\n\tvoid serverOnConnect(AioServerChannel channel);\r\n    void serverBeforeRead(AioServerChannel channel);\r\n    void serverOnData(AioServerChannel channel, ByteBuffer buffer, int bytes);\r\n    void serverAfterWrite(AioServerChannel channel, ByteBuffer buffer, int bytes);\r\n    void serverOnError(AioServerChannel channel,Throwable exc, ByteBuffer attachment);\r\n    void serverOnClose(AioServerChannel channel);\r\n    boolean serverAcceptsMessages();\r\n    \r\n\r\n}\r\nsrc/main/java/shui/common/buffer/BufferTools.java\npublic class BufferTools {\r\n\r\n\tprivate final static BufferPoolNIO bp = new BufferPoolNIO(2 * 1024, 32 * 1024);\r\n\t\r\n\tpublic final static ByteBuffer getBuffer() {\r\n\t\t//ByteBuffer input = ByteBuffer.allocate(16 * 1024);\r\n\t\tByteBuffer input = bp.allocate();\r\n\t\treturn input;\r\n\t}\r\n\r\n\tpublic final static void returnBuffer(ByteBuffer buffer) {\r\n\t\tbp.recycle(buffer);\r\n\t\t\r\n\t}\r\n\tpublic final static byte[] getBuffer2Byte(ByteBuffer b) {\r\n\t\tint p = b.position();\r\n\t\tbyte[] r = new byte[p];\r\n\t\tif (b.position() != 0) {\r\n\t\t\tb.flip();\r\n\t\t}\r\n\t\tb.get(r, 0, p);\r\n\r\n\t\treturn r;\r\n\t}\r\n\r\n\tpublic static void main(String[] args) {\r\n\t\t// TODO Auto-generated method stub\r\n\r\n\t}\r\n\r\n\r\n}\r\nsrc/main/java/waterwave/net/aio/define/AioClientDataDealer.java\npublic interface AioClientDataDealer {\r\n\r\n    \r\n\tvoid clientBeforeRead(AioClientChannel channel);\r\n\tvoid clientOnConnect(AioClientChannel channel);\r\n\tvoid clientOnData(AioClientChannel channel, ByteBuffer buffer, int result);\r\n\tvoid clientAfterWrite(AioClientChannel channel, ByteBuffer buffer, int bytes);\r\n\tvoid clientOnError(AioClientChannel channel,Throwable exc, ByteBuffer attachment);\r\n    void clientOnClose(AioClientChannel channel);\r\n\tboolean clientAcceptsMessages();\r\n}\r\nsrc/main/java/shui/common/log/Logger.java\npublic abstract class Logger {\r\n\r\n\tpublic String threadInfo = null;\r\n\tpublic int level;\r\n\r\n\tprotected boolean printTime = true;\r\n\tprotected boolean printThreadInfo = true;\r\n\r\n\tpublic Logger() {\r\n\t}\r\n\r\n\tpublic Logger(boolean printTime) {\r\n\t\tthis();\r\n\t\tthis.printTime = printTime;\r\n\t}\r\n\r\n\t\r\n\tpublic abstract void log(int level, Object o) ;\r\n\tpublic abstract void log(int level, Object... os) ;\r\n\tpublic abstract PrintWriter getErrorWriter();\r\n\r\n\r\n\tpublic final static void log(byte[] bs) {\r\n\t\tfor (byte b : bs) {\r\n\t\t\tlog1(b);\r\n\t\t\tlog1(\"\\t\");\r\n\t\t}\r\n\t\tlog(\"\\t\");\r\n\t}\r\n\t\r\n\t\r\n\tpublic final static void log1(Object o) {\r\n\t\tSystem.out.print(o);\r\n\t}\r\n\t\r\n\tpublic final static void log(Object o) {\r\n\t\tSystem.out.println(o);\r\n\t}\r\n\r\n\tpublic static void main(String[] args) {\r\n\t\t// TODO Auto-generated method stub\r\n\r\n\t}\r\n\r\n}\r\n", "answers": ["\t\t\t\tBufferTools.returnBuffer(b1);\r"], "length": 2646, "dataset": "repobench-p_e", "language": "java", "all_classes": null, "_id": "2f63d9a3d25c12812b10eb407a92d3744aaa94835463eb5b"}
{"input": "from math import pi\nfrom PyQt5 import QtGui\nfrom PyQt5.QtCore import QLocale\nfrom PyQt5.QtWidgets import QComboBox, QLineEdit, QHBoxLayout, QTableWidget, QPushButton, QTableWidgetItem, QInputDialog\nfrom PyQt5.QtWidgets import QDialog\nfrom PyQt5.QtWidgets import QDialogButtonBox\nfrom PyQt5.QtWidgets import QGridLayout\nfrom PyQt5.QtWidgets import QLabel\nfrom PyQt5.QtWidgets import QVBoxLayout\nfrom PyQt5.QtWidgets import QWidget\nfrom Business.ParameterActions import create_new_standard, create_new_type, remove_standard, remove_type\nfrom Data.Axis import Axis\nfrom Data.Parameters import Parameters\nfrom Data.Proformer import ProformerType\nfrom GUI.Widgets.SketchViewWidget import SketchViewWidget\nfrom GUI.init import formula_from_locale, gui_scale, tr\n\t\tcontents_layout.addWidget(self._sa_combo_box, 1, 1)\n\t\tcontents_layout.addWidget(self._ea_combo_box, 2, 1)\n\t\tself.layout().addWidget(contents_widget)\n\t\tdialog_buttons = QDialogButtonBox(QDialogButtonBox.Cancel | QDialogButtonBox.Ok)\n\t\tdialog_buttons.accepted.connect(self.accept)\n\t\tdialog_buttons.rejected.connect(self.reject)\n\t\tself.layout().addWidget(dialog_buttons)\n\n\tdef radius_param(self):\n\t\treturn self._radius_combo_box.currentText()\n\n\tdef start_angle_param(self):\n\t\treturn self._sa_combo_box.currentText()\n\n\tdef end_angle_param(self):\n\t\treturn self._ea_combo_box.currentText()\n\n\nclass StandardTypeManager(QDialog):\n\tdef __init__(self, parent, parameters):\n\t\tQDialog.__init__(self, parent)\n\t\tself._parameters = parameters\n\t\tself.setLayout(QVBoxLayout())\n\t\tdialog_buttons = QDialogButtonBox(QDialogButtonBox.Close)\n\t\tdialog_buttons.close.connect(self.accept)\n\t\tself.layout().addWidget(dialog_buttons)\n\n\nclass SketchMirrorDialog(QDialog):\n\tdef __init__(self, parent, sketch):\n\t\tQDialog.__init__(self, parent)\n\t\tself._sketch = sketch\n\t\tself.setWindowTitle(tr(\"Mirror\", 'dialogs'))\n\t\tself.setLayout(QVBoxLayout())\n\t\tcontents_widget = QWidget(self)\n\t\tcontents_layout = QGridLayout()\n\t\tcontents_widget.setLayout(contents_layout)\n\t\tself._mirror_type = ProformerType.Mirror\n\n\t\tcontents_layout.addWidget(QLabel(tr(\"Mirror type\", 'dialogs')), 0, 0)\n\t\tcontents_layout.addWidget(QLabel(tr(\"Mirror line\", 'dialogs')), 1, 0)\n\n\t\tself._mirror_type_combo_box = QComboBox()\n\t\tself._mirror_line_combo_box = QComboBox()\n\t\tself._mirror_type_combo_box.currentIndexChanged.connect(self.on_mirror_type_selection_changed)\n\n\t\tself._mirror_type_combo_box.addItem(ProformerType.Mirror.name, ProformerType.Mirror.value)\n\t\tself._mirror_type_combo_box.addItem(ProformerType.MirrorX.name, ProformerType.MirrorX.value)\n\t\tself._mirror_type_combo_box.addItem(ProformerType.MirrorY.name, ProformerType.MirrorY.value)\n\t\tself._mirror_type_combo_box.addItem(ProformerType.MirrorXY.name, ProformerType.MirrorXY.value)\n\t\tself._mirror_type_combo_box.setEditable(True)\n\t\t#self._mirror_line_combo_box.addItems(self._params)\n\t\tself._mirror_line_combo_box.setEditable(True)\n\t\t#self._ea_combo_box.addItems(self._params)\n\t\t#self._ea_combo_box.setEditable(True)\n\t\tcontents_layout.addWidget(self._mirror_type_combo_box, 0, 1)\n\t\tcontents_layout.addWidget(self._mirror_line_combo_box, 1, 1)\n\n\t\tself.layout().addWidget(contents_widget)\n\t\tself._sketch_view = SketchViewWidget(self, sketch, sketch.document)\n\t\tself._sketch_view.set_change_listener(self)\n\t\tself._sketch_view.edges_selectable = True\n\t\t#self._sketch_view.set_sketch(sketch)\n\t\tself.layout().addWidget(self._sketch_view)\n\n\t\tdialog_buttons = QDialogButtonBox(QDialogButtonBox.Cancel | QDialogButtonBox.Ok)\n\t\tdialog_buttons.accepted.connect(self.accept)\n\t\tdialog_buttons.rejected.connect(self.reject)\n\t\tself.layout().addWidget(dialog_buttons)\n\t\tself.fill_mirror_axi()\n\n\t@property\n\tdef mirror_type(self):\n\t\treturn self._mirror_type\n\n\t@property\n\tdef mirror_axis(self):\n\t\treturn self._sketch.get_edge_by_name(self._mirror_line_combo_box.currentText())\n\n\tdef on_edge_selected(self, edge):\n\t\tself._mirror_line_combo_box.setCurrentText(edge.name)\n\n\tdef on_area_selected(self, area):\n\t\tself._area_combo_box.setCurrentText(area.name)\n\n\tdef on_mirror_type_selection_changed(self):\n\t\tindex = self._mirror_type_combo_box.currentIndex()\n\t\tif index == 0:\n\t\t\tself._mirror_type = ProformerType.Mirror\n\t\t\tself._mirror_line_combo_box.setEnabled(True)\n\t\t\tself._mirror_line_combo_box.setCurrentIndex(0)\n\t\telif index == 1:\n\t\t\tself._mirror_type = ProformerType.MirrorX\n\t\t\tself._mirror_line_combo_box.setEnabled(False)\n\t\t\tself._mirror_line_combo_box.setCurrentText(\"X axis\")\n\t\telif index == 2:\n\t\t\tself._mirror_type = ProformerType.MirrorY\n\t\t\tself._mirror_line_combo_box.setEnabled(False)\n\t\t\tself._mirror_line_combo_box.setCurrentText(\"Y axis\")\n\t\telse:\n\t\t\tself._mirror_type = ProformerType.MirrorXY\n\t\t\tself._mirror_line_combo_box.setEnabled(False)\n\t\t\tself._mirror_line_combo_box.setCurrentText(\"XY axis\")\n\n\tdef fill_mirror_axi(self):\n\t\tself._mirror_line_combo_box.clear()\n\t\t# self._mirror_line_combo_box.addItems(['X Axis', 'Y Axis'])\n\t\tedge_names = []\n\t\tsketch = self._sketch\n\t\tself._sketch_view.set_sketch(sketch)\n\t\tfor edge in sketch.get_edges():\n\t\t\tedge_names.append(edge.name)\n\t\tedge_names.sort()\n\t\tself._mirror_line_combo_box.addItems(edge_names)\n\nclass ParameterSelectWidget():\n\tdef __init__(self, parent, parameters, layout, row, caption=\"\"):\n\t\tself._parameters = parameters\n\n\t\tself._caption_label = QLabel(caption)\n", "context": "GUI/Widgets/SketchViewWidget.py\nclass SketchViewWidget(QWidget):\n\tdef __init__(self, parent, sketch, document):\n\t\tQWidget.__init__(self, parent)\n\t\tself._doc = document\n\t\tself._sketch = sketch\n\t\tself.setMinimumHeight(250)\n\t\tself.setMinimumWidth(250)\n\t\tself.setMouseTracking(True)\n\t\tself._show_areas = False\n\t\tself._areas_selectable = False\n\t\tself._edges_selectable = False\n\t\tself._keypoints_selectable = False\n\t\tself._change_listener = None\n\t\tself._selected_areas = []\n\t\tself._selected_edges = []\n\t\tself._selected_kps = []\n\t\tself._area_hover = None\n\t\tself._edge_hover = None\n\t\tself._kp_hover = None\n\t\tself._mouse_position = None\n\n\t@property\n\tdef selected_kps(self):\n\t    return self._selected_kps\n\n\t@selected_kps.setter\n\tdef selected_kps(self, value):\n\t\tself._selected_kps = value\n\t\tself.update()\n\n\tdef mouseMoveEvent(self, q_mouse_event):\n\t\tposition = q_mouse_event.pos()\n\t\tif self._mouse_position is not None:\n\t\t\tmouse_move_x = self._mouse_position.x() - position.x()\n\t\t\tmouse_move_y = self._mouse_position.y() - position.y()\n\t\telse:\n\t\t\tmouse_move_x = 0\n\t\t\tmouse_move_y = 0\n\t\tself._mouse_position = position\n\t\tif self._sketch is None:\n\t\t\treturn\n\t\tupdate_view = False\n\t\tif self._area_hover is not None or self._edge_hover is not None:\n\t\t\tupdate_view = True\n\t\tself._area_hover = None\n\t\tself._edge_hover = None\n\t\twidth = self.width() / 2\n\t\theight = self.height() / 2\n\t\tlimits = self._sketch.get_limits()\n\t\tsketch_width = limits[2] - limits[0]\n\t\tsketch_height = limits[3] - limits[1]\n\t\tscale_x = self.width() / sketch_width\n\t\tscale_y = self.height() / sketch_height\n\t\tscale = min(scale_x, scale_y) * 0.9\n\t\toffset = Vertex(-limits[0] - sketch_width / 2, -limits[1] - sketch_height / 2)\n\t\tx = (self._mouse_position.x() - width) / scale - offset.x\n\t\ty = -((self._mouse_position.y() - height) / scale + offset.y)\n\n\n\t\tif self._keypoints_selectable:\n\t\t\tfor key_point in self._sketch.get_keypoints():\n\t\t\t\tx1 = key_point.x\n\t\t\t\ty1 = key_point.y\n\t\t\t\tif abs(x1 - x) < 5 / scale and abs(y1 - y) < 5 / scale:\n\t\t\t\t\tself._kp_hover = key_point\n\t\t\t\t\tupdate_view = True\n\t\t\t\t\tbreak\n\t\tif self._edges_selectable:\n\t\t\tsmallest_dist = 10e10\n\t\t\tclosest_edge = None\n\t\t\tfor edge in self._sketch.get_edges():\n\t\t\t\tdist = edge.distance(Vertex(x, y, 0))\n\t\t\t\tif dist < smallest_dist:\n\t\t\t\t\tsmallest_dist = dist\n\t\t\t\t\tclosest_edge = edge\n\t\t\tif smallest_dist * scale < 10:\n\t\t\t\tself._edge_hover = closest_edge\n\t\t\t\tupdate_view = True\n\t\tif self._areas_selectable and self._edge_hover is None:\n\t\t\tfor area in self._sketch.get_areas():\n\t\t\t\tif area.inside(Vertex(x, y, 0)):\n\t\t\t\t\tself._area_hover = area\n\t\t\t\t\tupdate_view = True\n\t\t\t\t\tbreak\n\t\tif update_view:\n\t\t\tself.update()\n\n\tdef mousePressEvent(self, q_mouse_event):\n\t\tself.setFocus()\n\t\tposition = q_mouse_event.pos()\n\t\tif q_mouse_event.button() == 4:\n\t\t\treturn\n\t\tif q_mouse_event.button() == 1:\n\t\t\tpass\n\n\t\tif self._kp_hover is not None and self._keypoints_selectable:\n\t\t\tself._selected_kps.clear()\n\t\t\tself._selected_kps.append(self._kp_hover)\n\t\t\tif self._change_listener is not None:\n\t\t\t\tself._change_listener.on_kp_selected(self._kp_hover)\n\t\t\tself.update()\n\n\t\tif self._edge_hover is not None and self._edges_selectable:\n\t\t\tself._selected_edges.clear()\n\t\t\tself._selected_edges.append(self._edge_hover)\n\t\t\tif self._change_listener is not None:\n\t\t\t\tself._change_listener.on_edge_selected(self._edge_hover)\n\t\t\tself.update()\n\n\t\tif self._area_hover is not None and self._areas_selectable and self._edge_hover is None:\n\t\t\tself._selected_areas.clear()\n\t\t\tself._selected_areas.append(self._area_hover)\n\t\t\tif self._change_listener is not None:\n\t\t\t\tself._change_listener.on_area_selected(self._area_hover)\n\t\t\tself.update()\n\n\t@property\n\tdef show_areas(self):\n\t\treturn self._show_areas\n\n\t@show_areas.setter\n\tdef show_areas(self, value):\n\t\tself._show_areas = value\n\t\tself.update()\n\n\t@property\n\tdef areas_selectable(self):\n\t\treturn self._areas_selectable\n\n\t@areas_selectable.setter\n\tdef areas_selectable(self, value):\n\t\tself._areas_selectable = value\n\n\tdef set_sketch(self, sketch):\n\t\tself._sketch = sketch\n\t\tself.update()\n\n\t@property\n\tdef edges_selectable(self):\n\t\treturn self._edges_selectable\n\n\t@edges_selectable.setter\n\tdef edges_selectable(self, value):\n\t\tself._edges_selectable = value\n\n\t@property\n\tdef keypoints_selectable(self):\n\t  return self._keypoints_selectable\n\n\t@keypoints_selectable.setter\n\tdef keypoints_selectable(self, value):\n\t\tself._keypoints_selectable = value\n\n\tdef set_change_listener(self, change_listener):\n\t\tself._change_listener = change_listener\n\n\tdef paintEvent(self, event):\n\t\tqp = QPainter()\n\t\tqp.begin(self)\n\t\tqp.setRenderHint(QPainter.Antialiasing)\n\n\t\tqp.fillRect(event.rect(), QColor(255, 255, 255))\n\t\thalf_width = self.width() / 2\n\t\thalf_height = self.height() / 2\n\t\tcenter = Vertex(half_width, half_height)\n\t\tif self._sketch is not None:\n\t\t\tlimits = self._sketch.get_limits()\n\t\t\tsketch_width = limits[2] - limits[0]\n\t\t\tsketch_height = limits[3] - limits[1]\n\t\t\tscale_x = self.width() / sketch_width\n\t\t\tscale_y = self.height() / sketch_height\n\t\t\tscale = min(scale_x, scale_y) * 0.9\n\n\t\t\tpens = create_pens(self._doc, 6000/scale, QColor(0, 0, 0))\n\t\t\tpens_hover = create_pens(self._doc, 6000/scale, QColor(100, 100, 200), 1)\n\t\t\tpens_select_high = create_pens(self._doc, 6000/scale, QColor(255, 0, 0), 2)\n\t\t\tpens_select = create_pens(self._doc, 6000/scale, QColor(255, 255, 255))\n\n\t\t\toffset = Vertex(-limits[0] - sketch_width / 2, -limits[1] - sketch_height / 2)\n\t\t\tdraw_sketch(qp, self._sketch, scale, 1/scale, offset, center, 0, pens, {})\n\n\t\t\tqp.save()\n\t\t\tqp.translate(center.x, center.y)\n\t\t\tqp.scale(scale, scale)\n\t\t\tqp.translate(offset.x, -offset.y)\n\t\t\tfor edge in self._selected_edges:\n\t\t\t\tdraw_edge(edge, qp, pens_select_high, None)\n\t\t\tfor edge in self._selected_edges:\n\t\t\t\tdraw_edge(edge, qp, pens_select, None)\n\t\t\tif self._keypoints_selectable:\n\t\t\t\tqp.setPen(pens['default'])\n\t\t\t\tfor kp in self._sketch.get_keypoints():\n\t\t\t\t\tdraw_kp(qp, kp, scale)\n\t\t\t\tif self._kp_hover:\n\t\t\t\t\tqp.setPen(pens_hover['default'])\n\t\t\t\t\tdraw_kp(qp, self._kp_hover, scale)\n\t\t\tif len(self._selected_kps) > 0:\n\t\t\t\tqp.setPen(pens_select_high['default'])\n\t\t\t\tfor kp in self._selected_kps:\n\t\t\t\t\tdraw_kp(qp, kp, scale)\n\t\t\tif self._edge_hover is not None:\n\t\t\t\tdraw_edge(self._edge_hover, qp, pens_hover, None)\n\t\t\tif self._show_areas:\n\t\t\t\tqp.setPen(pens['default'])\n\t\t\t\tfor area in self._sketch.get_areas():\n\t\t\t\t\tdraw_area(area, qp, True, QBrush(QColor(150, 150, 150, 80)), 1/scale, None)\n\t\t\t\tfor area in self._selected_areas:\n\t\t\t\t\tdraw_area(area, qp, True, QBrush(QColor(150, 150, 200, 150)), 1/scale, None)\n\t\t\t\tif self._area_hover is not None:\n\t\t\t\t\tdraw_area(self._area_hover, qp, True, QBrush(QColor(150, 150, 200, 80)), 1/scale, None)\n\t\t\tqp.restore()\n\n\t\tqp.end()\nBusiness/ParameterActions.py\ndef remove_standard(parameters_object: Parameters, standard_name):\n\tparameters_object.remove_standard(standard_name)\nData/Proformer.py\nclass ProformerType(Enum):\n\tCircular = 0\n\tDiamond = 1\n\tTriangular = 2\n\tSquare = 3\n\tRectangular = 4\n\tMirror = 5\n\tMirrorX = 6\n\tMirrorY = 7\n\tMirrorXY = 8\nBusiness/ParameterActions.py\ndef create_new_standard(parameters_object: Parameters, standard_name):\n\tparameters_object.make_standard(standard_name)\nData/Axis.py\nclass Axis(NamedObservableObject, IdObject):\n\tdef __init__(self, document, name=\"New Axis\"):\n\t\tNamedObservableObject.__init__(self, name)\n\t\tIdObject.__init__(self)\n\t\tself._doc = document\n\t\tself._sketch = None\n\t\tself._edge = None\n\t\tself._pm = None\n\t\tself._origo = Vertex()\n\t\tself._direction = Vertex(1, 1, 1)\n\n\t@property\n\tdef direction(self):\n\t\treturn self._direction\n\n\t@property\n\tdef origo(self):\n\t\treturn self._origo\n\n\tdef on_edge_changed(self, event):\n\t\told_value = self._origo\n\t\tkps = self._edge.get_end_key_points()\n\t\tself._origo.x = kps[0].x\n\t\tself._origo.y = kps[0].y\n\t\tself._origo.z = kps[0].z\n\t\tself._direction.xyz = kps[1].xyz - kps[0].xyz\n\t\tself.changed(ChangeEvent(self, ChangeEvent.ObjectChanged, event.sender))\n\t\tself.changed(ValueChangeEvent(self, 'origo', old_value, self._origo))\n\t\tself._pm = None\n\t\tif event.type == ChangeEvent.Deleted:\n\t\t\tevent.sender.remove_change_handler(self.on_edge_changed)\n\t\t\tself._sketch = None\n\n\tdef set_edge_governor(self, edge, sketch):\n\t\tif self._edge is not None:\n\t\t\tself._edge.remove_change_handler(self.on_edge_changed)\n\t\tself._edge = edge\n\t\tself._sketch = sketch\n\t\tkps = self._edge.get_end_key_points()\n\t\tself._origo.x = kps[0].x\n\t\tself._origo.y = kps[0].y\n\t\tself._origo.z = kps[0].z\n\t\tself._direction.xyz = kps[1].xyz - kps[0].xyz\n\t\tself._edge.add_change_handler(self.on_edge_changed)\n\n\tdef get_projection_matrix(self):\n\t\tif self._pm is None:\n\t\t\tif self._direction.z == 0:\n\t\t\t\tangle = atan2(self._direction.y, self._direction.x) + pi / 2\n\t\t\telif self._direction.x == 0:\n\t\t\t\tangle = atan2(self._direction.y, self._direction.z) + pi / 2\n\t\t\telse:\n\t\t\t\tangle = atan2(self._direction.x, self._direction.z) + pi / 2\n\t\t\td2 = np.array([cos(angle), sin(angle), self._direction.z])\n\t\t\tcp = np.cross(self._direction.xyz, d2)\n\t\t\td2 = cp / np.linalg.norm(cp)\n\t\t\tcp = np.cross(self._direction.xyz, d2)\n\t\t\td3 = cp / np.linalg.norm(cp)\n\t\t\td1 = self._direction.xyz / np.linalg.norm(self._direction.xyz)\n\t\t\tpm = np.array([d1, d2, d3])\n\t\t\tself._pm = pm\n\t\telse:\n\t\t\tpm = self._pm\n\t\treturn pm\n\n\tdef distance(self, point):\n\t\tp1 = self._origo.xyz\n\t\tpm = self.get_projection_matrix()\n\t\tnewp = pm.dot(point.xyz - p1)\n\t\tnewp[0] = 0.0\n\t\tdistance = np.linalg.norm(newp)\n\t\treturn distance\n\n\tdef distance_xyz(self, point):\n\t\tp1 = self._origo.xyz\n\t\tpm = self.get_projection_matrix()\n\t\tnewp = pm.dot(point - p1)\n\t\tnewp[0] = 0.0\n\t\tdistance = np.linalg.norm(newp)\n\t\treturn distance\n\n\tdef project_point(self, point):\n\t\tpm = self.get_projection_matrix()\n\t\tnewp = pm.dot(point.xyz - self._origo.xyz)\n\t\treturn newp[0] * pm[0] + self._origo.xyz\n\n\tdef project_point_xyz(self, point):\n\t\tpm = self.get_projection_matrix()\n\t\tnewp = pm.dot(point - self._origo.xyz)\n\t\treturn newp[0] * pm[0] + self._origo.xyz\n\n\t@property\n\tdef _sketch_uid(self):\n\t\tif self._sketch is None:\n\t\t\treturn None\n\t\telse:\n\t\t\treturn self._sketch.uid\n\n\t@property\n\tdef _edge_uid(self):\n\t\tif self._edge is None:\n\t\t\treturn None\n\t\telse:\n\t\t\treturn self._edge.uid\n\n\tdef serialize_json(self):\n\t\treturn {\n\t\t\t'no': NamedObservableObject.serialize_json(self),\n\t\t\t'uid': IdObject.serialize_json(self),\n\t\t\t'sketch_uid': self._sketch_uid,\n\t\t\t'edge_uid': self._edge_uid,\n\t\t\t'origo': self._origo,\n\t\t\t'direction': self._direction\n\t\t}\n\n\t@staticmethod\n\tdef deserialize(data, document):\n\t\taxis = Axis(document)\n\t\tif data is not None:\n\t\t\taxis.deserialize_data(data)\n\t\treturn axis\n\n\tdef deserialize_data(self, data):\n\t\tIdObject.deserialize_data(self, data.get('uid', {'uid': self.uid}))\n\t\tNamedObservableObject.deserialize_data(self, data['no'])\n\t\tsketch_uid = data['sketch_uid']\n\t\tif sketch_uid is not None:\n\t\t\tself._sketch = self._doc.get_geometries().get_geometry(sketch_uid)\n\t\tedge_uid = data['edge_uid']\n\t\tif self._sketch is not None and edge_uid is not None:\n\t\t\tself._edge = self._sketch.get_edge(edge_uid)\n\t\t\tself._edge.add_change_handler(self.on_edge_changed)\n\t\tself._origo = Vertex.deserialize(data['origo'])\n\t\tself._direction = Vertex.deserialize(data['direction'])\n\t\tif self._edge is not None:\n\t\t\tself.on_edge_changed(ChangeEvent(self, ChangeEvent.ObjectChanged, self._edge))\nGUI/init.py\ndef formula_from_locale(formula):\n\tlocale = QLocale()\n\tif locale.decimalPoint() == \",\":\n\t\treturn formula.replace(\",\", \".\").replace(\";\", \",\")\n\treturn formula\nGUI/init.py\ndef tr(string, context_name='app'):\n\tvalue = QCoreApplication.translate(context_name, string)\n\ttry:\n\t\tcontext = contexts[context_name]\n\texcept KeyError:\n\t\tcontexts[context_name] = {}\n\t\tcontext = contexts[context_name]\n\tcontext[string] = value\n\treturn value\nGUI/init.py\ndef gui_scale():\n\tscreen = QApplication.screens()[0];\n\tdpi = screen.logicalDotsPerInch()\n\treturn dpi / 96\nData/Parameters.py\nclass Parameters(ParametersBase):\n\tdef __init__(self, name, parent=None):\n\t\tParametersBase.__init__(self, name)\n\t\tself._parameter_list = []\n\t\tself._params = {}\n\t\tself._parent = parent\n\t\tself._custom_name_getter = None\n\t\tself._standards = {}\n\t\tself._current_standard_name = \"Normal\"\n\t\tself._current_type_name = \"Default\"\n\t\tself._current_type = self.make_type(self._current_standard_name, self._current_type_name)\n\n\t@property\n\tdef document(self):\n\t\tif self._parent is None:\n\t\t\treturn self\n\t\telse:\n\t\t\treturn self._parent.document\n\n\t@property\n\tdef parent(self):\n\t\treturn self._parent\n\n\tdef param_in_current_type(self, param):\n\t\tif self._current_type is None:\n\t\t\treturn False\n\t\treturn param.uid in self._current_type\n\n\tdef make_standard(self, name):\n\t\tstandard = {}\n\t\tself._standards[name] = standard\n\t\treturn standard\n\n\t@property\n\tdef standards(self):\n\t\treturn list(self._standards.keys())\n\n\t@property\n\tdef standard(self):\n\t\treturn self._current_standard_name\n\n\t@standard.setter\n\tdef standard(self, value):\n\t\tif value in self._standards:\n\t\t\tself._current_standard_name = value\n\t\t\tself._current_type_name = \"\"\n\t\t\tself._current_type = None\n\n\t@property\n\tdef types(self):\n\t\treturn self._standards[self._current_standard_name].keys()\n\n\tdef get_types_from_standard(self, standard):\n\t\tif standard in self._standards:\n\t\t\treturn list(self._standards[standard].keys())\n\t\treturn []\n\n\tdef make_type(self, standard_name, type_name):\n\t\tif standard_name not in self._standards:\n\t\t\tstandard = {}\n\t\t\tself._standards[standard_name] = standard\n\t\telse:\n\t\t\tstandard = self._standards[standard_name]\n\t\ttype = {}\n\t\tstandard[type_name] = type\n\t\treturn type\n\n\t@property\n\tdef type(self):\n\t\treturn self._current_type_name\n\n\t@type.setter\n\tdef type(self, type_name):\n\t\tif type_name in self._standards[self._current_standard_name]:\n\t\t\tself._current_type = self._standards[self._current_standard_name][type_name]\n\t\t\tself._current_type_name = type_name\n\t\t\tif self._current_type is None:\n\t\t\t\treturn\n\t\t\tfor param_definition_tuple in self._current_type.items():\n\t\t\t\tuid = param_definition_tuple[0]\n\t\t\t\tval = param_definition_tuple[1]\n\t\t\t\tif uid in self._params:\n\t\t\t\t\tparam = self._params[uid]\n\t\t\t\t\told_value = param.value\n\t\t\t\t\tparam.internal_formula = val['if']\n\t\t\t\t\tparam.internal_value = param.evaluate(None)\n\t\t\t\t\tchange_object = {\n\t\t\t\t\t\t'new value': param.value,\n\t\t\t\t\t\t'old value': old_value,\n\t\t\t\t\t\t'instance': None\n\t\t\t\t\t}\n\t\t\t\t\tparam.changed(ChangeEvent(param, ChangeEvent.ValueChanged, change_object))\n\t\t\tself.changed(ChangeEvent(self, ChangeEvent.ObjectChanged, self))\n\n\tdef _add_parameter_object(self, param):\n\t\tparam.add_change_handler(self.on_parameter_changed)\n\t\tself.changed(ChangeEvent(self, ChangeEvent.BeforeObjectAdded, param))\n\t\tself._params[param.uid] = param\n\t\tself._parameter_list.append(param.uid)\n\t\tself.changed(ChangeEvent(self, ChangeEvent.ObjectAdded, param))\n\n\tdef _remove_parameter_object(self, uid):\n\t\tif uid in self._params:\n\t\t\tself._params.pop(uid)\n\n\tdef get_parameter_by_uid(self, uid) -> Parameter:\n\t\tif uid in self._params:\n\t\t\treturn self._params[uid]\n\t\telif self._parent is not None:\n\t\t\treturn self._parent.get_parameter_by_uid(uid)\n\t\telse:\n\t\t\treturn None\n\n\tdef get_parameter_by_name(self, name) -> Parameter:\n\t\tparam = None\n\t\tfor prm in self._params.values():\n\t\t\tif prm.name == name:\n\t\t\t\tparam = prm\n\t\tif param is None and self._custom_name_getter is not None:\n\t\t\tparam = self._custom_name_getter(name)\n\t\tif param is None and self._parent is not None:\n\t\t\tparam = self._parent.get_parameter_by_name(name)\n\n\t\treturn param\n\n\tdef get_all_local_parameters(self):\n\t\tparams = list(self._params.items())\n\t\treturn params\n\n\tdef get_all_parameters(self):\n\t\tparams = list(self._params.values())\n\t\tif self._parent is not None:\n\t\t\tparams.extend(self._parent.get_all_parameters())\n\t\treturn params\n\n\tdef create_parameter(self, name=None, value=0.0):\n\t\tif name is None:\n\t\t\tif self._parent is None:\n\t\t\t\tname = \"Global\" + str(len(self._parameter_list))\n\t\t\telse:\n\t\t\t\tname = self.name + str(len(self._parameter_list))\n\t\tparam = Parameter(self, name, value)\n\t\tself._add_parameter_object(param)\n\t\treturn param\n\n\tdef delete_parameter(self, uid):\n\t\tparam = self.get_parameter_by_uid(uid)\n\t\tif param is not None:\n\t\t\tparam.remove_change_handler(self.on_parameter_changed)\n\t\t\tself.changed(ChangeEvent(self, ChangeEvent.BeforeObjectRemoved, param))\n\t\t\tself._parameter_list.remove(uid)\n\t\t\tself.changed(ChangeEvent(self, ChangeEvent.ObjectRemoved, param))\n\t\t\tself._remove_parameter_object(uid)\n\t\t\tparam.changed(ChangeEvent(param, ChangeEvent.Deleted, param))\n\n\tdef delete_parameters(self, params):\n\t\tfor param in params:\n\t\t\tif param.uid in self._parameter_list:\n\t\t\t\tself.delete_parameter(param.uid)\n\n\tdef on_parameter_changed(self, event):\n\t\tparam = event.sender\n\t\tself.changed(ChangeEvent(self, event.type, event.sender))\n\t\tif self._current_type is not None and param.uid in self._params:\n\t\t\tif 'instance' in event.object and 'new formula' in event.object:\n\t\t\t\tif event.object['instance'] is None and event.object['new formula'] != event.object['old formula']:\n\n\t\t\t\t\tif param.uid in self._current_type:\n\t\t\t\t\t\tpredef = self._current_type[param.uid]\n\t\t\t\t\telse:\n\t\t\t\t\t\tpredef = {}\n\t\t\t\t\t\tself._current_type[param.uid] = predef\n\t\t\t\t\tpredef['if'] = param.internal_formula\n\t\t\t\t\tpredef['iv'] = param.internal_value\n\n\tdef get_index_of(self, parameter):\n\t\tif parameter.uid in self._parameter_list:\n\t\t\treturn self._parameter_list.index(parameter.uid)\n\t\telse:\n\t\t\treturn -1\n\n\t@property\n\tdef length(self):\n\t\treturn len(self._parameter_list)\n\n\t@property\n\tdef length_all(self):\n\t\tif self._parent is not None:\n\t\t\treturn self._parent.length_all + self.length\n\t\telse:\n\t\t\treturn self.length\n\n\tdef get_parameter_item(self, index):\n\t\tif index >= self.length:\n\t\t\treturn self._parent.get_parameter_item(index - self.length)\n\t\telse:\n\t\t\tuid = self._parameter_list[index]\n\t\tparam = self.get_parameter_by_uid(uid)\n\t\treturn param\n\n\tdef serialize_json(self):\n\t\treturn {\n\t\t\t'name': self._name,\n\t\t\t'params': self._params,\n\t\t\t'parameter_list': self._parameter_list,\n\t\t\t'pps': self._standards,\n\t\t\t'csn': self._current_standard_name,\n\t\t\t'ctn': self._current_type_name\n\t\t}\n\n\t@staticmethod\n\tdef deserialize(data, parent):\n\t\tparam = Parameters(parent)\n\t\tparam.deserialize_data(data)\n\t\treturn param\n\n\tdef deserialize_data(self, data):\n\t\tself._parameter_list = data.get('parameter_list', [])\n\t\tself._name = data.get('name', 'name missing')\n\t\tself._standards = data.get('pps', self._standards)\n\t\tself._current_standard_name = data.get(\"csn\", self._current_standard_name)\n\t\tself._current_type_name = data.get(\"ctn\", self._current_type_name)\n\t\tself._current_type = self._standards[self._current_standard_name][self._current_type_name]\n\t\tfor param_data in data.get('params', {}).items():\n\t\t\tparam = Parameter.deserialize(param_data[1], self)\n\t\t\tself._params[param.uid] = param\n\t\t\tparam.add_change_handler(self.on_parameter_changed)\n\n\t\tfor param_tuple in self._params.items():\n\t\t\tif param_tuple[1].formula != '':\n\t\t\t\ttry:\n\t\t\t\t\tparam_tuple[1].value = param_tuple[1].formula\n\t\t\t\texcept Exception as e:\n\t\t\t\t\tpass\nBusiness/ParameterActions.py\ndef remove_type(parameters_object: Parameters, standard_name, type_name):\n\tparameters_object.remove_type(standard_name, type_name)\nBusiness/ParameterActions.py\ndef create_new_type(parameters_object: Parameters, standard_name, type_name):\n\tparameters_object.make_type(standard_name, type_name)\n", "answers": ["\t\tself._caption_label.setMinimumWidth(100*gui_scale())"], "length": 2012, "dataset": "repobench-p_e", "language": "python", "all_classes": null, "_id": "4d2668d05f3f636a1bb6b0389a03b382f3f3e06b19dec51d"}
{"input": "import re\nimport struct\nfrom itertools import zip_longest\nfrom mutagen._tags import Tags\nfrom mutagen._util import DictProxy, convert_error, read_full\nfrom ._util import BitPaddedInt, unsynch, ID3JunkFrameError, \\\n    ID3EncryptionUnsupportedError, is_valid_frame_id, error, \\\n    ID3NoHeaderError, ID3UnsupportedVersionError, ID3SaveConfig\nfrom ._frames import TDRC, APIC, TDOR, TIME, TIPL, TORY, TDAT, Frames_2_2, \\\n    TextFrame, TYER, Frame, IPLS, Frames\n                timestamps.append(timestamp)\n        if timestamps and \"TDRC\" not in self:\n            self.add(TDRC(encoding=0, text=timestamps))\n\n        # TORY can be the first part of a TDOR.\n        if \"TORY\" in self:\n            f = self.pop(\"TORY\")\n            if \"TDOR\" not in self:\n                try:\n                    self.add(TDOR(encoding=0, text=str(f)))\n                except UnicodeDecodeError:\n                    pass\n\n        # IPLS is now TIPL.\n        if \"IPLS\" in self:\n            f = self.pop(\"IPLS\")\n            if \"TIPL\" not in self:\n                self.add(TIPL(encoding=f.encoding, people=f.people))\n\n        # These can't be trivially translated to any ID3v2.4 tags, or\n        # should have been removed already.\n        for key in [\"RVAD\", \"EQUA\", \"TRDA\", \"TSIZ\", \"TDAT\", \"TIME\"]:\n            if key in self:\n                del(self[key])\n\n        # Recurse into chapters\n        for f in self.getall(\"CHAP\"):\n            f.sub_frames.update_to_v24()\n        for f in self.getall(\"CTOC\"):\n            f.sub_frames.update_to_v24()\n\n    def update_to_v23(self):\n        \"\"\"Convert older (and newer) tags into an ID3v2.3 tag.\n\n        This updates incompatible ID3v2 frames to ID3v2.3 ones. If you\n        intend to save tags as ID3v2.3, you must call this function\n        at some point.\n\n        If you want to to go off spec and include some v2.4 frames\n        in v2.3, remove them before calling this and add them back afterwards.\n        \"\"\"\n\n        self.__update_common()\n\n        # TMCL, TIPL -> TIPL\n        if \"TIPL\" in self or \"TMCL\" in self:\n            people = []\n            if \"TIPL\" in self:\n                f = self.pop(\"TIPL\")\n                people.extend(f.people)\n            if \"TMCL\" in self:\n                f = self.pop(\"TMCL\")\n                people.extend(f.people)\n            if \"IPLS\" not in self:\n                self.add(IPLS(encoding=f.encoding, people=people))\n\n        # TDOR -> TORY\n        if \"TDOR\" in self:\n            f = self.pop(\"TDOR\")\n            if f.text:\n                d = f.text[0]\n                if d.year and \"TORY\" not in self:\n                    self.add(TORY(encoding=f.encoding, text=\"%04d\" % d.year))\n\n        # TDRC -> TYER, TDAT, TIME\n        if \"TDRC\" in self:\n            f = self.pop(\"TDRC\")\n            if f.text:\n                d = f.text[0]\n                if d.year and \"TYER\" not in self:\n                    self.add(TYER(encoding=f.encoding, text=\"%04d\" % d.year))\n                if d.month and d.day and \"TDAT\" not in self:\n                    self.add(TDAT(encoding=f.encoding,\n                                  text=\"%02d%02d\" % (d.day, d.month)))\n                if d.hour and d.minute and \"TIME\" not in self:\n                    self.add(TIME(encoding=f.encoding,\n                                  text=\"%02d%02d\" % (d.hour, d.minute)))\n\n        # New frames added in v2.4\n        v24_frames = [\n            'ASPI', 'EQU2', 'RVA2', 'SEEK', 'SIGN', 'TDEN', 'TDOR',\n            'TDRC', 'TDRL', 'TDTG', 'TIPL', 'TMCL', 'TMOO', 'TPRO',\n            'TSOA', 'TSOP', 'TSOT', 'TSST',\n        ]\n\n        for key in v24_frames:\n            if key in self:\n                del(self[key])\n\n        # Recurse into chapters\n        for f in self.getall(\"CHAP\"):\n            f.sub_frames.update_to_v23()\n        for f in self.getall(\"CTOC\"):\n            f.sub_frames.update_to_v23()\n\n    def _copy(self):\n        \"\"\"Creates a shallow copy of all tags\"\"\"\n\n        items = self.items()\n        subs = {}\n        for f in (self.getall(\"CHAP\") + self.getall(\"CTOC\")):\n            subs[f.HashKey] = f.sub_frames._copy()\n        return (items, subs)\n\n    def _restore(self, value):\n        \"\"\"Restores the state copied with _copy()\"\"\"\n\n        items, subs = value\n        self.clear()\n        for key, value in items:\n            self[key] = value\n            if key in subs:\n                value.sub_frames._restore(subs[key])\n\n\ndef save_frame(frame, name=None, config=None):\n    if config is None:\n        config = ID3SaveConfig()\n\n    flags = 0\n", "context": "mutagen/id3/_frames.py\ndef _bytes2key(b):\n    def __init__(self, *args, **kwargs):\n    def __setattr__(self, name, value):\n    def _setattr(self, name, value):\n    def _to_other(self, other):\n    def _merge_frame(self, other):\n    def _upgrade_frame(self):\n    def _get_v23_frame(self, **kwargs):\n    def HashKey(self):\n    def FrameID(self):\n    def __repr__(self):\n    def _readData(self, id3, data):\n    def _writeData(self, config=None):\n    def pprint(self):\n    def _pprint(self):\n    def _fromData(cls, header, tflags, data):\n    def __hash__(self: object):\n    def HashKey(self):\n    def __eq__(self, other):\n    def _pprint(self):\n    def HashKey(self):\n    def __eq__(self, other):\n    def _pprint(self):\n    def __bytes__(self):\n    def __str__(self):\n    def __eq__(self, other):\n    def __getitem__(self, item):\n    def __iter__(self):\n    def append(self, value):\n    def extend(self, value):\n    def _merge_frame(self, other):\n    def _pprint(self):\n    def __pos__(self):\n    def __pos__(self):\n    def __bytes__(self):\n    def __str__(self):\n    def _pprint(self):\n    def __bytes__(self):\n    def __str__(self):\n    def __eq__(self, other):\n    def _pprint(self):\n    def HashKey(self):\n    def __get_genres(self):\n    def __set_genres(self, genres):\n    def __decode(self, value):\n    def _pprint(self):\n    def HashKey(self):\n    def _pprint(self):\n    def HashKey(self):\n    def __eq__(self, other):\n    def __eq__(self, other):\n    def __eq__(self, other):\n    def __eq__(self, other):\n    def __eq__(self, other):\n    def HashKey(self):\n    def __bytes__(self):\n    def __str__(self):\n    def __eq__(self, other):\n    def _pprint(self):\n    def HashKey(self):\n    def _pprint(self):\n    def __eq__(self, other):\n    def __str__(self):\n    def __bytes__(self):\n    def HashKey(self):\n    def _pprint(self):\n    def HashKey(self):\n    def __eq__(self, other):\n    def __str__(self):\n    def __eq__(self, other):\n    def HashKey(self):\n    def __eq__(self, other):\n    def __eq__(self, other):\n    def __eq__(self, other):\n    def HashKey(self):\n    def _merge_frame(self, other):\n    def _pprint(self):\n    def __eq__(self, other):\n    def __pos__(self):\n    def _pprint(self):\n    def __eq__(self, other):\n    def __pos__(self):\n    def _pprint(self):\n    def HashKey(self):\n    def __eq__(self, other):\n    def __pos__(self):\n    def _pprint(self):\n    def HashKey(self):\n    def __eq__(self, other):\n    def __eq__(self, other):\n    def __pos__(self):\n    def HashKey(self):\n    def __bytes__(self):\n    def __str__(self):\n    def __eq__(self, other):\n    def HashKey(self):\n    def __eq__(self, other):\n    def __pos__(self):\n    def __eq__(self, other):\n    def HashKey(self):\n    def __eq__(s, o):\n    def _pprint(self):\n    def HashKey(self):\n    def __bytes__(self):\n    def __str__(self):\n    def __eq__(self, other):\n    def _pprint(self):\n    def __bytes__(self):\n    def __str__(self):\n    def __eq__(self, other):\n    def HashKey(self):\n    def __eq__(self, other):\n    def HashKey(self):\n    def __bytes__(self):\n    def __eq__(self, other):\n    def HashKey(self):\n    def __pos__(self):\n    def __bytes__(self):\n    def __str__(self):\n    def __eq__(self, other):\n    def HashKey(self):\n    def __bytes__(self):\n    def __eq__(self, other):\n    def _pprint(self):\n    def HashKey(self):\n    def __bytes__(self):\n    def __eq__(self, other):\n    def __pos__(self):\n    def __eq__(self, other):\n    def __eq__(self, other):\n    def _to_other(self, other):\n    def _to_other(self, other):\n    def __eq__(self, other):\n    def _to_other(self, other):\nclass Frame(object):\nclass CHAP(Frame):\nclass CTOC(Frame):\nclass TextFrame(Frame):\nclass NumericTextFrame(TextFrame):\nclass NumericPartTextFrame(TextFrame):\nclass TimeStampTextFrame(TextFrame):\nclass UrlFrame(Frame):\nclass UrlFrameU(UrlFrame):\nclass TALB(TextFrame):\nclass TBPM(NumericTextFrame):\nclass TCOM(TextFrame):\nclass TCON(TextFrame):\nclass TCOP(TextFrame):\nclass TCMP(NumericTextFrame):\nclass TDAT(TextFrame):\nclass TDEN(TimeStampTextFrame):\nclass TDES(TextFrame):\nclass TKWD(TextFrame):\nclass TCAT(TextFrame):\nclass MVNM(TextFrame):\nclass MVN(MVNM):\nclass MVIN(NumericPartTextFrame):\nclass MVI(MVIN):\nclass GRP1(TextFrame):\nclass GP1(GRP1):\nclass TDOR(TimeStampTextFrame):\nclass TDLY(NumericTextFrame):\nclass TDRC(TimeStampTextFrame):\nclass TDRL(TimeStampTextFrame):\nclass TDTG(TimeStampTextFrame):\nclass TENC(TextFrame):\nclass TEXT(TextFrame):\nclass TFLT(TextFrame):\nclass TGID(TextFrame):\nclass TIME(TextFrame):\nclass TIT1(TextFrame):\nclass TIT2(TextFrame):\nclass TIT3(TextFrame):\nclass TKEY(TextFrame):\nclass TLAN(TextFrame):\nclass TLEN(NumericTextFrame):\nclass TMED(TextFrame):\nclass TMOO(TextFrame):\nclass TOAL(TextFrame):\nclass TOFN(TextFrame):\nclass TOLY(TextFrame):\nclass TOPE(TextFrame):\nclass TORY(NumericTextFrame):\nclass TOWN(TextFrame):\nclass TPE1(TextFrame):\nclass TPE2(TextFrame):\nclass TPE3(TextFrame):\nclass TPE4(TextFrame):\nclass TPOS(NumericPartTextFrame):\nclass TPRO(TextFrame):\nclass TPUB(TextFrame):\nclass TRCK(NumericPartTextFrame):\nclass TRDA(TextFrame):\nclass TRSN(TextFrame):\nclass TRSO(TextFrame):\nclass TSIZ(NumericTextFrame):\nclass TSO2(TextFrame):\nclass TSOA(TextFrame):\nclass TSOC(TextFrame):\nclass TSOP(TextFrame):\nclass TSOT(TextFrame):\nclass TSRC(TextFrame):\nclass TSSE(TextFrame):\nclass TSST(TextFrame):\nclass TYER(NumericTextFrame):\nclass TXXX(TextFrame):\nclass WCOM(UrlFrameU):\nclass WCOP(UrlFrame):\nclass WFED(UrlFrame):\nclass WOAF(UrlFrame):\nclass WOAR(UrlFrameU):\nclass WOAS(UrlFrame):\nclass WORS(UrlFrame):\nclass WPAY(UrlFrame):\nclass WPUB(UrlFrame):\nclass WXXX(UrlFrame):\nclass PairedTextFrame(Frame):\nclass TIPL(PairedTextFrame):\nclass TMCL(PairedTextFrame):\nclass IPLS(TIPL):\nclass BinaryFrame(Frame):\nclass MCDI(BinaryFrame):\nclass ETCO(Frame):\nclass MLLT(Frame):\nclass SYTC(Frame):\nclass USLT(Frame):\nclass SYLT(Frame):\nclass COMM(TextFrame):\nclass RVA2(Frame):\nclass EQU2(Frame):\nclass RVAD(Frame):\nclass RVRB(Frame):\nclass APIC(Frame):\nclass PCNT(Frame):\nclass PCST(Frame):\nclass POPM(Frame):\nclass GEOB(Frame):\nclass RBUF(Frame):\nclass AENC(Frame):\nclass LINK(Frame):\nclass POSS(Frame):\nclass UFID(Frame):\nclass USER(Frame):\nclass OWNE(Frame):\nclass COMR(Frame):\nclass ENCR(Frame):\nclass GRID(Frame):\nclass PRIV(Frame):\nclass SIGN(Frame):\nclass SEEK(Frame):\nclass ASPI(Frame):\nclass UFI(UFID):\nclass TT1(TIT1):\nclass TT2(TIT2):\nclass TT3(TIT3):\nclass TP1(TPE1):\nclass TP2(TPE2):\nclass TP3(TPE3):\nclass TP4(TPE4):\nclass TCM(TCOM):\nclass TXT(TEXT):\nclass TLA(TLAN):\nclass TCO(TCON):\nclass TAL(TALB):\nclass TPA(TPOS):\nclass TRK(TRCK):\nclass TRC(TSRC):\nclass TYE(TYER):\nclass TDA(TDAT):\nclass TIM(TIME):\nclass TRD(TRDA):\nclass TMT(TMED):\nclass TFT(TFLT):\nclass TBP(TBPM):\nclass TCP(TCMP):\nclass TCR(TCOP):\nclass TPB(TPUB):\nclass TEN(TENC):\nclass TST(TSOT):\nclass TSA(TSOA):\nclass TS2(TSO2):\nclass TSP(TSOP):\nclass TSC(TSOC):\nclass TSS(TSSE):\nclass TOF(TOFN):\nclass TLE(TLEN):\nclass TSI(TSIZ):\nclass TDY(TDLY):\nclass TKE(TKEY):\nclass TOT(TOAL):\nclass TOA(TOPE):\nclass TOL(TOLY):\nclass TOR(TORY):\nclass TXX(TXXX):\nclass WAF(WOAF):\nclass WAR(WOAR):\nclass WAS(WOAS):\nclass WCM(WCOM):\nclass WCP(WCOP):\nclass WPB(WPUB):\nclass WXX(WXXX):\nclass IPL(IPLS):\nclass MCI(MCDI):\nclass ETC(ETCO):\nclass MLL(MLLT):\nclass STC(SYTC):\nclass ULT(USLT):\nclass SLT(SYLT):\nclass COM(COMM):\nclass RVA(RVAD):\nclass REV(RVRB):\nclass PIC(APIC):\nclass GEO(GEOB):\nclass CNT(PCNT):\nclass POP(POPM):\nclass BUF(RBUF):\nclass CRM(Frame):\nclass CRA(AENC):\nclass LNK(LINK):\n    FLAG23_ALTERTAG = 0x8000\n    FLAG23_ALTERFILE = 0x4000\n    FLAG23_READONLY = 0x2000\n    FLAG23_COMPRESS = 0x0080\n    FLAG23_ENCRYPT = 0x0040\n    FLAG23_GROUP = 0x0020\n    FLAG24_ALTERTAG = 0x4000\n    FLAG24_ALTERFILE = 0x2000\n    FLAG24_READONLY = 0x1000\n    FLAG24_GROUPID = 0x0040\n    FLAG24_COMPRESS = 0x0008\n    FLAG24_ENCRYPT = 0x0004\n    FLAG24_UNSYNCH = 0x0002\n    FLAG24_DATALEN = 0x0001\n    GENRES = GENRES\nmutagen/_util.py\ndef read_full(fileobj, size):\n    \"\"\"Like fileobj.read but raises IOError if not all requested data is\n    returned.\n\n    If you want to distinguish IOError and the EOS case, better handle\n    the error yourself instead of using this.\n\n    Args:\n        fileobj (fileobj)\n        size (int): amount of bytes to read\n    Raises:\n        IOError: In case read fails or not enough data is read\n    \"\"\"\n\n    if size < 0:\n        raise ValueError(\"size must not be negative\")\n\n    data = fileobj.read(size)\n    if len(data) != size:\n        raise IOError\n    return data\nmutagen/id3/_util.py\nclass error(MutagenError):\n    pass\nmutagen/id3/_util.py\nclass ID3UnsupportedVersionError(error, NotImplementedError):\n    pass\nmutagen/id3/_util.py\ndef is_valid_frame_id(frame_id):\n    return frame_id.isalnum() and frame_id.isupper()\nmutagen/id3/_util.py\nclass ID3NoHeaderError(error, ValueError):\n    pass\nmutagen/_util.py\nclass DictProxy(DictMixin):\n    def __init__(self, *args, **kwargs):\n        self.__dict = {}\n        super(DictProxy, self).__init__(*args, **kwargs)\n\n    def __getitem__(self, key):\n        return self.__dict[key]\n\n    def __setitem__(self, key, value):\n        self.__dict[key] = value\n\n    def __delitem__(self, key):\n        del(self.__dict[key])\n\n    def keys(self):\n        return self.__dict.keys()\nmutagen/id3/_util.py\nclass BitPaddedInt(int, _BitPaddedMixin):\n\n    def __new__(cls, value, bits=7, bigendian=True):\n\n        mask = (1 << (bits)) - 1\n        numeric_value = 0\n        shift = 0\n\n        if isinstance(value, int):\n            if value < 0:\n                raise ValueError\n            while value:\n                numeric_value += (value & mask) << shift\n                value >>= 8\n                shift += bits\n        elif isinstance(value, bytes):\n            if bigendian:\n                value = reversed(value)\n            for byte in bytearray(value):\n                numeric_value += (byte & mask) << shift\n                shift += bits\n        else:\n            raise TypeError\n\n        self = int.__new__(BitPaddedInt, numeric_value)\n\n        self.bits = bits\n        self.bigendian = bigendian\n        return self\nmutagen/_util.py\ndef convert_error(exc_src, exc_dest):\n    \"\"\"A decorator for reraising exceptions with a different type.\n    Mostly useful for IOError.\n\n    Args:\n        exc_src (type): The source exception type\n        exc_dest (type): The target exception type.\n    \"\"\"\n\n    def wrap(func):\n\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                return func(*args, **kwargs)\n            except exc_dest:\n                raise\n            except exc_src as err:\n                reraise(exc_dest, err, sys.exc_info()[2])\n\n        return wrapper\n\n    return wrap\nmutagen/_tags.py\nclass Tags(object):\n    \"\"\"`Tags` is the base class for many of the tag objects in Mutagen.\n\n    In many cases it has a dict like interface.\n    \"\"\"\n\n    __module__ = \"mutagen\"\n\n    def pprint(self):\n        \"\"\"\n        Returns:\n            text: tag information\n        \"\"\"\n\n        raise NotImplementedError\nmutagen/id3/_util.py\nclass ID3EncryptionUnsupportedError(error, NotImplementedError):\n    pass\nmutagen/id3/_util.py\nclass ID3JunkFrameError(error):\n    pass\nmutagen/id3/_util.py\nclass ID3SaveConfig(object):\n\n    def __init__(self, v2_version=4, v23_separator=None):\n        assert v2_version in (3, 4)\n        self.v2_version = v2_version\n        self.v23_separator = v23_separator\nmutagen/id3/_util.py\nclass unsynch(object):\n    @staticmethod\n    def decode(value):\n        fragments = bytearray(value).split(b'\\xff')\n        if len(fragments) > 1 and not fragments[-1]:\n            raise ValueError('string ended unsafe')\n\n        for f in fragments[1:]:\n            if (not f) or (f[0] >= 0xE0):\n                raise ValueError('invalid sync-safe string')\n\n            if f[0] == 0x00:\n                del f[0]\n\n        return bytes(bytearray(b'\\xff').join(fragments))\n\n    @staticmethod\n    def encode(value):\n        fragments = bytearray(value).split(b'\\xff')\n        for f in fragments[1:]:\n            if (not f) or (f[0] >= 0xE0) or (f[0] == 0x00):\n                f.insert(0, 0x00)\n        return bytes(bytearray(b'\\xff').join(fragments))\n", "answers": ["    if isinstance(frame, TextFrame):"], "length": 1608, "dataset": "repobench-p_e", "language": "python", "all_classes": null, "_id": "22cc5edeea2a251a75ec8950fd9bd0000a1dc7ee084af4ce"}
{"input": "import wx.html\nfrom history import *\nfrom controls import *\nfrom planning import *\nfrom cotisation import *\nfrom document_dialog import *\nfrom generation.contrat_accueil import ContratAccueilModifications, DevisAccueilModifications, FraisGardeModifications, AvenantContratAccueilModifications\nfrom config import config\nfrom database import Inscrit, TimeslotInscription, Fratrie, Referent, Parent, CongeInscrit, Inscription, Revenu\n#    Gertrude is free software; you can redistribute it and/or modify\n#    it under the terms of the GNU General Public License as published by\n#    the Free Software Foundation; either version 3 of the License, or\n#    (at your option) any later version.\n#\n#    Gertrude is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU General Public License for more details.\n#\n#    You should have received a copy of the GNU General Public License\n#    along with Gertrude; if not, see <http://www.gnu.org/licenses/>.\n\nfrom __future__ import unicode_literals\nfrom __future__ import print_function\n\n\n\nclass FraisGardePanel(wx.Panel):\n    def __init__(self, parent):\n        self.parent = parent\n        self.inscrit = None\n        wx.Panel.__init__(self, parent)\n        self.sizer = wx.BoxSizer(wx.VERTICAL)\n        sizer1 = wx.BoxSizer(wx.HORIZONTAL)\n        self.periodechoice = wx.Choice(self, size=(220, -1))\n        self.Bind(wx.EVT_CHOICE, self.onPeriodeChoice, self.periodechoice)\n        sizer1.Add(self.periodechoice, 0, wx.ALIGN_CENTER_VERTICAL)\n        self.frais_accueil_button = wx.Button(self, -1, \"Exporter\")\n        sizer1.Add(self.frais_accueil_button, 0, wx.ALIGN_CENTER_VERTICAL | wx.LEFT, 5)\n        self.Bind(wx.EVT_BUTTON, self.EvtGenerationFraisAccueil, self.frais_accueil_button)\n        if IsTemplateFile(\"Devis accueil.odt\"):\n            self.devis_button = wx.Button(self, -1, \"Générer un devis\")\n            sizer1.Add(self.devis_button, 0, wx.LEFT, 5)\n            self.Bind(wx.EVT_BUTTON, self.EvtGenerationDevis, self.devis_button)\n        else:\n            self.devis_button = None\n        self.contrat_button = wx.Button(self, -1, \"Générer le contrat\")\n        sizer1.Add(self.contrat_button, 0, wx.ALIGN_CENTER_VERTICAL | wx.LEFT, 5)\n        self.Bind(wx.EVT_BUTTON, self.EvtGenerationContrat, self.contrat_button)\n        if IsTemplateFile(\"Avenant contrat accueil.odt\"):\n            self.avenant_button = wx.Button(self, -1, \"Générer un avenant\")\n            sizer1.Add(self.avenant_button, 0, wx.LEFT, 5)\n            self.Bind(wx.EVT_BUTTON, self.EvtGenerationAvenant, self.avenant_button)\n        else:\n            self.avenant_button = None\n        self.sizer.Add(sizer1, 0, wx.ALL, 5)\n        self.html_window = wx.html.HtmlWindow(self, style=wx.SUNKEN_BORDER)\n        self.sizer.Add(self.html_window, 1, wx.EXPAND | wx.ALL-wx.TOP, 5)\n        self.SetSizer(self.sizer)\n\n    def EnableButtons(self, state):\n        for button in (self.contrat_button, self.devis_button, self.avenant_button, self.frais_accueil_button):\n            if button:\n                button.Enable(state)\n\n    def UpdatePage(self):\n        if self.inscrit is None:\n            self.html = '<html><body>Aucun inscrit s&eacute;lectionn&eacute; !</body></html>'\n            self.periodechoice.Disable()\n        elif not self.current_cotisation:\n            self.html = '<html><body>Aucune inscription !</body></html>'\n            self.periodechoice.Disable()\n        else:\n            context = self.current_cotisation[-1]\n            if isinstance(context, CotisationException):\n                error = '<br>'.join(context.errors)\n                self.html = \"<html><body><b>Les frais de garde ne peuvent être calcul&eacute;s pour la (les) raison(s) suivante(s) :</b><br>\" + error + \"</body></html>\"\n                self.EnableButtons(False)\n            else:\n                self.html = generateFraisGardeHtml(context)\n                self.EnableButtons(True)\n        self.html_window.SetPage(self.html)\n        \n    def SetInscrit(self, inscrit):\n        self.inscrit = inscrit\n        self.UpdateContents()\n    \n    def UpdateContents(self):\n        self.periodechoice.Clear()\n        if self.inscrit:\n            self.cotisations = GetCotisations(self.inscrit)\n            if len(self.cotisations) > 0:\n                index = len(self.cotisations) - 1\n                self.current_cotisation = self.cotisations[index]\n                for i, cotisation in enumerate(self.cotisations):\n                    if cotisation[0] and cotisation[0] <= today and (not cotisation[1] or today <= cotisation[1]):\n                        self.current_cotisation = cotisation\n                        index = i\n                        break\n                self.periodechoice.Enable()\n                self.EnableButtons(True)\n                for c in self.cotisations:\n                    self.periodechoice.Append(date2str(c[0]) + ' - ' + date2str(c[1]))\n                self.periodechoice.SetSelection(index)\n            else:\n                self.current_cotisation = None\n                self.periodechoice.Disable()\n                self.EnableButtons(False)\n        else:\n            self.current_cotisation = None\n            self.periodechoice.Disable()\n            self.EnableButtons(False)\n        self.UpdatePage()\n        \n    def onPeriodeChoice(self, evt):\n        ctrl = evt.GetEventObject()\n        self.current_cotisation = self.cotisations[ctrl.GetSelection()]\n        self.UpdatePage()\n        \n    def EvtGenerationFraisAccueil(self, _):\n        DocumentDialog(self, FraisGardeModifications(self.inscrit, self.current_cotisation[0])).ShowModal()\n\n    def EvtGenerationDevis(self, _):\n        DocumentDialog(self, DevisAccueilModifications(self.inscrit, self.current_cotisation[0])).ShowModal()\n\n    def EvtGenerationContrat(self, _):\n        DocumentDialog(self, ContratAccueilModifications(self.inscrit, self.current_cotisation[0])).ShowModal()\n\n    def EvtGenerationAvenant(self, _):\n", "context": "database.py\nclass TimeslotInscription(Base, Timeslot):\n    __tablename__ = \"ref_activities\"\n    idx = Column(Integer, primary_key=True)\n    inscription_id = Column(Integer, ForeignKey(\"inscriptions.idx\"))\n    inscription = relationship(Inscription)\n    day = Column(Integer)\n    activity_id = Column(Integer, ForeignKey(\"activities.idx\"), name=\"activity\")\n    activity = relationship(Activite)\n    debut = Column(Integer)\n    fin = Column(Integer)\ngeneration/contrat_accueil.py\nclass ContratAccueilModifications(OdtDocumentAccueilModifications):\n    title = \"Contrat d'accueil\"\n    template = 'Contrat accueil.odt'\n\n    def __init__(self, who, date):\n        OdtDocumentAccueilModifications.__init__(self, who, date)\n        if self.inscription.mode == MODE_TEMPS_PARTIEL and IsTemplateFile(\"Contrat accueil temps partiel.odt\"):\n            self.template = \"Contrat accueil temps partiel.odt\"\n        elif self.inscription.mode == MODE_FORFAIT_MENSUEL and IsTemplateFile(\"Contrat accueil forfait mensuel.odt\"):\n            self.template = \"Contrat accueil forfait mensuel.odt\"\n        elif self.inscription.mode == MODE_HALTE_GARDERIE and IsTemplateFile(\"Contrat accueil halte garderie.odt\"):\n            self.template = \"Contrat accueil halte garderie.odt\"\n        self.default_output = \"Contrat accueil %s - %s.odt\" % (GetPrenomNom(who), GetDateString(date, weekday=False))\ndatabase.py\nclass Referent(Base):\n    __tablename__ = \"referents\"\n    idx = Column(Integer, primary_key=True)\n    famille_id = Column(Integer, ForeignKey(\"familles.idx\"))\n    famille = relationship(Famille)\n    prenom = Column(String)\n    nom = Column(String)\n    telephone = Column(String)\n\n    def __init__(self, famille, prenom=None, nom=None, telephone=None):\n        Base.__init__(self, famille=famille, prenom=prenom, nom=nom, telephone=telephone)\ndatabase.py\nclass Inscrit(Base):\n    __tablename__ = \"inscrits\"\n    idx = Column(Integer, primary_key=True)\n    creche_id = Column(Integer, ForeignKey(\"creche.idx\"))\n    creche = relationship(Creche)\n    famille_id = Column(Integer, ForeignKey(\"familles.idx\"))\n    famille = relationship(Famille)\n    prenom = Column(String, default=\"\")\n    nom = Column(String, default=\"\")\n    sexe = Column(Integer, default=MASCULIN)\n    naissance = Column(Date)\n    handicap = Column(Boolean, default=False)\n    marche = Column(Boolean, default=False)\n    photo = Column(String)\n    notes = Column(String, default=\"\")\n    combinaison = Column(String, default=\"\")\n    categorie_id = Column(Integer, ForeignKey(\"categories.idx\"))\n    categorie = relationship(\"Categorie\")\n    allergies = Column(String, default=\"\")\n    garde_alternee = Column(Boolean, default=False)\n    type_repas = Column(Integer, default=REPAS_PUREE)\n    type_repas2 = Column(Integer, default=REPAS_ASSIETTE)\n    date_premier_contact = Column(Date)\n    date_entretien_directrice = Column(Date)\n    date_envoi_devis = Column(Date)\n    date_reponse_parents = Column(Date)\n    preinscription_state = Column(Integer)\n    inscriptions = relationship(\"Inscription\", cascade=\"all, delete-orphan\")\n    days = relationship(\"TimeslotInscrit\", collection_class=lambda: DayCollection(\"date\"), cascade=\"all, delete-orphan\")\n    weekslots = relationship(\"WeekSlotInscrit\", cascade=\"all, delete-orphan\")\n    commentaires = relationship(\"CommentaireInscrit\", collection_class=attribute_mapped_collection(\"date\"), cascade=\"all, delete-orphan\")\n    clotures = relationship(\"ClotureFacture\", collection_class=attribute_mapped_collection(\"date\"), cascade=\"all, delete-orphan\")\n    conges = relationship(\"CongeInscrit\", cascade=\"all, delete-orphan\")\n    corrections = relationship(\"Correction\", collection_class=attribute_mapped_collection(\"date\"), cascade=\"all, delete-orphan\")\n\n    def __init__(self, prenom=\"\", nom=\"\", sexe=MASCULIN, handicap=False, allergies=\"\", automatic=True, **kwargs):\n        Base.__init__(self, prenom=prenom, nom=nom, sexe=sexe, handicap=handicap, allergies=allergies, **kwargs)\n        self.famille = Famille(creche=self.creche, automatic=automatic)\n        self.famille.inscrits.append(self)\n        if automatic:\n            self.inscriptions.append(Inscription(inscrit=self))\n        self.jours_conges = {}\n\n    @reconstructor\n    def init_on_load(self):\n        self.calcule_jours_conges()\n\n    def slug(self):\n        return \"child-%d\" % self.idx\n\n    def get_preinscription_state(self):\n        if self.preinscription_state >= STATE_ACCORD_PARENTS:\n            pass\n        elif not self.date_premier_contact:\n            self.preinscription_state = STATE_PREINSCRIPTION_RECUE\n        elif not self.date_entretien_directrice:\n            self.preinscription_state = STATE_ATTENTE_ENTRETIEN\n        elif self.date_entretien_directrice > datetime.date.today():\n            self.preinscription_state = STATE_ENTRETIEN_PROGRAMME\n        elif not self.date_envoi_devis:\n            self.preinscription_state = STATE_DEVIS_A_ENVOYER\n        elif not self.date_reponse_parents:\n            self.preinscription_state = STATE_ATTENTE_REPONSE_PARENTS\n        else:\n            self.preinscription_state = STATE_ATTENTE_REPONSE_PARENTS\n        return self.preinscription_state\n\n    def get_groupe_order(self, date):\n        inscription = self.get_inscription(date)\n        return inscription.groupe.ordre if inscription and inscription.groupe else 255\n\n    def get_groupe_auto(self):\n        result = None\n        age = GetAge(self.naissance)\n        for groupe in self.creche.groupes:\n            if not groupe.age_maximum or age <= groupe.age_maximum:\n                if result is None or not result.age_maximum or (groupe.age_maximum and groupe.age_maximum < result.age_maximum):\n                    result = groupe\n        return result\n\n    def get_facture_cloturee(self, date):\n        if self.creche.temps_facturation == FACTURATION_FIN_MOIS:\n            result = self.clotures.get(GetMonthEnd(date), None)\n            if result:\n                return result\n        return self.clotures.get(GetMonthStart(date), None)\n\n    def get_week_slots(self, monday):\n        return [weekslot for weekslot in self.weekslots if weekslot.date == monday]\n\n    def get_week_activity_slot(self, monday, activity):\n        for weekslot in self.weekslots:\n            # print(weekslot.date, weekslot.activity, activity, weekslot)\n            if weekslot.date == monday and weekslot.activity == activity:\n                return weekslot\n        else:\n            return None\n\n    def add_conge(self, conge):\n        self.conges.append(conge)\n        self.calcule_jours_conges()\n\n    def delete_conge(self, conge):\n        self.conges.remove(conge)\n        self.calcule_jours_conges()\n\n    def calcule_jours_conges(self):\n        self.jours_conges = {}\n\n        def AddPeriode(debut, fin, conge):\n            date = debut\n            while date <= fin:\n                if date not in self.creche.jours_fermeture:\n                    self.jours_conges[date] = conge\n                date += datetime.timedelta(1)\n\n        for conge in self.conges:\n            if conge.debut:\n                try:\n                    count = conge.debut.count('/')\n                    if count == 2:\n                        debut = str2date(conge.debut)\n                        if not conge.fin or conge.fin.strip() == \"\":\n                            fin = debut\n                        else:\n                            fin = str2date(conge.fin)\n                        AddPeriode(debut, fin, conge)\n                    elif count == 1:\n                        for year in range(config.first_date.year, config.last_date.year + 1):\n                            debut = str2date(conge.debut, year)\n                            if not conge.fin or conge.fin.strip() == \"\":\n                                fin = debut\n                            else:\n                                fin = str2date(conge.fin, year)\n                            AddPeriode(debut, fin, conge)\n                except Exception as e:\n                    print(\"Exception congé %s %s\" % (self.prenom, self.nom), e)\n\n    def is_present(self, debut, fin, site=None, handicap=None, reservataire=None):\n        for inscription in self.inscriptions:\n            inscription_fin = inscription.depart if inscription.depart else inscription.fin\n            if ((inscription_fin is None or inscription_fin >= debut) and\n                    (not self.creche.preinscriptions or not inscription.preinscription) and\n                    (site is None or inscription.site == site) and\n                    (reservataire is None or inscription.reservataire == reservataire) and\n                    (inscription.debut is not None) and\n                    (not fin or inscription.debut <= fin) and\n                    (handicap is None or self.handicap == handicap)):\n                return True\n        return False\n\n    def get_mode_arrondi(self):\n        return self.creche.arrondi_heures\n\n    def get_allergies(self):\n        return [allergie.strip() for allergie in self.allergies.split(\",\")]\n\n    def has_facture(self, date, site=None):\n        if not date:  # or date.month in database.creche.mois_sans_facture:\n            return False\n        month_start = GetMonthStart(date)\n        month_end = GetMonthEnd(date)\n        if self.get_inscriptions(month_start, month_end, site):\n            return True\n        if self.creche.temps_facturation != FACTURATION_FIN_MOIS:\n            previous_month_end = month_start - datetime.timedelta(1)\n            previous_month_start = GetMonthStart(previous_month_end)\n            if self.get_inscriptions(previous_month_start, previous_month_end, site):\n                day = previous_month_start\n                while day.month == previous_month_start.month:\n                    state = self.GetState(day)\n                    if state.heures_facturees != state.heures_contractualisees:\n                        return True\n                    day += datetime.timedelta(1)\n        return False\n\n    def get_factures_list(self):\n        result = []\n        date = config.get_first_monday()\n        while date <= datetime.date.today():\n            if self.has_facture(date):\n                result.append(date)\n            date = GetNextMonthStart(date)\n        return result\n\n    def GetPeriodeInscriptions(self):\n        if len(self.inscriptions) == 0:\n            return None, None\n        else:\n            debut, fin = self.inscriptions[0].debut, self.inscriptions[0].fin\n            for inscription in self.inscriptions:\n                if debut is None or (inscription.debut is not None and inscription.debut < debut):\n                    debut = inscription.debut\n                if fin is not None and (inscription.fin is None or inscription.fin > fin):\n                    fin = inscription.fin\n            return debut, fin\n\n    def get_planning(self, date):\n        return self.get_inscription(date)\n\n    def get_contrat(self, date):\n        return self.get_inscription(date)\n\n    def get_inscription(self, date, preinscription=False, departanticipe=True, array=False):\n        result = []\n        for inscription in self.inscriptions:\n            if (preinscription or not self.creche.preinscriptions or not inscription.preinscription) and \\\n                    (inscription.debut and date >= inscription.debut and (not inscription.fin or date <= inscription.fin)) and \\\n                    (not departanticipe or not inscription.depart or date <= inscription.depart):\n                if array:\n                    result.append(inscription)\n                else:\n                    return inscription\n        if array:\n            return result\n        else:\n            return None\n\n    def get_inscriptions(self, date_debut=None, date_fin=None, site=None, preinscriptions=False, departanticipe=True):\n        result = []\n        if not date_debut:\n            date_debut = datetime.date.min\n        if not date_fin:\n            date_fin = datetime.date.max\n        for inscription in self.inscriptions:\n            if (site is None or site == inscription.site) \\\n                    and (preinscriptions or not self.creche.preinscriptions or not inscription.preinscription) \\\n                    and inscription.debut:\n                date_debut_periode = inscription.debut\n                try:\n                    if departanticipe and inscription.depart:\n                        date_fin_periode = inscription.depart\n                    elif inscription.fin:\n                        date_fin_periode = inscription.fin\n                    else:\n                        date_fin_periode = datetime.date.max\n                    if date_fin_periode < date_debut_periode:\n                        print(\"Période incorrecte pour %s %s :\" % (self.prenom, self.nom), date_debut_periode, date_fin_periode)\n                        continue\n                    if (date_debut_periode <= date_debut <= date_fin_periode) or (date_debut_periode <= date_fin <= date_fin_periode) or (date_debut < date_debut_periode and date_fin > date_fin_periode):\n                        result.append(inscription)\n                except Exception as e:\n                    print(\"Exception inscriptions\", e)\n        result.sort(key=lambda inscription: inscription.debut)\n        return result\n\n    def is_date_conge(self, date):\n        if date in self.creche.jours_fermeture:\n            return True\n        if date in self.jours_conges:\n            if self.creche.conges_inscription != GESTION_CONGES_INSCRIPTION_MENSUALISES_AVEC_POSSIBILITE_DE_SUPPLEMENT:\n                return True\n            if date in self.days:\n                return self.days[date].get_state() == ABSENT\n        return False\n\n    def GetRattachement(self):\n        result = None\n        for inscrit in self.creche.inscrits:\n            if inscrit is not self:\n                if inscrit.famille is self.famille:\n                    return True\n                if inscrit.nom == self.nom:\n                    result = False\n        return result\n\n    def ChangeRattachement(self, state):\n        if state:\n            for inscrit in self.creche.inscrits:\n                if inscrit is not self and inscrit.nom == self.nom:\n                    self.famille = inscrit.famille\n                    break\n        else:\n            self.famille = Famille(creche=self.creche)\n\n    def GetJournee(self, date):\n        if self.is_date_conge(date):\n            return None\n\n        inscription = self.get_inscription(date)\n        if inscription is None:\n            return None\n\n        result = self.days.get(date, None)\n        if result:\n            return result\n\n        return self.GetJourneeReference(date)\n\n    def GetJourneeReference(self, date):\n        if date in self.jours_conges:\n            return Day()\n        else:\n            inscription = self.get_inscription(date)\n            if inscription:\n                return inscription.get_day_from_date(date)\n            else:\n                return None\n\n    def get_nombre_jours_maladie(self, date):\n        # recherche du premier et du dernier jour\n        premier_jour_maladie = tmp = date\n        nombre_jours_ouvres_maladie = 0\n        pile = 0\n        while tmp > self.inscriptions[0].debut:\n            tmp -= datetime.timedelta(1)\n            state = self.get_state(tmp)\n            if tmp not in self.creche.jours_fermeture:\n                pile += 1\n            if state == MALADE:\n                premier_jour_maladie = tmp\n                if self.creche.traitement_maladie == DEDUCTION_MALADIE_AVEC_CARENCE_JOURS_CONSECUTIFS:\n                    nombre_jours_ouvres_maladie += 1\n                else:\n                    nombre_jours_ouvres_maladie += pile\n                pile = 0\n            elif state != ABSENT:\n                break\n        if self.creche.traitement_maladie in (DEDUCTION_MALADIE_AVEC_CARENCE_JOURS_OUVRES, DEDUCTION_MALADIE_AVEC_CARENCE_JOURS_CONSECUTIFS):\n            nombre_jours_maladie = nombre_jours_ouvres_maladie + 1\n        elif self.creche.traitement_maladie == DEDUCTION_MALADIE_AVEC_CARENCE_JOURS_CALENDAIRES:\n            nombre_jours_maladie = (date - premier_jour_maladie).days + 1\n        else:\n            dernier_jour_maladie = tmp = date\n            while not self.inscriptions[-1].fin or tmp < self.inscriptions[-1].fin:\n                tmp += datetime.timedelta(1)\n                state = self.get_state(tmp)\n                if state == MALADE:\n                    dernier_jour_maladie = tmp\n                else:\n                    break\n            nombre_jours_maladie = (dernier_jour_maladie - premier_jour_maladie).days + 1\n        return nombre_jours_maladie\n\n    def GetState(self, date, mode_arrondi=SANS_ARRONDI):\n        \"\"\"Retourne les infos sur une journée\n        :param date: la journée\n        \"\"\"\n\n        if self.is_date_conge(date):\n            return State(ABSENT)\n\n        inscription = self.get_inscription(date)\n        if inscription is None:\n            return State(ABSENT)\n\n        reference = self.GetJourneeReference(date)  # Attention pas depuis inscription à cause des congés inscription avec supplément\n        heures_reference = reference.get_duration(mode_arrondi)\n        ref_state = reference.get_state()\n\n        if self.creche.jours_fermeture_non_prevus.get(date):\n            return State(ABSENT, heures_reference, 0, 0)\n\n        if date in self.days:\n            journee = self.days[date]\n            state = journee.get_state()\n            if state == ABSENCE_NON_PREVENUE:\n                heures_facturees = heures_reference\n                if heures_facturees == 0:\n                    for timeslot in journee.timeslots:\n                        if timeslot.activity.mode == MODE_ABSENCE_NON_PREVENUE:\n                            heures_facturees += timeslot.get_duration()\n                        heures_facturees = heures_facturees / 60\n                return State(state, heures_reference, 0, heures_facturees)\n            elif state == HOPITAL:\n                return State(state, heures_reference, 0, 0)\n            elif state == MALADE and self.get_nombre_jours_maladie(date) > self.creche.minimum_maladie:\n                return State(state, heures_reference, 0, 0)\n            elif state in (MALADE, MALADE_SANS_JUSTIFICATIF, ABSENCE_NON_PREVENUE, ABSENCE_CONGE_SANS_PREAVIS):\n                return State(state, heures_reference, 0, heures_reference)\n            elif state in (ABSENT, VACANCES):\n                if inscription.mode == MODE_TEMPS_PLEIN or ref_state:\n                    return State(VACANCES, heures_reference, 0, heures_reference)\n                else:\n                    return State(ABSENT, heures_reference, 0, heures_reference)\n            else:  # PRESENT\n                tranche = 5.0 / 60\n                heures_realisees = 0.0\n                heures_facturees = 0.0\n\n                # TODO une petite fonction pour ce code duplique dans le test\n                timeslots = GetUnionTimeslots([timeslot for timeslot in journee.timeslots if timeslot.activity.mode in (0, MODE_PRESENCE_NON_FACTUREE)])\n                for timeslot in timeslots:\n                    heures_realisees += tranche * GetDureeArrondie(self.creche.arrondi_heures, timeslot.debut, timeslot.fin)\n\n                union = GetUnionHeures(journee, reference)\n                if inscription.IsInPeriodeAdaptation(date):\n                    if self.creche.facturation_periode_adaptation == FACTURATION_HORAIRES_REELS:\n                        union = journee.timeslots\n                    for timeslot in union:\n                        heures_facturees += tranche * GetDureeArrondie(self.creche.arrondi_facturation_periode_adaptation, timeslot.debut, timeslot.fin)\n                else:\n                    for timeslot in union:\n                        heures_facturees += tranche * GetDureeArrondie(self.creche.arrondi_facturation, timeslot.debut, timeslot.fin)\n                    timeslots_presence_non_facturee = GetUnionTimeslots([timeslot for timeslot in journee.timeslots if timeslot.activity.mode == MODE_PRESENCE_NON_FACTUREE])\n                    for timeslot in timeslots_presence_non_facturee:\n                        heures_facturees -= tranche * GetDureeArrondie(self.creche.arrondi_facturation, timeslot.debut, timeslot.fin)\n\n                return State(PRESENT, heures_reference, heures_realisees, heures_facturees)\n        else:\n            if ref_state:\n                return State(PRESENT, heures_reference, heures_reference, heures_reference)\n            else:\n                return State(ABSENT)\n\n    def get_state(self, date):\n        if self.is_date_conge(date):\n            return ABSENT\n        elif date in self.days:\n            return self.days[date].get_state()\n        else:\n            inscription = self.get_inscription(date)\n            return inscription.get_day_from_date(date).get_state() if inscription else ABSENT\n\n    def GetExtraActivites(self, date):\n        # TODO il y a un problème avec les CONGES_AVEC_POSSIBILITE_DE_SUPPLEMENT parce que la journée retournée est la journée de référence et donc les activités sont comptées à tort\n        day = self.GetJournee(date)\n        if day is None:\n            return []\n        result = set()\n        for timeslot in day.timeslots:\n            if timeslot.activity.mode > 0:\n                result.add(timeslot)\n        if result:\n            for activity in self.creche.activites:\n                if activity.mode == MODE_SYSTEMATIQUE_SANS_HORAIRES:\n                    result.add(Timeslot(debut=None, fin=None, activity=activity))\n        return result\n\n    def GetTotalActivitesPresenceNonFacturee(self, date):\n        day = self.GetJournee(date)\n        return 0 if day is None else day.get_duration_per_activity_mode(MODE_PRESENCE_NON_FACTUREE)\n\n    def GetTotalActivitesPresenceFactureesEnSupplement(self, date):\n        day = self.GetJournee(date)\n        return 0 if day is None else day.get_duration_per_activity_mode(MODE_PRESENCE_SUPPLEMENTAIRE)\n\n    def GetTotalActivitesConges(self, date):\n        day = self.GetJournee(date)\n        return 0 if day is None else day.get_duration_per_activity_mode(MODE_CONGES)\n\n    def GetDecomptePermanences(self):\n        today = datetime.date.today()\n        total, effectue = 0.0, 0.0\n        date = self.creche.date_raz_permanences\n        if date:\n            while date < today:\n                journee = self.GetJournee(date)\n                if journee:\n                    effectue += journee.get_duration_permanences()\n                date += datetime.timedelta(1)\n            anniversaire = GetDateAnniversaire(self.creche.date_raz_permanences)\n            for inscription in self.inscriptions:\n                if inscription.debut is not None and self.creche.date_raz_permanences <= inscription.debut < today:\n                    fin = inscription.fin if inscription.fin else anniversaire\n                    if fin < today:\n                        total += inscription.heures_permanences\n                    else:\n                        total += inscription.heures_permanences * (today - inscription.debut).days / (fin - inscription.debut).days\n        return total, effectue\n\n    def GetRegime(self, date):\n        result = 0\n        for parent in self.famille.parents:\n            if parent:\n                revenu = Select(parent.revenus, date)\n                if revenu and revenu.regime:\n                    result = revenu.regime\n                    break\n        return result\ndatabase.py\nclass Fratrie(Base):\n    __tablename__ = \"fratries\"\n    idx = Column(Integer, primary_key=True)\n    famille_id = Column(Integer, ForeignKey(\"familles.idx\"))\n    famille = relationship(Famille)\n    prenom = Column(String)\n    naissance = Column(Date)\n    entree = Column(Date)\n    sortie = Column(Date)\n\n    def __init__(self, famille, prenom=None, naissance=None, entree=None, sortie=None):\n        Base.__init__(self, famille=famille, prenom=prenom, naissance=naissance, entree=entree, sortie=sortie)\ndatabase.py\nclass Revenu(Base):\n    __tablename__ = \"revenus\"\n    idx = Column(Integer, primary_key=True)\n    parent_id = Column(Integer, ForeignKey(\"parents.idx\"))\n    parent = relationship(Parent)\n    debut = Column(Date)\n    fin = Column(Date)\n    revenu = Column(Integer, default=\"\")\n    chomage = Column(Boolean, default=False)\n    conge_parental = Column(Boolean, default=False)\n    regime = Column(Integer, default=0)\n\n    def __init__(self, parent, debut=None, fin=None, revenu=\"\", chomage=False, conge_parental=False, regime=0, **kwargs):\n        Base.__init__(self, parent=parent, debut=debut, fin=fin, revenu=revenu, chomage=chomage, conge_parental=conge_parental, regime=regime, **kwargs)\nconfig.py\nCONFIG_FILENAME = \"gertrude.ini\"\nDEFAULT_SECTION = \"gertrude\"\n    DEFAULT_DATABASE = \"gertrude.db\"\n    BACKUPS_DIRECTORY = \"./backups\"\n    CONFIG_PATHS = [\"\"]\n    HOME = os.path.expanduser(\"~\")\n    GERTRUDE_DIRECTORY = HOME + \"/.gertrude\"\n    DEFAULT_DATABASE = GERTRUDE_DIRECTORY + \"/gertrude.db\"\n    BACKUPS_DIRECTORY = GERTRUDE_DIRECTORY + '/backups'\n    CONFIG_PATHS = [\"./\", GERTRUDE_DIRECTORY + \"/\", \"/etc/gertrude/\"]\nDEMO_DATABASE = \"demo.db\"\ndef getDefaultDocumentsDirectory():\n    def __init__(self, parser, name=None):\n    def getStringParameter(self, key, default=\"\"):\n    def getIntegerParameter(self, key, default=0):\n    def getDateParameter(self, key, default=None):\n    def getTimeParameter(self, key, default=None):\n    def getOptionsParameter(self):\n    def getWindowSize(self):\n    def getPictos(self):\n    def getDocumentsDirectory(self):\n    def getTemplatesDirectory(self):\n    def getBackupsDirectory(self):\n    def __init__(self):\n    def __init__(self):\n    def find_config_file():\n    def load(self, path=None, progress_handler=default_progress_handler):\n    def save(self, progress_handler):\n    def set_current_section(self, section_name):\n    def get_first_monday(self):\n    def is_date_after_reglements_start(self, date):\n    def __getattr__(self, key):\n    def Filter():\n    def Update():\nclass Section(object):\nclass DefaultConfig(object):\nclass Config(object):\ngeneration/contrat_accueil.py\nclass FraisGardeModifications(DocumentAccueilModifications):\n    title = \"Frais de garde\"\n    template = \"Frais de garde.ods\"\n\n    def __init__(self, who, date):\n        DocumentAccueilModifications.__init__(self, who, date)\n        self.multi = False\n        self.default_output = \"Frais de garde %s - %s.odt\" % (GetPrenomNom(who), GetDateString(date, weekday=False))\n        \n    def execute(self, filename, dom):\n        if filename != 'content.xml':\n            return None\n        \n        spreadsheet = dom.getElementsByTagName('office:spreadsheet').item(0)\n        table = spreadsheet.getElementsByTagName(\"table:table\").item(0)       \n        lignes = table.getElementsByTagName(\"table:table-row\")\n\n        fields = self.GetFields()\n        ReplaceFields(lignes, fields)\n\n        if len(self.cotisation.revenus_parents) < 2:\n            RemoveNodesContaining(lignes, \"parent2\")\n        elif not self.cotisation.revenus_parents[1][2]:\n            RemoveNodesContaining(lignes, \"abattement-parent2\")\n        if len(self.cotisation.revenus_parents) < 1:\n            RemoveNodesContaining(lignes, \"parent1\")\n        elif not self.cotisation.revenus_parents[0][2]:\n            RemoveNodesContaining(lignes, \"abattement-parent1\")\n            \n        if database.creche.mode_facturation == FACTURATION_FORFAIT_MENSUEL:\n            RemoveNodesContaining(lignes, \"assiette-annuelle\")\n            RemoveNodesContaining(lignes, \"assiette-mensuelle\")\n            RemoveNodesContaining(lignes, \"taux-effort\")\n            RemoveNodesContaining(lignes, \"heures-mois\")\n            RemoveNodesContaining(lignes, \"montant-heure-garde\")\n\n        return {}\ngeneration/contrat_accueil.py\nclass AvenantContratAccueilModifications(OdtDocumentAccueilModifications):\n    title = \"Avenant au contrat d'accueil\"\n    template = \"Avenant contrat accueil.odt\"\n\n    def __init__(self, who, date):\n        OdtDocumentAccueilModifications.__init__(self, who, date)\n        self.default_output = \"Avenant contrat accueil %s - %s.odt\" % (GetPrenomNom(who), GetDateString(date, weekday=False))\ndatabase.py\nclass Inscription(Base, PeriodeReference):\n    __tablename__ = \"inscriptions\"\n    idx = Column(Integer, primary_key=True)\n    inscrit_id = Column(Integer, ForeignKey(\"inscrits.idx\"))\n    inscrit = relationship(Inscrit)\n    preinscription = Column(Boolean)\n    reservataire_id = Column(Integer, ForeignKey(\"reservataires.idx\"))\n    reservataire = relationship(Reservataire)\n    groupe_id = Column(Integer, ForeignKey(\"groupes.idx\"))\n    groupe = relationship(\"Groupe\")\n    forfait_mensuel = Column(Float, default=0)\n    frais_inscription = Column(Float, default=0)\n    allocation_mensuelle_caf = Column(Float)\n    site_id = Column(Integer, ForeignKey(\"sites.idx\"))\n    site = relationship(Site)\n    _sites_preinscription = Column(String, name=\"sites_preinscription\")\n    professeur_id = Column(Integer, ForeignKey(\"professeurs.idx\"))\n    professeur = relationship(Professeur)\n    debut_asap = Column(Boolean)\n    debut = Column(Date)\n    fin = Column(Date)\n    depart = Column(Date)\n    mode = Column(Integer)\n    fin_periode_adaptation = Column(Date)\n    duree_reference = Column(Integer, default=7)\n    forfait_mensuel_heures = Column(Float, default=0)\n    semaines_conges = Column(Integer, default=0)\n    heures_permanences = Column(Float, default=0)\n    newsletters = Column(Integer, default=\"\")\n    tarifs = Column(Integer, default=0)\n    days = relationship(\"TimeslotInscription\", collection_class=lambda: DayCollection(\"day\"), cascade=\"all, delete-orphan\")\n\n    def __init__(self, inscrit, mode=MODE_TEMPS_PARTIEL, duree_reference=7, debut=datetime.date.today(), forfait_mensuel_heures=0, forfait_mensuel=0, frais_inscription=0, semaines_conges=0, heures_permanences=0, tarifs=0, allocation_mensuelle_caf=0, **kwargs):\n        Base.__init__(self, inscrit=inscrit, mode=mode, duree_reference=duree_reference, debut=debut, forfait_mensuel_heures=forfait_mensuel_heures, forfait_mensuel=forfait_mensuel, frais_inscription=frais_inscription, semaines_conges=semaines_conges, heures_permanences=heures_permanences, tarifs=tarifs, allocation_mensuelle_caf=allocation_mensuelle_caf, **kwargs)\n        if is_power_of_two(inscrit.creche.modes_inscription):\n            self.mode = int(math.log(inscrit.creche.modes_inscription, 2))\n        self.__dict__[\"sites_preinscription\"] = []\n\n    @reconstructor\n    def init_on_load(self):\n        self.__dict__[\"sites_preinscription\"] = []\n        if self._sites_preinscription:\n            sites_list = [int(index) for index in self._sites_preinscription.split()]\n            for site in self.inscrit.creche.sites:\n                if site.idx in sites_list:\n                    self.sites_preinscription.append(site)\n\n    def __setattr__(self, name, value):\n        Base.__setattr__(self, name, value)\n        # TODO remove this in next conversions\n        if name == \"sites_preinscription\":\n            self._sites_preinscription = \" \".join([str(value.idx) for value in value])\n\n    def GetNombreJoursCongesPeriode(self):\n        if self.preinscription:\n            return 0\n        elif self.semaines_conges:\n            if self.mode == MODE_FORFAIT_HEBDOMADAIRE:\n                return self.semaines_conges * 7\n            else:\n                return self.semaines_conges * self.get_days_per_week()\n        else:\n            return 0\n\n    def GetNombreJoursCongesPris(self, debut, fin):\n        jours = 0\n        date = debut\n        # print \"GetNombreJoursCongesPris(%s - %s)\" % (debut, fin)\n\n        if config.options & REGULARISATION_UNIQUEMENT_SEMAINES_FERMETURE:\n            while date <= fin:\n                if date in self.inscrit.creche.jours_conges:\n                    # print(date)\n                    jours += 1\n                date += datetime.timedelta(1)\n            return jours\n\n        while date <= fin:\n            if self.mode in (MODE_FORFAIT_HEBDOMADAIRE, MODE_FORFAIT_MENSUEL):\n                if date in self.inscrit.creche.periodes_fermeture or date in self.inscrit.jours_conges:\n                    # print(date)\n                    jours += 1\n            else:\n                state = self.inscrit.get_state(date)\n                if self.inscrit.creche.facturation_jours_feries == ABSENCES_DEDUITES_EN_JOURS:\n                    if state == VACANCES:\n                        # print(\"VACANCES\", date)\n                        jours += 1\n                else:\n                    if state in (ABSENT, VACANCES):\n                        reference = self.get_day_from_date(date)\n                        if reference.get_duration() > 0:\n                            # print(date)\n                            jours += 1\n            date += datetime.timedelta(1)\n        return jours\n\n    def GetDebutDecompteJoursConges(self):\n        if self.fin_periode_adaptation:\n            return self.fin_periode_adaptation + datetime.timedelta(1)\n        else:\n            return self.debut\n\n    def GetFin(self):\n        return self.depart if (self.inscrit.creche.gestion_depart_anticipe and self.depart) else (self.fin if self.fin else datetime.date.max)\n\n    def GetFinDecompteJoursConges(self):\n        if self.inscrit.creche.gestion_depart_anticipe and self.depart:\n            return self.depart\n        else:\n            return self.fin\n\n    def GetNombreJoursCongesPoses(self):\n        if self.debut and self.fin and not self.preinscription:\n            return self.GetNombreJoursCongesPris(self.GetDebutDecompteJoursConges(), self.GetFinDecompteJoursConges())\n        else:\n            return 0\n\n    def IsNombreSemainesCongesDepasse(self, jalon):\n        if self.inscrit.creche.facturation_jours_feries == ABSENCES_DEDUITES_SANS_LIMITE:\n            return False\n        if self.mode == MODE_FORFAIT_GLOBAL_CONTRAT:\n            return False\n        if self.debut:\n            if not self.semaines_conges:\n                return True\n            debut = self.GetDebutDecompteJoursConges()\n            pris = self.GetNombreJoursCongesPris(debut, jalon)\n            total = self.GetNombreJoursCongesPeriode()\n            return pris > total\n        else:\n            return False\n\n    def GetDatesFromReference(self, index):\n        if self.debut is not None:\n            fin = self.fin if self.fin else datetime.date(self.debut.year + 1, self.debut.month, self.debut.day)\n            date = self.debut + datetime.timedelta(index + 7 - self.debut.weekday())\n            while date < fin:\n                yield date\n                date += datetime.timedelta(self.duree_reference)\n\n    def IsInPeriodeAdaptation(self, date):\n        if self.debut is None or self.fin_periode_adaptation is None:\n            return False\n        return self.debut <= date <= self.fin_periode_adaptation\n\n    def GetListeActivites(self):\n        result = []\n        for i in range(self.duree_reference):\n            jour = self.get_day_from_index(i)\n            s = jour.GetHeureArriveeDepart()\n            if s:\n                if self.duree_reference <= 7:\n                    s = days[i] + \" \" + s\n                else:\n                    s = days[i % 7] + \" semaine %d\" % (1 + (i / 7)) + s\n                result.append(s)\n        return ', '.join(result)\ndatabase.py\nclass Parent(Base):\n    __tablename__ = \"parents\"\n    idx = Column(Integer, primary_key=True)\n    famille_id = Column(Integer, ForeignKey(\"familles.idx\"))\n    famille = relation(Famille)\n    sexe = Column(Integer, default=MASCULIN)\n    prenom = Column(String, default=\"\")\n    nom = Column(String, default=\"\")\n    adresse = Column(String, default=\"\")\n    code_postal = Column(Integer, default=\"\")\n    ville = Column(String, default=\"\")\n    telephone_domicile = Column(String, default=\"\")\n    telephone_domicile_notes = Column(String, default=\"\")\n    telephone_portable = Column(String, default=\"\")\n    telephone_portable_notes = Column(String, default=\"\")\n    telephone_travail = Column(String, default=\"\")\n    telephone_travail_notes = Column(String, default=\"\")\n    profession = Column(String, default=\"\")\n    email = Column(String, default=\"\")\n    revenus = relationship(\"Revenu\", cascade=\"all, delete-orphan\")\n\n    def __init__(self, famille, sexe=FEMININ, prenom=\"\", nom=\"\", adresse=\"\", code_postal=\"\", ville=\"\",\n                 telephone_domicile=\"\", telephone_domicile_notes=\"\", telephone_portable=\"\", telephone_portable_notes=\"\", telephone_travail=\"\", telephone_travail_notes=\"\",\n                 profession=\"\", email=\"\", add_revenus=True, **kwargs):\n        Base.__init__(self,\n                      famille=famille,\n                      sexe=sexe,\n                      prenom=prenom,\n                      nom=nom,\n                      adresse=adresse,\n                      code_postal=code_postal,\n                      ville=ville,\n                      telephone_domicile=telephone_domicile,\n                      telephone_domicile_notes=telephone_domicile_notes,\n                      telephone_portable=telephone_portable,\n                      telephone_portable_notes=telephone_portable_notes,\n                      telephone_travail=telephone_travail,\n                      telephone_travail_notes=telephone_travail_notes,\n                      profession=profession,\n                      email=email,\n                      **kwargs)\n        if add_revenus:\n            date_revenus = famille.creche.GetDateRevenus(datetime.date.today())\n            self.revenus.append(Revenu(self, GetYearStart(date_revenus), GetYearEnd(date_revenus)))\ndatabase.py\nclass CongeInscrit(Base):\n    __tablename__ = \"conges_inscrits\"\n    idx = Column(Integer, primary_key=True)\n    inscrit_id = Column(Integer, ForeignKey(\"inscrits.idx\"))\n    inscrit = relationship(Inscrit)\n    debut = Column(String)\n    fin = Column(String)\n    label = Column(String)\n\n    def __init__(self, inscrit, **kwargs):\n        Base.__init__(self, inscrit=inscrit, **kwargs)\n\n    def __setattr__(self, name, value):\n        # Call the parent class method first.\n        super(CongeInscrit, self).__setattr__(name, value)\n        if self.inscrit and name in (\"debut\", \"fin\"):\n            self.inscrit.calcule_jours_conges()\ngeneration/contrat_accueil.py\nclass DevisAccueilModifications(OdtDocumentAccueilModifications):\n    title = \"Devis\"\n    template = \"Devis accueil.odt\"\n\n    def __init__(self, who, date):\n        OdtDocumentAccueilModifications.__init__(self, who, date)\n        self.default_output = \"Devis accueil %s - %s.odt\" % (GetPrenomNom(who), GetDateString(date, weekday=False))\n", "answers": ["        DocumentDialog(self, AvenantContratAccueilModifications(self.inscrit, self.current_cotisation[0])).ShowModal()"], "length": 3291, "dataset": "repobench-p_e", "language": "python", "all_classes": null, "_id": "6b4f4a52ac5e336182dc4a4f91a06a0476144355eda7d4f7"}
{"input": "package io.github.redwallhp.athenagm;\nimport com.sk89q.worldedit.bukkit.WorldEditPlugin;\nimport io.github.redwallhp.athenagm.arenas.ArenaHandler;\nimport io.github.redwallhp.athenagm.commands.AdminCommands;\nimport io.github.redwallhp.athenagm.commands.ArenaCommands;\nimport io.github.redwallhp.athenagm.commands.MatchCommands;\nimport io.github.redwallhp.athenagm.configuration.Configuration;\nimport io.github.redwallhp.athenagm.hub.Hub;\nimport io.github.redwallhp.athenagm.maps.VoidGenerator;\nimport io.github.redwallhp.athenagm.modules.Module;\nimport io.github.redwallhp.athenagm.modules.ModuleLoader;\nimport io.github.redwallhp.athenagm.regions.RegionHandler;\nimport io.github.redwallhp.athenagm.tracker.Tracker;\nimport org.bukkit.Bukkit;\nimport org.bukkit.generator.ChunkGenerator;\nimport org.bukkit.plugin.Plugin;\nimport org.bukkit.plugin.java.JavaPlugin;\nimport java.io.File;\n\n\n\n\n/**\n * Project copyright 2015 redwall_hp\n * Licensed under the Lesser GNU Public License\n * http://www.gnu.org/licenses/lgpl-3.0.en.html\n * http://github.com/redwallhp\n */\npublic class AthenaGM extends JavaPlugin {\n\n\n    public Configuration config;", "context": "src/main/java/io/github/redwallhp/athenagm/commands/MatchCommands.java\npublic class MatchCommands implements CommandExecutor {\n\n\n    private AthenaGM plugin;\n\n\n    public MatchCommands(AthenaGM plugin) {\n        this.plugin = plugin;\n        plugin.getCommand(\"teams\").setExecutor(this);\n        plugin.getCommand(\"team\").setExecutor(this);\n        plugin.getCommand(\"autojoin\").setExecutor(this);\n        plugin.getCommand(\"spectate\").setExecutor(this);\n        plugin.getCommand(\"score\").setExecutor(this);\n        plugin.getCommand(\"players\").setExecutor(this);\n        plugin.getCommand(\"timeleft\").setExecutor(this);\n        plugin.getCommand(\"tmsg\").setExecutor(this);\n    }\n\n\n    public boolean onCommand(CommandSender sender, Command cmd, String label, String[] args) {\n\n        if (cmd.getName().equalsIgnoreCase(\"teams\")) {\n            listTeams(sender);\n            return true;\n        }\n\n        if (cmd.getName().equalsIgnoreCase(\"team\")) {\n            joinTeam(sender, args);\n            return true;\n        }\n\n        if (cmd.getName().equalsIgnoreCase(\"autojoin\")) {\n            autoJoinTeam(sender);\n            return true;\n        }\n\n        if (cmd.getName().equalsIgnoreCase(\"spectate\")) {\n            String[] arguments = { \"spectator\" };\n            joinTeam(sender, arguments);\n            return true;\n        }\n\n        if (cmd.getName().equalsIgnoreCase(\"score\")) {\n            printPlayerScore(sender);\n            return true;\n        }\n\n        if (cmd.getName().equalsIgnoreCase(\"players\")) {\n            printPlayersList(sender);\n            return true;\n        }\n\n        if (cmd.getName().equalsIgnoreCase(\"timeleft\")) {\n            timeLeft(sender, args);\n            return true;\n        }\n\n        if (cmd.getName().equalsIgnoreCase(\"tmsg\")) {\n            teamChat(sender, args);\n            return true;\n        }\n\n        return false;\n\n    }\n\n\n    private void listTeams(CommandSender sender) {\n\n        if (!(sender instanceof Player)) {\n            sender.sendMessage(ChatColor.RED + \"You must join an arena to list teams.\");\n            return;\n        }\n\n        Player player = (Player) sender;\n        Arena arena = plugin.getArenaHandler().getArenaForPlayer(player);\n\n        if (arena == null) {\n            sender.sendMessage(ChatColor.RED + \"You must join an arena to list teams.\");\n            return;\n        }\n\n        List<String> teamStrings = new ArrayList<String>();\n        for (Team team : arena.getMatch().getTeams().values()) {\n            teamStrings.add(String.format(\"%s%s (%d/%d)%s\", team.getChatColor(), team.getId(), team.getPlayers().size(), team.getSize(), ChatColor.RESET));\n        }\n\n        String list = StringUtil.joinList(\", \", teamStrings);\n        sender.sendMessage(String.format(\"%sTeams: %s\", ChatColor.DARK_AQUA, list));\n\n    }\n\n\n    private void joinTeam(CommandSender sender, String[] args) {\n\n        if (!(sender instanceof Player)) {\n            sender.sendMessage(ChatColor.RED + \"Console can't join a team.\");\n            return;\n        }\n\n        if (args.length != 1) {\n            sender.sendMessage(ChatColor.RED + \"Usage: /team <team id>\");\n            return;\n        }\n\n        Player player = (Player) sender;\n        Arena arena = plugin.getArenaHandler().getArenaForPlayer(player);\n\n        if (arena == null) {\n            sender.sendMessage(ChatColor.RED + \"You must join an arena first.\");\n            return;\n        }\n\n        for (Team team : arena.getMatch().getTeams().values()) {\n            if (team.getId().equalsIgnoreCase(args[0])) {\n                team.add(player, false);\n                return;\n            }\n        }\n\n        sender.sendMessage(ChatColor.RED + \"Invalid team id.\");\n\n    }\n\n\n    private void autoJoinTeam(CommandSender sender) {\n\n        if (!(sender instanceof Player)) {\n            sender.sendMessage(ChatColor.RED + \"Console can't join a team.\");\n            return;\n        }\n\n        Player player = (Player) sender;\n        Arena arena = plugin.getArenaHandler().getArenaForPlayer(player);\n\n        if (arena == null) {\n            sender.sendMessage(ChatColor.RED + \"You must join an arena first.\");\n            return;\n        }\n\n        Team lowest = null;\n        for (Team t : arena.getMatch().getTeams().values()) {\n            if (t.isSpectator()) continue;\n            if (t.getPlayers().size() >= t.getSize()) continue;\n            if (lowest == null || t.getPlayers().size() < lowest.getPlayers().size()) {\n                lowest = t;\n            }\n        }\n\n        if (lowest != null) {\n            lowest.add(player, false);\n        } else {\n            sender.sendMessage(ChatColor.RED + \"Unable to join team.\");\n        }\n\n    }\n\n\n    private void printPlayerScore(CommandSender sender) {\n\n        if (!(sender instanceof Player)) {\n            sender.sendMessage(ChatColor.RED + \"Console can't have a score.\");\n            return;\n        }\n\n        Player player = (Player) sender;\n        Arena arena = plugin.getArenaHandler().getArenaForPlayer(player);\n        if (arena == null) {\n            sender.sendMessage(ChatColor.RED + \"You must join an arena first.\");\n            return;\n        }\n        Team team = PlayerUtil.getTeamForPlayer(arena.getMatch(), player);\n        if (team == null || team.isSpectator()) {\n            sender.sendMessage(ChatColor.RED + \"You must join a team to have a score.\");\n            return;\n        }\n        PlayerScore playerScore = team.getPlayerScore(player);\n\n        LinkedHashMap<String, Integer> values = new LinkedHashMap<String, Integer>();\n        values.put(\"Points\", playerScore.getPoints());\n        values.put(\"Kills\", playerScore.getKills());\n        values.put(\"Deaths\", playerScore.getDeaths());\n        try {\n            values.put(\"KDR\", (playerScore.getKills() / playerScore.getDeaths()));\n        } catch(ArithmeticException ex) {\n            if (playerScore.getKills() > 0) {\n                values.put(\"KDR\", playerScore.getKills());\n            } else {\n                values.put(\"KDR\", 0);\n            }\n        }\n\n        StringBuilder sb = new StringBuilder(ChatColor.DARK_AQUA + \"Personal score: \");\n        for (Map.Entry<String, Integer> pair : values.entrySet()) {\n            sb.append(String.format(\"%s%s: %s%d\", ChatColor.AQUA, pair.getKey(), ChatColor.GREEN, pair.getValue()));\n            sb.append(\" \");\n        }\n        sender.sendMessage(sb.toString());\n\n    }\n\n\n    private void printPlayersList(CommandSender sender) {\n\n        if (!(sender instanceof Player)) {\n            sender.sendMessage(ChatColor.RED + \"Console can't join a team.\");\n            return;\n        }\n\n        Player player = (Player) sender;\n        Arena arena = plugin.getArenaHandler().getArenaForPlayer(player);\n\n        if (arena == null) {\n            sender.sendMessage(ChatColor.RED + \"You must join an arena first.\");\n            return;\n        }\n\n        for (Team team : arena.getMatch().getTeams().values()) {\n\n            if (team.getPlayers().size() < 1) continue;\n\n            TreeMap<Integer, PlayerScore> ranking = new TreeMap<Integer, PlayerScore>();\n            for (Player p : team.getPlayers()) {\n                PlayerScore ps = team.getPlayerScore(p);\n                ranking.put(ps.getOverallScore(), ps);\n            }\n\n            StringBuilder sb = new StringBuilder(team.getColoredName() + ChatColor.RESET + \": \");\n            for (PlayerScore ps : ranking.values()) {\n                sb.append(String.format(\"%s %s(%d)%s\", ps.getPlayer().getName(), ChatColor.GRAY, ps.getOverallScore(), ChatColor.RESET));\n                if (ranking.lastEntry().getValue() != ps) {\n                    sb.append(\", \");\n                }\n            }\n            sender.sendMessage(sb.toString());\n\n        }\n\n    }\n\n\n    private void timeLeft(CommandSender sender, String[] args) {\n\n        Arena arena = null;\n\n        if (!(sender instanceof Player) && args.length < 1) {\n            sender.sendMessage(ChatColor.RED + \"Specify an arena name to see the time left in the match. (/timeleft <arena>)\");\n            return;\n        }\n\n        if (args.length == 1) {\n            for (Arena a : plugin.getArenaHandler().getArenas()) {\n                if (a.getId().equalsIgnoreCase(args[0])) {\n                    arena = a;\n                    break;\n                }\n            }\n        } else {\n            Player player = (Player) sender;\n            arena = plugin.getArenaHandler().getArenaForPlayer(player);\n        }\n\n        if (arena != null) {\n            String secString = \"00\";\n            String minString = \"00\";\n            if (arena.getMatch().getTimer() != null) {\n                long secondsLeft = arena.getMatch().getTimer().timeLeftInSeconds();\n                long sec = secondsLeft % 60;\n                long min = (secondsLeft / 60) % 60;\n                secString = String.format(\"%02d\", sec);\n                minString = String.format(\"%02d\", min);\n            }\n            sender.sendMessage(String.format(\"%s%s:%s\", ChatColor.DARK_AQUA, minString, secString));\n        } else {\n            sender.sendMessage(ChatColor.RED + \"You must join an arena first, or use /timeleft <arena>\");\n        }\n\n    }\n\n\n    private void teamChat(CommandSender sender, String[] args) {\n\n        if (!(sender instanceof Player) || args.length < 1) {\n            sender.sendMessage(ChatColor.RED + \"Send a message to your team: /t <msg>\");\n            return;\n        } else {\n            ChatModule cm = (ChatModule) plugin.getModule(\"chat\");\n            boolean success = cm.sendTeamMessage((Player) sender, StringUtil.joinArray(\" \", args));\n            if (!success) {\n                sender.sendMessage(ChatColor.RED + \"You must be on a team to do that.\");\n            }\n        }\n\n    }\n\n\n}\nsrc/main/java/io/github/redwallhp/athenagm/hub/Hub.java\npublic class Hub {\n\n\n    private AthenaGM plugin;\n    private HubListener listener;\n    private HubConfiguration config;\n    private WeakReference<World> world;\n    private ItemStack helpBookItem;\n\n\n    public Hub(AthenaGM plugin) {\n\n        this.plugin = plugin;\n        this.world = null;\n        this.listener = new HubListener(plugin, this);\n        this.helpBookItem = loadHelpBookItem();\n\n        try {\n            config = new HubConfiguration(this, new File(plugin.getDataFolder(), \"hub.yml\"));\n        } catch (IOException ex) {\n            plugin.getLogger().warning(ex.getMessage());\n        }\n\n        Bukkit.getScheduler().runTask(this.plugin, new Runnable() {\n            public void run() {\n                loadWorld();\n                loadPortalRegions();\n                updatePortalSigns();\n            }\n        });\n\n    }\n\n\n    /**\n     * Loads the world from disk and prepares it for use\n     */\n    private void loadWorld() {\n        if (config.getWorldName() == null || config.getWorldName().equalsIgnoreCase(\"\")) return;\n        File file = new File(Bukkit.getWorldContainer(), config.getWorldName());\n        if (file.exists()) {\n            try {\n                WorldCreator creator = new WorldCreator(config.getWorldName());\n                creator.generator(new VoidGenerator());\n                creator.environment(World.Environment.NORMAL);\n                creator.generateStructures(false);\n                World world = creator.createWorld();\n                world.setPVP(false);\n                world.setSpawnFlags(false, false); //no mobs\n                world.setKeepSpawnInMemory(false);\n                this.world = new WeakReference<World>(world);\n            } catch (Exception ex) {\n                plugin.getLogger().warning(\"Error loading hub world: \" + ex.getMessage());\n                if (plugin.config.DEBUG) {\n                    ex.printStackTrace();\n                }\n            }\n        }\n    }\n\n\n    /**\n     * Set up the Hub portals.\n     * Creates new Regions for the Hub world, and applies a StringFlag with the value of the arena ID, which\n     * the Region system's PlayerMovementListener can watch for.\n     */\n    private void loadPortalRegions() {\n        if (getWorld() == null || config.getPortals() == null) return;\n        for (HubPortal portal : config.getPortals()) {\n            CuboidRegion region = new CuboidRegion(portal.getArena().getId(), getWorld(), portal.getStart(), portal.getEnd());\n            StringFlag portalFlag = new StringFlag(\"join_arena\");\n            portalFlag.setValue(portal.getArena().getId());\n            region.setFlag(portalFlag);\n            plugin.getRegionHandler().addRegion(region);\n        }\n    }\n\n\n    /**\n     * Update the arena information on Hub signs every couple of seconds\n     */\n    private void updatePortalSigns() {\n        if (config.getSigns() == null || config.getSigns().size() < 1 || getWorld() == null) return;\n        Bukkit.getScheduler().runTaskTimer(this.plugin, new Runnable() {\n            public void run() {\n                for (HubSign sign : config.getSigns()) {\n                    sign.update();\n                }\n            }\n        }, 40L, 40L);\n    }\n\n\n    /**\n     * Handle player spawning on join. Ensures a player rejoining will always go to the Hub and not a stale Match.\n     * Order of priority:\n     * 1. If dedicated mode is on, put the player directly in the specified arena if it exists. Fall back to defaul world.\n     * 2. If the world specified in hub.yml exists and is loaded, spawn the player there.\n     * 3. If all else fails, teleport them to the default world.\n     */\n    public void spawnPlayer(Player player) {\n        playerSetUp(player);\n        if (plugin.config.DEDICATED_ARENA != null) {\n            // If dedicated mode is on, put the player directly in an arena, as a hub is not desired\n            for (Arena arena : plugin.getArenaHandler().getArenas()) {\n                if (arena.getId().equalsIgnoreCase(plugin.config.DEDICATED_ARENA)) {\n                    player.teleport(arena.getWorld().getSpawnLocation());\n                    return;\n                }\n            }\n            player.teleport(Bukkit.getWorld(\"world\").getSpawnLocation());\n        } else if (getWorld() != null) {\n            // Spawn the player in the loaded hub world if the world specified in hub.yml exists\n            player.teleport(getWorld().getSpawnLocation());\n        } else {\n            // Spawn the player in the default world if all else fails\n            player.teleport(Bukkit.getWorld(\"world\").getSpawnLocation());\n        }\n    }\n\n\n    /**\n     * Reset player attributes and visibility\n     */\n    public void playerSetUp(Player player) {\n        PlayerUtil.resetPlayer(player);\n        player.setGameMode(GameMode.ADVENTURE);\n        player.spigot().setCollidesWithEntities(false);\n        player.setScoreboard(Bukkit.getScoreboardManager().getNewScoreboard());\n        for (Player p : Bukkit.getServer().getOnlinePlayers()) p.showPlayer(player);\n        for (Player p : Bukkit.getServer().getOnlinePlayers()) player.showPlayer(p);\n        if (config.isHelpBook()) {\n            player.getInventory().addItem(helpBookItem);\n        }\n    }\n\n\n    /**\n     * Load the help book for the Hub\n     */\n    private ItemStack loadHelpBookItem() {\n        File file = new File(plugin.getDataFolder(), \"helpbook.txt\");\n        BookBuilder bookBuilder = new BookBuilder(\"Help\");\n        bookBuilder.setDefaultContents(\"This book will be populated with the contents of a &lhelpbook.txt&0 file in the plugin directory.\");\n        bookBuilder.setPagesFromFile(file);\n        this.helpBookItem = bookBuilder.getBook(); //cache to avoid hitting the disk all the time\n        return bookBuilder.getBook();\n    }\n\n\n    /**\n     * Get an instance of the Hub world\n     * @return Returns the World if applicable or null if a World is not loaded\n     */\n    public World getWorld() {\n        if (world != null) {\n            return world.get();\n        } else {\n            return null;\n        }\n    }\n\n\n    /**\n     * Convenience method to check if a player is in the Hub world\n     * @param player Player to check\n     * @return True if the player is in the Hub world\n     */\n    public boolean hasPlayer(Player player) {\n        return getWorld() != null && player.getWorld().equals(getWorld());\n    }\n\n\n    /**\n     * Get the plugin instance\n     */\n    public AthenaGM getPlugin() {\n        return this.plugin;\n    }\n\n\n    /**\n     * Get the HubConfiguration instance\n     */\n    public HubConfiguration getConfig() {\n        return config;\n    }\n\n\n}\nsrc/main/java/io/github/redwallhp/athenagm/modules/Module.java\npublic interface Module extends Listener {\n\n\n    /**\n     * A string identifier for the module, to be used for getting instances with\n     * the getModule() method.\n     * @see ModuleLoader\n     */\n    String getModuleName();\n\n\n    /**\n     * Called when the plugin unloads. This is a good place to clean up database connections, etc.\n     */\n    void unload();\n\n\n}\nsrc/main/java/io/github/redwallhp/athenagm/commands/AdminCommands.java\npublic class AdminCommands implements CommandExecutor {\n\n\n    private AthenaGM plugin;\n\n\n    public AdminCommands(AthenaGM plugin) {\n        this.plugin = plugin;\n        plugin.getCommand(\"athena\").setExecutor(this);\n        plugin.getCommand(\"region\").setExecutor(this);\n    }\n\n\n    public boolean onCommand(CommandSender sender, Command cmd, String label, String[] args) {\n\n        if (cmd.getName().equalsIgnoreCase(\"athena\")) {\n            if (args.length == 0) {\n                sender.sendMessage(ChatColor.RED + \"Valid subcommands: reload, forcestart, changemap, checkperms\");\n            }\n            else if (args[0].equalsIgnoreCase(\"reload\")) {\n                reloadCommand(sender, args);\n            }\n            else if (args[0].equalsIgnoreCase(\"forcestart\")) {\n                forceStartCommand(sender, args);\n            }\n            else if (args[0].equalsIgnoreCase(\"changemap\")) {\n                changeMapCommand(sender, args);\n            }\n            else if (args[0].equalsIgnoreCase(\"checkperms\")) {\n                checkPerms(sender, args);\n            }\n            return true;\n        }\n\n        if (cmd.getName().equalsIgnoreCase(\"region\")) {\n            if (args.length == 0) {\n                sender.sendMessage(ChatColor.RED + \"Valid subcommands: info, select\");\n            }\n            else if (args[0].equalsIgnoreCase(\"info\")) {\n                regionInfoCommand(sender, args);\n            }\n            else if (args[0].equalsIgnoreCase(\"select\")) {\n                regionSelectCommand(sender, args);\n            }\n            return true;\n        }\n\n        return false;\n\n    }\n\n\n    private void reloadCommand(CommandSender sender, String[] args) {\n\n        if (args.length < 2) {\n            sender.sendMessage(ChatColor.RED + \"Usage: /athena reload <what>\");\n            sender.sendMessage(ChatColor.RED + \"What: permissions, helpbook\");\n            return;\n        }\n\n        String what = args[1];\n\n        if (what.equalsIgnoreCase(\"permissions\")) {\n            PermissionsModule module = (PermissionsModule) plugin.getModule(\"permissions\");\n            module.reloadPermissions();\n            sender.sendMessage(ChatColor.LIGHT_PURPLE + \"Permissions reloaded.\");\n        }\n\n        if (what.equalsIgnoreCase(\"helpbook\")) {\n            SpectatorModule module = (SpectatorModule) plugin.getModule(\"spectator\");\n            module.getHelpBookItem();\n            sender.sendMessage(ChatColor.LIGHT_PURPLE + \"Spectator help book reloaded.\");\n        }\n\n    }\n\n\n    private void forceStartCommand(CommandSender sender, String[] args) {\n\n        if (args.length < 2) {\n            sender.sendMessage(ChatColor.RED + \"Usage: /athena forcestart <arena>\");\n            return;\n        }\n\n        for (Arena arena : plugin.getArenaHandler().getArenas()) {\n            if (arena.getId().equalsIgnoreCase(args[1])) {\n                sender.sendMessage(ChatColor.DARK_AQUA + \"Forcing match start.\");\n                arena.getMatch().startCountdown();\n                break;\n            }\n        }\n\n    }\n\n\n    private void changeMapCommand(CommandSender sender, String[] args) {\n\n        if (args.length < 2) {\n            sender.sendMessage(ChatColor.RED + \"Usage: /athena changemap <arena> <map>\");\n            return;\n        }\n\n        for (Arena arena : plugin.getArenaHandler().getArenas()) {\n            if (arena.getId().equalsIgnoreCase(args[1])) {\n                boolean success = arena.forceMapChange(args[2]);\n                if (!success) {\n                    sender.sendMessage(ChatColor.RED + \"There is no configured map by that name.\");\n                }\n                break;\n            }\n        }\n\n    }\n\n\n    private void checkPerms(CommandSender sender, String[] args) {\n\n        if (args.length < 2) {\n            sender.sendMessage(ChatColor.RED + \"Usage: /athena checkperms <player>\");\n        }\n\n        Player player = plugin.getServer().getPlayer(args[1]);\n        if (player != null) {\n            PermissionsModule module = (PermissionsModule) plugin.getModule(\"permissions\");\n            sender.sendMessage(\"Results printed to console.\");\n            plugin.getServer().getLogger().info(String.format(\"--- Permission Check for %s ---\", player.getName()));\n            plugin.getServer().getLogger().info(\"UUID: \" + player.getUniqueId());\n            if (module.getUsers().containsKey(player.getUniqueId())) {\n                String group = module.getUsers().get(player.getUniqueId()).getGroup();\n                plugin.getServer().getLogger().info(\"Group: \" + group);\n            }\n            for (PermissionAttachmentInfo info : player.getEffectivePermissions()) {\n                plugin.getServer().getLogger().info(\"* \" + info.getPermission());\n            }\n            plugin.getServer().getLogger().info(\"--- End Permission Check ---\");\n        } else {\n            sender.sendMessage(ChatColor.RED + \"Could not find online player.\");\n        }\n\n    }\n\n\n    private void regionInfoCommand(CommandSender sender, String[] args) {\n\n        if (!(sender instanceof Player)) {\n            sender.sendMessage(ChatColor.RED + \"Console can't run this command.\");\n            return;\n        }\n\n        Player player = (Player) sender;\n\n        Location loc = player.getLocation();\n        CuboidRegion rg = plugin.getRegionHandler().getApplicableRegion(loc.getWorld(), loc.toVector());\n        if (rg != null) {\n            StringBuilder inherited = new StringBuilder();\n            List<CuboidRegion> all = plugin.getRegionHandler().getAllApplicableRegions(loc);\n            for (CuboidRegion ir : all) {\n                inherited.append(ir.getName());\n                if (all.indexOf(ir) < all.size() - 1) inherited.append(\", \");\n            }\n            sender.sendMessage(String.format(\"%sYou are standing in '%s'\", ChatColor.DARK_AQUA, rg.getName()));\n            sender.sendMessage(String.format(\"%sInherits from: %s\", ChatColor.DARK_AQUA, inherited.toString()));\n            sender.sendMessage(String.format(\"%sStart: %s, End: %s\", ChatColor.DARK_AQUA, rg.getStart().toString(), rg.getEnd().toString()));\n        }\n\n    }\n\n\n    private void regionSelectCommand(CommandSender sender, String[] args) {\n\n        if (!(sender instanceof Player)) {\n            sender.sendMessage(ChatColor.RED + \"Console can't run this command.\");\n            return;\n        }\n\n        Player player = (Player) sender;\n\n        if (args.length < 2) {\n            sender.sendMessage(ChatColor.RED + \"Usage: /region select <name>\");\n            return;\n        }\n\n        CuboidRegion rg = plugin.getRegionHandler().getRegion(player.getWorld(), args[1]);\n        if (rg == null) {\n            sender.sendMessage(String.format(\"%sCould not find region \\\"%s\\\"\", ChatColor.RED, args[1]));\n            return;\n        }\n\n        if (plugin.getWE() == null) {\n            sender.sendMessage(ChatColor.RED + \"WorldEdit does not appear to be installed.\");\n        } else {\n            WorldEditUtil.setPlayerSelection(player, rg.getWorld(), rg.getMin(), rg.getMax());\n            sender.sendMessage(String.format(\"%sSelected region \\\"%s\\\"\", ChatColor.DARK_AQUA, rg.getName()));\n        }\n\n    }\n\n\n}\nsrc/main/java/io/github/redwallhp/athenagm/arenas/ArenaHandler.java\npublic class ArenaHandler {\n\n\n    private AthenaGM plugin;\n    private Map<String, Arena> arenas;\n\n\n    public ArenaHandler(AthenaGM plugin) {\n        this.plugin = plugin;\n        this.arenas = new HashMap<>();\n        MapLoader.cleanUpWorldInstances(plugin.getMatchesDirectory());\n        loadArenas();\n        plugin.getServer().getPluginManager().registerEvents(new ArenaListener(this), plugin);\n    }\n\n\n    private void loadArenas() {\n        for (ConfiguredArena ca : plugin.config.ARENAS.values()) {\n            Arena arena = new Arena(this, ca);\n            this.arenas.put(arena.getId(), arena);\n        }\n    }\n\n\n    public AthenaGM getPluginInstance() {\n        return this.plugin;\n    }\n\n\n    public Set<Arena> getArenas() {\n        return new HashSet<>(this.arenas.values());\n    }\n\n\n    public Arena getArenaForPlayer(Player player) {\n        for (Arena arena : this.arenas.values()) {\n            if (player.getWorld().equals(arena.getWorld())) {\n                return arena;\n            }\n        }\n        return null;\n    }\n\n\n    public Arena getArenaById(String id) {\n        if (this.arenas.containsKey(id)) {\n            return this.arenas.get(id);\n        }\n        return null;\n    }\n\n\n}\nsrc/main/java/io/github/redwallhp/athenagm/commands/ArenaCommands.java\npublic class ArenaCommands implements CommandExecutor {\n\n\n    private AthenaGM plugin;\n\n\n    public ArenaCommands(AthenaGM plugin) {\n        this.plugin = plugin;\n        plugin.getCommand(\"hub\").setExecutor(this);\n        plugin.getCommand(\"arenas\").setExecutor(this);\n        plugin.getCommand(\"join\").setExecutor(this);\n        plugin.getCommand(\"votemap\").setExecutor(this);\n        plugin.getCommand(\"votenext\").setExecutor(this);\n        plugin.getCommand(\"vote\").setExecutor(this);\n    }\n\n\n    public boolean onCommand(CommandSender sender, Command cmd, String label, String[] args) {\n\n        if (cmd.getName().equalsIgnoreCase(\"hub\")) {\n            if (sender instanceof Player) {\n                plugin.getHub().spawnPlayer((Player) sender);\n            }\n            return true;\n        }\n\n        if (cmd.getName().equalsIgnoreCase(\"arenas\")) {\n            listArenas(sender);\n            return true;\n        }\n\n        if (cmd.getName().equalsIgnoreCase(\"join\")) {\n            if (args.length == 1) {\n                joinArena(sender, args[0]);\n            } else {\n                sender.sendMessage(ChatColor.RED + \"Usage: /join <arena id>\");\n            }\n            return true;\n        }\n\n        if (cmd.getName().equalsIgnoreCase(\"votemap\")) {\n            voteMap(sender, args);\n            return true;\n        }\n\n        if (cmd.getName().equalsIgnoreCase(\"votenext\")) {\n            voteNext(sender);\n            return true;\n        }\n\n        if (cmd.getName().equalsIgnoreCase(\"vote\")) {\n            if (args.length == 1) {\n                vote(sender, args[0]);\n            } else {\n                sender.sendMessage(ChatColor.RED + \"Usage: /vote <choice>\");\n            }\n            return true;\n        }\n\n        return false;\n\n    }\n\n\n    private void listArenas(CommandSender sender) {\n        for (Arena arena : plugin.getArenaHandler().getArenas()) {\n            int players = arena.getMatch().getTotalPlayers();\n            StringBuilder head = new StringBuilder(\"\" + ChatColor.STRIKETHROUGH);\n            head.append(\"--- \");\n            head.append(ChatColor.GRAY);\n            head.append(ChatColor.BOLD);\n            head.append(arena.getName());\n            head.append(ChatColor.RESET);\n            head.append(ChatColor.STRIKETHROUGH);\n            head.append(\" ---\");\n            String id = String.format(\"%sID: %s%s\", ChatColor.GRAY, ChatColor.WHITE, arena.getId());\n            String map = String.format(\"%sMap: %s%s\", ChatColor.GRAY, ChatColor.WHITE, arena.getMatch().getMap().getName());\n            String pl = String.format(\"%sPlayers: %s%d/%d\", ChatColor.GRAY, ChatColor.WHITE, players, arena.getMaxPlayers());\n            sender.sendMessage(head.toString());\n            sender.sendMessage(id + \" \" + map + \" \" + pl);\n        }\n        sender.sendMessage(ChatColor.DARK_AQUA + String.format(\"Use %s/join <id>%s to join an arena.\", ChatColor.GREEN, ChatColor.DARK_AQUA));\n    }\n\n\n    private void joinArena(CommandSender sender, String id) {\n        if (!(sender instanceof Player)) {\n            sender.sendMessage(ChatColor.RED + \"Console can't join an arena.\");\n            return;\n        }\n        for (Arena arena : plugin.getArenaHandler().getArenas()) {\n            if (arena.getId().equals(id)) {\n                Player player = (Player) sender;\n                Location loc = arena.getMatch().getSpawnPoint(player);\n                player.teleport(loc);\n                return;\n            }\n        }\n        sender.sendMessage(ChatColor.RED + String.format(\"Could not find arena '%s'\", id));\n    }\n\n\n    private void voteMap(CommandSender sender, String[] args) {\n        if (!(sender instanceof Player)) {\n            sender.sendMessage(ChatColor.RED + \"Console can't vote.\");\n            return;\n        }\n        Player player = (Player) sender;\n        Arena arena = plugin.getArenaHandler().getArenaForPlayer(player);\n        String map = (args.length == 1) ? args[0] : \"\";\n        VotingModule module = (VotingModule) plugin.getModule(\"voting\");\n        if (arena != null) {\n            module.createMapVote(arena, player, map);\n        }\n    }\n\n\n    private void voteNext(CommandSender sender) {\n        if (!(sender instanceof Player)) {\n            sender.sendMessage(ChatColor.RED + \"Console can't vote.\");\n            return;\n        }\n        Player player = (Player) sender;\n        Arena arena = plugin.getArenaHandler().getArenaForPlayer(player);\n        VotingModule module = (VotingModule) plugin.getModule(\"voting\");\n        if (arena != null) {\n            module.createNextMapVote(arena, player);\n        }\n    }\n\n\n    private void vote(CommandSender sender, String choice) {\n        if (!(sender instanceof Player)) {\n            sender.sendMessage(ChatColor.RED + \"Console can't vote.\");\n            return;\n        }\n        Player player = (Player) sender;\n        Arena arena = plugin.getArenaHandler().getArenaForPlayer(player);\n        VotingModule module = (VotingModule) plugin.getModule(\"voting\");\n        if (arena != null) {\n            module.vote(arena, player, choice);\n        }\n    }\n\n\n}\nsrc/main/java/io/github/redwallhp/athenagm/regions/RegionHandler.java\npublic class RegionHandler {\n\n\n    private AthenaGM plugin;\n    private HashMap<UUID, LinkedHashMap<String, CuboidRegion>> regions;\n\n\n    public RegionHandler(AthenaGM plugin) {\n        this.plugin = plugin;\n        this.regions = new HashMap<UUID, LinkedHashMap<String, CuboidRegion>>();\n        listen();\n    }\n\n\n    private void listen() {\n        new BlockPlaceListener(this);\n        new BlockBreakListener(this);\n        new PlayerMovementListener(this);\n        new PlayerInteractListener(this);\n        new EnvironmentalListener(this);\n        new PistonListener(this);\n    }\n\n\n    /**\n     * Add a region to the map of currently loaded regions\n     */\n    public void addRegion(CuboidRegion region) {\n        UUID worldId = region.getWorld().getUID();\n        if (!regions.containsKey(worldId)) regions.put(worldId, new LinkedHashMap<String, CuboidRegion>());\n        regions.get(worldId).put(region.getName(), region);\n    }\n\n\n    /**\n     * Remove a region from the map of currently loaded regions\n     */\n    public void removeRegion(CuboidRegion region) {\n        UUID worldId = region.getWorld().getUID();\n        if (regions.containsKey(worldId)) {\n            regions.get(worldId).remove(region.getName());\n            if (regions.get(worldId).size() < 1) regions.remove(worldId);\n        }\n    }\n\n\n    /**\n     * Get a region by its name\n     */\n    public CuboidRegion getRegion(World world, String name) {\n        if (regions.containsKey(world.getUID())) {\n            return regions.get(world.getUID()).get(name);\n        } else {\n            return null;\n        }\n    }\n\n\n    /**\n     * Returns a list of all regions that intersect a given vector point in a world.\n     * @param world The world the vector is in\n     * @param vector The point to check for applicable regions\n     * @return A list of region objects\n     */\n    public List<CuboidRegion> getAllApplicableRegions(World world, Vector vector) {\n        List<CuboidRegion> inRegions = new ArrayList<CuboidRegion>();\n        if (regions.containsKey(world.getUID())) {\n            for (CuboidRegion region : regions.get(world.getUID()).values()) {\n                if (region.contains(world, vector)) inRegions.add(region);\n            }\n        }\n        return inRegions;\n    }\n\n\n    /**\n     * Alternate getAllApplicableRegions() method that takes a Location instead of a Vector.\n     * Returns a list of all regions that intersect a given vector point in a world.\n     * @param location A Location to check for applicable regions.\n     * @return A list of region objects\n     */\n    public List<CuboidRegion> getAllApplicableRegions(Location location) {\n        return getAllApplicableRegions(location.getWorld(), location.toVector());\n    }\n\n\n    /**\n     * Returns the region with the highest priority that intersects a given vector point.\n     * Priority is determined by the priority value in the YAML. If a priority is not specified,\n     * it will try to fall back to the configured ordinal index.\n     * Regions with a positive priority will take priority over all that don't specify one.\n     * @param world The world the vector is in\n     * @param vector The point to check for an applicable region\n     * @return A region object, or null if one does not exist at the vector's location\n     */\n    public CuboidRegion getApplicableRegion(World world, Vector vector) {\n\n        // Get the highest priority region\n        List<CuboidRegion> applicableRegions = getAllApplicableRegions(world, vector);\n        TreeMap<Integer, CuboidRegion> regionMap = new TreeMap<Integer, CuboidRegion>();\n        int i = 0;\n        for (CuboidRegion rg : applicableRegions) {\n            if (rg.getPriority() > 0) {\n                regionMap.put(rg.getPriority()+applicableRegions.size()+1, rg);\n            } else {\n                regionMap.put(i, rg);\n            }\n            i++;\n        }\n        if (regionMap.size() < 1) return null;\n        CuboidRegion r = regionMap.lastEntry().getValue();\n\n        // Create and return a compound region that inherits flags from lower regions\n        CuboidRegion compound = new CuboidRegion(r.getName(), r.getWorld(), r.getStart(), r.getEnd());\n        for (CuboidRegion rg : regionMap.values()) {\n            for (Map.Entry<String, Flag<?>> flag : rg.getFlags().entrySet()) {\n                compound.setFlag(flag.getValue());\n            }\n        }\n        return compound;\n\n    }\n\n\n    /**\n     * Alternate getApplicableRegion method that takes a Location instead of a Vector.\n     * Returns the region with the highest priority that intersects a given vector point.\n     * If the regions have equal priority, their ordinal index will be used instead, with\n     * later regions superseding previous ones.\n     * @param location A Location to check for an applicable region.\n     * @return A region object, or null if one does not exist at the vector's location\n     */\n    public CuboidRegion getApplicableRegion(Location location) {\n        return getApplicableRegion(location.getWorld(), location.toVector());\n    }\n\n\n    /**\n     * Load configured regions for a map. This is called when a new match starts.\n     */\n    public void loadRegions(World world, GameMap map) {\n        for (MapInfoRegion mir : map.getRegions().values()) {\n            CuboidRegion region = new CuboidRegion(mir.getName(), world, mir.getStart(), mir.getEnd());\n            region.setFlags(mir.getFlags());\n            region.setPriority(mir.getPriority());\n            addRegion(region);\n        }\n    }\n\n\n    /**\n     * Unload regions belonging to a world. Called when a match world is destructed.\n     */\n    public void unloadRegions(World world) {\n        if (regions.containsKey(world.getUID())) {\n            Iterator<CuboidRegion> iterator = regions.get(world.getUID()).values().iterator();\n            while (iterator.hasNext()) {\n                CuboidRegion region = iterator.next();\n                if (region.getWorld().getUID() == world.getUID()) {\n                    iterator.remove();\n                }\n            }\n            if (regions.get(world.getUID()).size() < 1) regions.remove(world.getUID());\n        }\n    }\n\n\n    /**\n     * Get the plugin reference\n     */\n    public AthenaGM getPlugin() {\n        return plugin;\n    }\n\n\n}\nsrc/main/java/io/github/redwallhp/athenagm/maps/VoidGenerator.java\npublic class VoidGenerator extends ChunkGenerator {\n\n    @Override\n    public byte[][] generateBlockSections(World world, Random random, int x, int z, BiomeGrid biomes) {\n        return new byte[16][];\n    }\n\n}\nsrc/main/java/io/github/redwallhp/athenagm/modules/ModuleLoader.java\npublic class ModuleLoader {\n\n\n    /**\n     * List of module classes to load\n     */\n    private static Class[] moduleClasses = {\n            PermissionsModule.class,\n            SpectatorModule.class,\n            KitsModule.class,\n            ScoreboardModule.class,\n            FriendlyFireModule.class,\n            BroadcastsModule.class,\n            PlayerFreezeModule.class,\n            DeathMessageModule.class,\n            WorldBorderModule.class,\n            ChatModule.class,\n            VotingModule.class\n    };\n\n\n    private AthenaGM plugin;\n    private HashMap<String, Module> modules;\n\n\n    public ModuleLoader(AthenaGM plugin) {\n        this.plugin = plugin;\n        this.modules = new HashMap<String, Module>();\n        load();\n    }\n\n\n    /**\n     * Build and load modules, registering their events and stashing references for later use.\n     * Modules are cast to their Module interface to make this sorcery work, but can be\n     * recast to their original class later in order to access methods.\n     */\n    @SuppressWarnings(\"unchecked\")\n    public void load() {\n        for (Class c : moduleClasses) {\n            try {\n                plugin.getLogger().info(String.format(\"Loading module: %s\", c.getName()));\n                Module module = (Module) c.getConstructor(AthenaGM.class).newInstance(plugin); //evil sorcery\n                plugin.getServer().getPluginManager().registerEvents(module, plugin);\n                modules.put(module.getModuleName(), module);\n            } catch (Exception ex) {\n                plugin.getLogger().warning(String.format(\"Error loading module '%s\", c.getName()));\n                ex.printStackTrace();\n            }\n        }\n    }\n\n\n    /**\n     * Iterate the loaded modules and call the unload() method each one implements.\n     */\n    public void unload() {\n        for (Module module : modules.values()) {\n            module.unload();\n        }\n    }\n\n\n    /**\n     * Get the module with a specified name.\n     * The returned object will be a Module, but you can recast it to the module's original class\n     * in order to access its methods. e.g. casting the module found by getting \"permissions\"\n     * to be a PermissionsModule will allow you to access its member methods.\n     * @param name Name of the module, as returned by its getModuleName() method\n     */\n    public Module getModule(String name) {\n        return modules.get(name);\n    }\n\n\n}\nsrc/main/java/io/github/redwallhp/athenagm/configuration/Configuration.java\npublic class Configuration {\n\n    private final AthenaGM plugin;\n    public boolean DEBUG;\n    public String NETWORK_NAME;\n    public String DEDICATED_ARENA;\n    public boolean VOTING;\n    public HashMap<String, ConfiguredArena> ARENAS;\n\n    public Configuration(AthenaGM instance) {\n        plugin = instance;\n        plugin.saveDefaultConfig();\n        this.load();\n    }\n\n    /**\n     * Save the config out to disk\n     */\n    public void save() {\n        plugin.saveConfig();\n    }\n\n    /**\n     * Load the configuration from disk and set the simple keys to public properties\n     */\n    public void load() {\n        plugin.reloadConfig();\n        DEBUG = plugin.getConfig().getBoolean(\"debug\", false);\n        NETWORK_NAME = plugin.getConfig().getString(\"network_name\", \"Server\");\n        DEDICATED_ARENA = plugin.getConfig().getString(\"dedicated_arena\", null);\n        VOTING = plugin.getConfig().getBoolean(\"voting\", true);\n        getArenas();\n    }\n\n    /**\n     * Handle the loading of arena configuration blocks, creating ConfiguredArena objects\n     */\n    private void getArenas() {\n        this.ARENAS = new HashMap<String, ConfiguredArena>();\n        Set<String> ids = plugin.getConfig().getConfigurationSection(\"arenas\").getKeys(false);\n        if (ids.size() > 0) {\n            for (String id : ids) {\n                String name = plugin.getConfig().getString(String.format(\"arenas.%s.name\", id), \"Default Arena\");\n                String gameMode = plugin.getConfig().getString(String.format(\"arenas.%s.gamemode\", id), \"deathmatch\");\n                Integer timeLimit = plugin.getConfig().getInt(String.format(\"arenas.%s.time_limit\", id), 600);\n                List<String> mapList = plugin.getConfig().getStringList(String.format(\"arenas.%s.maps\", id));\n                ConfiguredArena arena = new ConfiguredArena(id, name, gameMode, timeLimit, mapList);\n                this.ARENAS.put(id, arena);\n            }\n        }\n    }\n\n}\nsrc/main/java/io/github/redwallhp/athenagm/tracker/Tracker.java\npublic class Tracker implements Listener {\n\n\n    private AthenaGM plugin;\n    private HashMap<UUID, PlayerDamagePlayerEvent> lastDamage;\n\n\n    public Tracker(AthenaGM plugin) {\n        this.plugin = plugin;\n        this.lastDamage = new HashMap<UUID, PlayerDamagePlayerEvent>();\n        plugin.getServer().getPluginManager().registerEvents(this, plugin);\n    }\n\n\n    /**\n     * Fire PlayerDamagePlayerEvent when a player attacks another player\n     * while in an ongoing match.\n     * @see PlayerDamagePlayerEvent\n     */\n    @EventHandler(priority = EventPriority.LOW)\n    public void triggerPlayerDamagePlayerEvent(EntityDamageByEntityEvent event) {\n\n        Player victim = null;\n        Player attacker = null;\n        boolean ranged = false;\n\n        // Melee\n        if (event.getEntityType().equals(EntityType.PLAYER) && event.getDamager().getType().equals(EntityType.PLAYER)) {\n            victim = (Player) event.getEntity();\n            attacker = (Player) event.getDamager();\n        }\n\n        // Projectile\n        if (event.getEntityType().equals(EntityType.PLAYER) && event.getCause().equals(EntityDamageEvent.DamageCause.PROJECTILE)) {\n            Projectile a = (Projectile) event.getDamager();\n            if (a.getShooter() instanceof Player) {\n                victim = (Player) event.getEntity();\n                attacker = (Player) a.getShooter();\n                ranged = true;\n            }\n        }\n\n        // Potion\n        if (event.getEntityType().equals(EntityType.PLAYER) && event.getCause().equals(EntityDamageEvent.DamageCause.MAGIC)) {\n            ThrownPotion potion = (ThrownPotion) event.getDamager();\n            if (potion.getShooter() instanceof Player) {\n                victim = (Player) event.getEntity();\n                attacker = (Player) potion.getShooter();\n                ranged = true;\n            }\n        }\n\n        if (attacker == null || victim == null) return;\n        Arena arena = plugin.getArenaHandler().getArenaForPlayer(victim);\n        if (arena == null) return;\n\n        PlayerDamagePlayerEvent e = new PlayerDamagePlayerEvent(arena.getMatch(), attacker, victim, ranged, event);\n        Bukkit.getPluginManager().callEvent(e);\n        if (e.isCancelled()) {\n            event.setCancelled(true);\n        } else {\n            lastDamage.put(victim.getUniqueId(), e);\n        }\n\n    }\n\n\n    /**\n     * Fire AthenaDeathEvent when a player dies during a match.\n     * @see AthenaDeathEvent\n     */\n    @EventHandler(priority = EventPriority.LOW)\n    public void triggerAthenaDeathEvent(PlayerDeathEvent event) {\n\n        Player player = event.getEntity();\n        Arena arena = plugin.getArenaHandler().getArenaForPlayer(player);\n        if (arena == null) return;\n        AthenaDeathEvent e;\n\n        PlayerDamagePlayerEvent damage = lastDamage.get(player.getUniqueId());\n        boolean wasPlayer = damage != null && System.currentTimeMillis() - damage.getTime() <= 7500;\n        if (wasPlayer) {\n            e = new AthenaDeathEvent(arena.getMatch(), damage.getVictim(), damage.getDamager(), damage, event);\n        } else {\n            e = new AthenaDeathEvent(arena.getMatch(), player, event);\n        }\n\n        Bukkit.getPluginManager().callEvent(e);\n\n    }\n\n\n}\n", "answers": ["    private ArenaHandler arenaHandler;"], "length": 3929, "dataset": "repobench-p_e", "language": "java", "all_classes": null, "_id": "557d3ab70793b587bdf62fa00270e094d9787de7fba71c44"}
{"input": "package nami.beitrag.gui;\nimport java.awt.Component;\nimport java.awt.event.ActionEvent;\nimport java.awt.event.ActionListener;\nimport java.awt.event.ItemEvent;\nimport java.awt.event.ItemListener;\nimport java.text.DateFormat;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.Date;\nimport java.util.TreeSet;\nimport javax.swing.ButtonGroup;\nimport javax.swing.JButton;\nimport javax.swing.JCheckBox;\nimport javax.swing.JFrame;\nimport javax.swing.JLabel;\nimport javax.swing.JPanel;\nimport javax.swing.JRadioButton;\nimport javax.swing.JScrollPane;\nimport javax.swing.JTextField;\nimport javax.swing.ListSelectionModel;\nimport javax.swing.border.EmptyBorder;\nimport javax.swing.border.TitledBorder;\nimport javax.swing.event.TreeSelectionEvent;\nimport javax.swing.event.TreeSelectionListener;\nimport javax.swing.tree.TreePath;\nimport nami.beitrag.db.BeitragLastschrift;\nimport nami.beitrag.db.BeitragMitglied;\nimport nami.beitrag.db.BeitragRechnung;\nimport nami.beitrag.db.BeitragSammelLastschrift;\nimport nami.beitrag.db.BeitragSepaMandat;\nimport nami.beitrag.db.LastschriftenMapper;\nimport nami.beitrag.db.LastschriftenMapper.DataMandateRechnungen;\nimport nami.beitrag.db.LastschriftenMapper.DataRechnungMitglied;\nimport nami.beitrag.db.LastschriftenMapper.FilterSettings;\nimport nami.beitrag.gui.utils.Colors;\nimport nami.beitrag.gui.utils.DisabledCellRenderer;\nimport nami.beitrag.gui.utils.MyStringUtils;\nimport net.miginfocom.swing.MigLayout;\nimport org.apache.commons.lang3.StringUtils;\nimport org.apache.ibatis.session.SqlSession;\nimport org.apache.ibatis.session.SqlSessionFactory;\nimport org.jdesktop.swingx.JXTreeTable;\nimport org.jdesktop.swingx.decorator.ColorHighlighter;\nimport org.jdesktop.swingx.decorator.ComponentAdapter;\nimport org.jdesktop.swingx.decorator.HighlightPredicate;\nimport org.jdesktop.swingx.decorator.Highlighter;\nimport org.jdesktop.swingx.treetable.AbstractTreeTableModel;\nimport com.toedter.calendar.JDateChooser;\n\n\n\n\n\n\n/**\n * Stellt ein Fenster dar, in dem Sammellastschriften zusammengestellt werden\n * können. Dazu werden nach bestimmten Kriterien offene Rechnungen aus der\n * Datenbank geholt und angezeigt. Der Benutzer kann manuell Rechnungen\n * deaktivieren und lässt anschließend die Sammellastschrift erstellen.\n * \n * @author Fabian Lipp\n * \n */\npublic class LastschriftErstellenWindow extends JFrame {\n    private static final long serialVersionUID = 7409328875312329467L;\n\n    private final SqlSessionFactory sqlSessionFactory;\n\n    // Komponenten für Suche\n    private JCheckBox chckbxRechnungsdatum;\n    private JDateChooser inputRechnungsdatum;\n    private JCheckBox chckbxBereitsErstellt;\n\n    // Komponenten für Tabelle\n    private JXTreeTable treeTable;\n    // Model der Tabelle\n    private LastschriftTreeTableModel treeTableModel;\n    // Mandat, dessen Zeile momentan in der Tabelle ausgewählt ist (null, falls\n    // keine Zeile oder kein Mandat ausgewählt ist)\n    private MandatNode selectedMandat;\n\n    // Komponenten für \"Lastschrift erstellen\"\n    private JDateChooser inputFaelligkeit;\n    private JTextField inputBezeichnung;\n    private JRadioButton rdbtnMitgliedernamen;\n    private JRadioButton rdbtnRechnungsnummer;\n\n    /**\n     * Erzeugt ein neues Rechnungs-Fenster.\n     * \n     * @param sqlSessionFactory\n     *            Zugriff auf die Datenbank\n     */\n    public LastschriftErstellenWindow(SqlSessionFactory sqlSessionFactory) {\n        super(\"Sammellastschrift erstellen\");\n        this.sqlSessionFactory = sqlSessionFactory;\n        buildFrame();\n    }\n\n    private void buildFrame() {\n        JPanel contentPane = new JPanel();\n        contentPane.setBorder(new EmptyBorder(5, 5, 5, 5));\n        setContentPane(contentPane);\n        contentPane.setLayout(new MigLayout(\"\", \"[grow]\", \"[][grow][][]\"));\n\n        /*** Rechnungs-Suche ***/\n        contentPane.add(createSearchPanel(), \"cell 0 0,grow\");\n\n        /*** Tabelle vorbereiten ***/\n        JScrollPane scrollPane = new JScrollPane();\n        contentPane.add(scrollPane, \"cell 0 1,grow\");\n        treeTable = new JXTreeTable(new LastschriftTreeTableModel());\n        // Model initialisieren\n        treeTableModel = new LastschriftTreeTableModel();\n        treeTable.setTreeTableModel(treeTableModel);\n        // Verhindert, dass die Spalten neu initialisiert werden, wenn sich das\n        // Model verändert (dabei gehen der TableCellRenderer und die\n        // Spaltenbreiten verloren)\n        treeTable.setAutoCreateColumnsFromModel(false);\n        // Setzt den Renderer, der dafür sorgt, dass Checkboxen, die nicht\n        // bearbeitet werden können, disabled werden\n        treeTable.getColumn(LastschriftTreeTableModel.CHECK_COLUMN_INDEX)\n                .setCellRenderer(new DisabledCellRenderer());\n        // Reagiert auf die Auswahl einer Zeile durch den Benutzer\n        treeTable.setSelectionMode(ListSelectionModel.SINGLE_SELECTION);\n        treeTable.addTreeSelectionListener(new SelectMandatListener());\n        scrollPane.setViewportView(treeTable);\n\n        Highlighter high;\n        // Färbt den Hintergrund aller Rechnungen ein\n        high = new ColorHighlighter(new RechnungHighlightPredicate(),\n                Colors.TT_CHILD_BG, null);\n        treeTable.addHighlighter(high);\n        // Färbt den Hintergrund der Rechnungen, die zum aktuell ausgewählten\n        // Mandat gehören, ein\n        high = new ColorHighlighter(new RechnungSelectedPredicate(),\n                Colors.TT_SEL_BG, null);\n        treeTable.addHighlighter(high);\n        // Färbt den Text der abgewählten Mandate und Rechnungen ein\n        high = new ColorHighlighter(new RowDeactivatedPredicate(), null,\n                Colors.TT_DEACTIV_FG, null, Colors.TT_DEACTIV_FG);\n        treeTable.addHighlighter(high);\n\n        /*** Buttons unter der Tabelle ***/\n        JPanel btnsBelow = new JPanel();\n        btnsBelow.setBorder(new TitledBorder(null, \"Alle Mandate\",\n                TitledBorder.LEADING, TitledBorder.TOP, null, null));\n        contentPane.add(btnsBelow, \"cell 0 2,growx\");\n        btnsBelow.setLayout(new MigLayout(\"\", \"[][grow]\", \"[]\"));\n\n        JButton btnAlleAusklappen = new JButton(\"Ausklappen\");\n        btnsBelow.add(btnAlleAusklappen, \"cell 0 0,alignx left\");\n        btnAlleAusklappen.addActionListener(new AlleAusklappenListener());\n\n        JButton btnAlleEinklappen = new JButton(\"Einklappen\");\n        btnsBelow.add(btnAlleEinklappen, \"cell 0 0,alignx left\");\n        btnAlleEinklappen.addActionListener(new AlleEinklappenListener());\n\n        JButton btnAuswaehlen = new JButton(\"Auswählen\");\n        btnsBelow.add(btnAuswaehlen, \"cell 1 0,span,alignx right\");\n        btnAuswaehlen.addActionListener(new MandateSelectListener(true));\n\n        JButton btnAbwaehlen = new JButton(\"Abwählen\");\n        btnsBelow.add(btnAbwaehlen, \"cell 1 0,span,alignx right\");\n        btnAbwaehlen.addActionListener(new MandateSelectListener(false));\n\n        /*** Lastschrift-Erstellungs-Einstellungen und -Button ***/\n        contentPane.add(createErstellenPanel(), \"cell 0 3,growx\");\n\n        pack();\n    }\n\n    private JPanel createSearchPanel() {\n        JPanel searchPanel = new JPanel();\n        searchPanel.setBorder(new TitledBorder(null, \"Rechnungen suchen\",\n                TitledBorder.LEADING, TitledBorder.TOP, null, null));\n        searchPanel.setLayout(new MigLayout(\"\", \"[][grow]\", \"[][grow][]\"));\n\n        JLabel lblEsWerdenNur = new JLabel(\n                \"<html>Es werden nur Rechnungen von Mitgliedern angezeigt, für die ein \"\n                        + \"gültiges, aktives SEPA-Mandat vorhanden ist.\");\n        searchPanel.add(lblEsWerdenNur, \"cell 0 0 2 1,wmax 100%\");\n\n        /*** Rechnungsdatum ***/\n        chckbxRechnungsdatum = new JCheckBox(\"Rechnungsdatum:\");\n        chckbxRechnungsdatum\n                .addItemListener(new RechnungsdatumCheckboxListener());\n        searchPanel.add(chckbxRechnungsdatum, \"flowx,cell 0 1\");\n\n        inputRechnungsdatum = new JDateChooser();\n        inputRechnungsdatum.setEnabled(false);\n        searchPanel.add(inputRechnungsdatum, \"cell 1 1,growy,w 100::\");\n\n        /*** Untere Zeile ***/\n        JPanel sucheBottomPanel = new JPanel();\n        searchPanel.add(sucheBottomPanel, \"cell 0 2 2 1,growx\");\n        sucheBottomPanel.setLayout(new MigLayout(\"insets 0\", \"[][grow]\", \"[]\"));\n\n        chckbxBereitsErstellt = new JCheckBox(\n                \"Rechnungen, für die bereits eine Lastschrift erstellt wurde\");\n        sucheBottomPanel.add(chckbxBereitsErstellt,\n                \"cell 0 0,alignx left,growy\");\n\n        JButton btnSuchen = new JButton(\"Suchen\");\n        btnSuchen.addActionListener(new MandateSucheListener());\n        sucheBottomPanel.add(btnSuchen, \"cell 1 0,alignx right,aligny top\");\n\n        return searchPanel;\n    }\n\n    /**\n     * Aktiviert das Eingabefeld für das Rechnungsdatum nur, wenn auch die\n     * Checkbox aktiviert ist.\n     */\n    private class RechnungsdatumCheckboxListener implements ItemListener {\n        @Override\n        public void itemStateChanged(ItemEvent e) {\n            inputRechnungsdatum.setEnabled(chckbxRechnungsdatum.isSelected());\n        }\n    }\n\n    private JPanel createErstellenPanel() {\n        JPanel erstellenPanel = new JPanel();\n        erstellenPanel.setBorder(new TitledBorder(null,\n                \"Sammellastschrift erstellen\", TitledBorder.LEADING,\n                TitledBorder.TOP, null, null));\n        erstellenPanel.setLayout(new MigLayout(\"\", \"[][grow]\", \"[][][][]\"));\n\n        // Fälligkeit\n        JLabel lblFaelligkeit = new JLabel(\"Fälligkeit:\");\n        erstellenPanel.add(lblFaelligkeit, \"cell 0 0\");\n\n        inputFaelligkeit = new JDateChooser();\n        inputFaelligkeit.setDate(new Date());\n        erstellenPanel.add(inputFaelligkeit, \"cell 1 0,alignx left,w 100::\");\n\n        // Bezeichnung\n        JLabel lblBezeichnung = new JLabel(\"Bezeichnung:\");\n        erstellenPanel.add(lblBezeichnung, \"cell 0 1,alignx left\");\n\n        inputBezeichnung = new JTextField();\n        erstellenPanel.add(inputBezeichnung, \"cell 1 1,growx\");\n        inputBezeichnung.setColumns(10);\n\n        // Verwendungszweck\n        JLabel lblVerwendungszweck = new JLabel(\"Verwendungszweck:\");\n        erstellenPanel.add(lblVerwendungszweck, \"cell 0 2\");\n\n        rdbtnMitgliedernamen = new JRadioButton(\"Mitgliedernamen\");\n        rdbtnMitgliedernamen.setSelected(true);\n        erstellenPanel.add(rdbtnMitgliedernamen, \"flowx,cell 1 2\");\n\n        rdbtnRechnungsnummer = new JRadioButton(\"Rechnungsnummer\");\n        erstellenPanel.add(rdbtnRechnungsnummer, \"cell 1 2\");\n\n        ButtonGroup verwendungszweckGrp = new ButtonGroup();\n        verwendungszweckGrp.add(rdbtnMitgliedernamen);\n        verwendungszweckGrp.add(rdbtnRechnungsnummer);\n\n        // Button\n        JButton btnLastschriftErzeugen = new JButton(\n                \"Sammellastschrift erstellen\");\n        btnLastschriftErzeugen\n                .addActionListener(new LastschriftErzeugenListener());\n        erstellenPanel.add(btnLastschriftErzeugen,\n                \"cell 0 3 2 1,alignx right,aligny top\");\n\n        return erstellenPanel;\n    }\n\n    /**\n     * Listet alle Mandate und mögliche Rechnungen in der Tabelle auf, die die\n     * eingegeben Kriterien erfüllen.\n     */\n    private class MandateSucheListener implements ActionListener {\n        @Override\n        public void actionPerformed(ActionEvent e) {\n            FilterSettings filterSettings = new FilterSettings();\n\n            if (chckbxRechnungsdatum.isSelected()) {\n                filterSettings.setRechnungsdatum(inputRechnungsdatum.getDate());\n            }\n\n            filterSettings.setBereitsErstellt(chckbxBereitsErstellt\n                    .isSelected());\n\n            treeTableModel.reloadMandate(filterSettings);\n        }\n    }\n\n    /**\n     * Zeigt die Buchungen für alle Personen in der Tabelle an.\n     */\n    private class AlleAusklappenListener implements ActionListener {\n        @Override\n        public void actionPerformed(ActionEvent e) {\n            treeTable.expandAll();\n        }\n    }\n\n    /**\n     * Blendet die Buchungen für alle Personen in der Tabelle aus.\n     */\n    private class AlleEinklappenListener implements ActionListener {\n        @Override\n        public void actionPerformed(ActionEvent e) {\n            treeTable.collapseAll();\n        }\n    }\n\n    /**\n     * (De-)Selektiert alle Personen in der Tabelle.\n     */\n    private final class MandateSelectListener implements ActionListener {\n        private final boolean desiredState;\n\n        private MandateSelectListener(boolean desiredState) {\n            this.desiredState = desiredState;\n        }\n\n        @Override\n        public void actionPerformed(ActionEvent e) {\n            if (treeTableModel.root == null) {\n                // es stehen keine Personen in der Tabelle\n                return;\n            }\n\n            for (MandatNode mNode : treeTableModel.root.mandate) {\n                mNode.checked = desiredState;\n            }\n\n            treeTable.repaint();\n        }\n    }\n\n    /**\n     * Erzeugt eine Sammellastschrift für die ausgewählten Personen und\n     * Buchungen.\n     */\n    private class LastschriftErzeugenListener implements ActionListener {\n        @Override\n        public void actionPerformed(ActionEvent e) {\n\n            if (treeTableModel.root == null) {\n                // es stehen keine Mandate in der Tabelle\n                return;\n            }\n\n            // Gibt an, ob schon eine (Einzel-)Lastschrift in die Datenbank\n            // eingefügt wurde (andernfalls wird am Ende der Funktion die\n            // Transaktion rückgängig gemacht, also auch keine\n            // Sammellastschrift in der Datenbank gespeichert)\n            boolean lastschriftEingefuegt = false;\n\n            try (SqlSession session = sqlSessionFactory.openSession()) {\n                LastschriftenMapper mapper = session\n                        .getMapper(LastschriftenMapper.class);\n\n                // Sammellastschrift einfügen", "context": "src/nami/beitrag/db/BeitragMitglied.java\n@Data\n@NoArgsConstructor\npublic class BeitragMitglied {\n    private int mitgliedId;\n    private int mitgliedsnummer;\n    private String nachname;\n    private String vorname;\n    private MitgliedStatus status;\n    private Mitgliedstyp mitgliedstyp;\n    private Beitragsart beitragsart;\n    private Date eintrittsdatum;\n    private String strasse;\n    private String plz;\n    private String ort;\n    private String email;\n    private String emailVertretungsber;\n    private int version;\n    private Integer aktivesMandat;\n    private boolean deleted;\n\n    /**\n     * Aktualisiert die Felder mit einem übergebenen Mitglieds-Datensatz aus\n     * NaMi. Die Mitglieds-ID wird dabei nicht aktualisiert.\n     * \n     * @param namiMgl\n     *            der Mitglieds-Datensatz aus NaMi.\n     */\n    public void updateFromNami(NamiMitglied namiMgl) {\n        if (mitgliedId != namiMgl.getId()) {\n            throw new IllegalArgumentException(\"MitgliedIds are not equal\");\n        }\n        mitgliedsnummer = namiMgl.getMitgliedsnummer();\n        nachname = namiMgl.getNachname();\n        vorname = namiMgl.getVorname();\n        status = namiMgl.getStatus();\n        mitgliedstyp = namiMgl.getMitgliedstyp();\n        beitragsart = namiMgl.getBeitragsart();\n        eintrittsdatum = namiMgl.getEintrittsdatum();\n        strasse = namiMgl.getStrasse();\n        plz = namiMgl.getPlz();\n        ort = namiMgl.getOrt();\n        email = namiMgl.getEmail();\n        emailVertretungsber = Objects.requireNonNullElse(namiMgl.getEmailVertretungsberechtigter(), \"\");\n        version = namiMgl.getVersion();\n    }\n\n    /**\n     * Erzeugt ein neues Mitglied aus einem übergebenen Mitglieds-Datensatz aus\n     * NaMi. Die Felder, die in NaMi nicht vorhanden sind, werden mit ihren\n     * Standardwerten initialisiert.\n     * \n     * @param namiMgl\n     *            der Mitglieds-Datensatz aus NaMi\n     */\n    public BeitragMitglied(NamiMitglied namiMgl) {\n        mitgliedId = namiMgl.getId();\n        updateFromNami(namiMgl);\n        deleted = false;\n    }\n}\nsrc/nami/beitrag/gui/utils/Colors.java\npublic final class Colors {\n\n    // Farben für die Hervorhebung von Zeilen in einer TreeTable\n    private static final UIDefaults UIDEFAULTS = javax.swing.UIManager\n            .getDefaults();\n    /**\n     * Farbe, die für den Hintergrund der Kindeinträge in einer TreeTable der\n     * Tiefe 2 verwendet wird.\n     */\n    public static final Color TT_CHILD_BG = UIDEFAULTS\n            .getColor(\"Label.background\");\n\n    /**\n     * Farbe die zum Kennzeichnen von selektierten Einträgen in einer Tabelle\n     * verwendet wird.\n     */\n    public static final Color TT_SEL_BG = UIDEFAULTS\n            .getColor(\"List.selectionBackground\");\n\n    /**\n     * Schriftfarbe zum Kennzeichnen von deaktivierten Einträgen in Tabellen.\n     */\n    public static final Color TT_DEACTIV_FG = UIDEFAULTS\n            .getColor(\"Label.disabledForeground\");\n\n    private Colors() {\n    }\n}\nsrc/nami/beitrag/db/BeitragSepaMandat.java\n@Data\npublic class BeitragSepaMandat {\n    private int mandatId;\n    private String iban;\n    private String bic;\n    private Date datum;\n    private String kontoinhaber;\n    private String strasse;\n    private String plz;\n    private String ort;\n    private String email;\n    private boolean gueltig;\n}\nsrc/nami/beitrag/gui/utils/MyStringUtils.java\npublic final class MyStringUtils {\n\n    private static final String[] REPLACE_UML_FROM = { \"ä\", \"ö\", \"ü\", \"Ä\", \"Ö\",\n            \"Ü\", \"ß\", \"ë\" };\n    private static final String[] REPLACE_UML_TO = { \"ae\", \"oe\", \"ue\", \"Ae\",\n            \"Oe\", \"Ue\", \"ss\", \"e\" };\n\n    private MyStringUtils() {\n    }\n\n    /**\n     * Ersetzt alle Umlaute im übergebenen String durch ihre Umschreibung.\n     * \n     * @param text\n     *            der zu bearbeitende String\n     * @return String mit ersetzen Umlauten\n     */\n    public static String replaceUmlauts(String text) {\n        return StringUtils.replaceEach(text, REPLACE_UML_FROM, REPLACE_UML_TO);\n    }\n\n    public static int parseIntDefaultMinusOne(String value) {\n        try {\n            return Integer.parseInt(value);\n        } catch (NumberFormatException e) {\n            return -1;\n        }\n    }\n}\nsrc/nami/beitrag/db/BeitragRechnung.java\n@Data\npublic class BeitragRechnung {\n    private int rechnungId;\n    private int mitgliedId;\n    private int rechnungsNummer;\n    private Date datum;\n    private Date frist;\n    private Rechnungsstatus status;\n\n    /**\n     * Rechnungsbetrag (d. h. Summe der enthaltenen Posten).\n     * \n     * Dieses Feld steht nicht in der Datenbank, sondern wird in der SQL-Abfrage\n     * aus den verknüpften Posten berechnet.\n     */\n    @Setter(AccessLevel.NONE)\n    private BigDecimal betrag;\n\n    /**\n     * Liefert die vollständige Rechnungsnummer.\n     * \n     * @return Rechnungsnummer inklusive Jahreszahl\n     */\n    public String getCompleteRechnungsNummer() {\n        String jahreszahl = new SimpleDateFormat(\"y\").format(datum);\n        return jahreszahl + \"/\" + rechnungsNummer;\n    }\n}\nsrc/nami/beitrag/gui/utils/DisabledCellRenderer.java\npublic class DisabledCellRenderer extends DefaultTableCellRenderer {\n    private static final long serialVersionUID = -4317607684491411756L;\n\n    @Override\n    public Component getTableCellRendererComponent(JTable table,\n            Object value, boolean isSelected, boolean hasFocus, int row,\n            int column) {\n\n        TableCellRenderer renderer = table.getDefaultRenderer(table\n                .getColumnClass(column));\n        Component c = renderer.getTableCellRendererComponent(table, value,\n                isSelected, hasFocus, row, column);\n\n        // Deaktiviert die Komponente, falls das Feld nicht bearbeitbar ist\n        if (!table.isCellEditable(row, column)) {\n            c.setEnabled(false);\n        }\n\n        return c;\n    }\n}\nsrc/nami/beitrag/db/BeitragSammelLastschrift.java\n@Data\npublic class BeitragSammelLastschrift {\n    private int sammelLastschriftId;\n    private Date faelligkeit;\n    private boolean ausgefuehrt;\n    private String bezeichnung;\n\n    // Die folgenden Felder sind nicht direkt in der Datenbank gespeichert\n    @Setter(AccessLevel.NONE)\n    private int anzahlLastschriften;\n    @Setter(AccessLevel.NONE)\n    private BigDecimal betrag;\n    @Setter(AccessLevel.NONE)\n    private boolean alleGueltig;\n}\nsrc/nami/beitrag/db/BeitragLastschrift.java\n@Data\npublic class BeitragLastschrift {\n    private int lastschriftId;\n    private int sammelLastschriftId;\n    private int mandatId;\n    private String verwendungszweck;\n\n    // Die folgenden Felder sind nicht direkt in der Datenbank gespeichert,\n    // sondern werden bei der Abfrage berechnet.\n    @Setter(AccessLevel.NONE)\n    private BigDecimal betrag;\n}\nsrc/nami/beitrag/db/LastschriftenMapper.java\n@Data\npublic static class FilterSettings {\n    /**\n     * Dieser Parameter filtert die Rechnungen nach dem Erstellungsdatum.\n     * Wenn dieser Parameter <tt>null</tt> ist, wird nicht nach dem\n     * Rechnungsdatum gefiltert.\n     */\n    private Date rechnungsdatum = null;\n\n    /**\n     * Gibt an, ob auch Rechnungen selektiert werden, für die bereits eine\n     * Lastschrift erstellt wurde.\n     */\n    private boolean bereitsErstellt;\n}\nsrc/nami/beitrag/db/LastschriftenMapper.java\n@Getter\npublic static class DataMandateRechnungen {\n    private BeitragSepaMandat mandat;\n    private List<DataRechnungMitglied> rechnungen;\n}\nsrc/nami/beitrag/db/LastschriftenMapper.java\npublic interface LastschriftenMapper {\n\n    /**\n     * Beschreibt Filterkriterien für die Suche nach Mandaten und Buchungen.\n     */\n    @Data\n    public static class FilterSettings {\n        /**\n         * Dieser Parameter filtert die Rechnungen nach dem Erstellungsdatum.\n         * Wenn dieser Parameter <tt>null</tt> ist, wird nicht nach dem\n         * Rechnungsdatum gefiltert.\n         */\n        private Date rechnungsdatum = null;\n\n        /**\n         * Gibt an, ob auch Rechnungen selektiert werden, für die bereits eine\n         * Lastschrift erstellt wurde.\n         */\n        private boolean bereitsErstellt;\n    }\n\n    /**\n     * Ergebnis-Datentyp bei der Suche nach Mandaten und Rechnungen, die vom\n     * jeweiligen Mandat eingezogen werden können.\n     */\n    @Getter\n    public static class DataMandateRechnungen {\n        private BeitragSepaMandat mandat;\n        private List<DataRechnungMitglied> rechnungen;\n    }\n\n    /**\n     * Fasst ein Mitglied und eine Rechnung zusammen.\n     */\n    @Getter\n    public static class DataRechnungMitglied {\n        private BeitragMitglied mitglied;\n        private BeitragRechnung rechnung;\n    }\n\n    /**\n     * Liefert alle offenen Rechnungen und die zugehörigen aktiven, gültigen\n     * Mandate, die die übergebenen Filterkriterien erfüllen.\n     * \n     * @param filterSettings\n     *            Kriterien nach denen gefiltert wird\n     * @return Mandate und jeweils offene Rechnungen\n     */\n    Collection<DataMandateRechnungen> mandateOffeneRechnungen(\n            @Param(\"filterSettings\") FilterSettings filterSettings);\n\n    /**\n     * Fügt eine Lastschrift in die Datenbank ein. Die <tt>lastschriftId</tt>\n     * der neu eingefügten Lastschrift ist anschließend im Objekt gespeichert,\n     * das als Parameter übergeben wurde.\n     * \n     * @param lastschrift\n     *            Daten der einzufügenden Lastschrift\n     */\n    void insertLastschrift(BeitragLastschrift lastschrift);\n\n    /**\n     * Löscht eine Lastschrift aus der Datenbank. Die enthaltenen Rechnungen\n     * sollten vorher schon gelöscht worden seien (siehe\n     * {@link #deleteAllRechnungenFromLastschrift(int)}).\n     * \n     * @param lastschriftId\n     *            ID der Lastschrift, die gelöscht werden soll\n     */\n    void deleteLastschrift(int lastschriftId);\n\n    /**\n     * Fügt eine Rechnung zu einer Lastschrift hinzu, d. h. diese Rechnung wird\n     * mit der jeweiligen Lastschrift eingezogen.\n     * \n     * @param lastschriftId\n     *            ID der Lastschrift\n     * @param rechnungId\n     *            ID der Rechnung\n     */\n    void addRechnungToLastschrift(@Param(\"lastschriftId\") int lastschriftId,\n            @Param(\"rechnungId\") int rechnungId);\n\n    /**\n     * Löscht alle Rechnungen aus einer Lastschrift. Die Rechnungen werden dabei\n     * in der Datenbank belassen, nur ihre Verbindungen zur Lastschrift werden\n     * entfernt.\n     * \n     * @param lastschriftId\n     *            ID der Lastschrift\n     */\n    void deleteAllRechnungenFromLastschrift(int lastschriftId);\n\n    /**\n     * Fügt eine Sammellastschrift in die Datenbank ein. Die\n     * <tt>sammelLastschriftId</tt> der neu eingefügten Lastschrift ist\n     * anschließend im Objekt gespeichert, das als Parameter übergeben wurde.\n     * \n     * @param sammelLastschrift\n     *            Daten der einzufügenden Sammellastschrift\n     */\n    void insertSammelLastschrift(BeitragSammelLastschrift sammelLastschrift);\n\n    /**\n     * Aktualisiert den Datensatz einer Sammellastschrift.\n     * \n     * @param sammelLastschrift\n     *            Objekt, dessen geänderte Felder in die Datenbank gespeichert\n     *            werden sollen\n     */\n    void updateSammelLastschrift(BeitragSammelLastschrift sammelLastschrift);\n\n    /**\n     * Löscht eine Sammellastschrift aus der Datenbank. Die enthaltenen\n     * Lastschriften sollten vorher schon gelöscht worden seien.\n     * \n     * @param sammelLastschriftId\n     *            ID der Sammellastschrift\n     */\n    void deleteSammelLastschrift(int sammelLastschriftId);\n\n    /**\n     * Fragt alle Sammellastschriften (inkl. der Anzahl der enthaltenen\n     * Einzellastschriften und des Gesamtbetrages) aus der Datenbank ab, die dem\n     * übergebenen Filterkriterium entsprechen.\n     * \n     * @param ausgefuehrt\n     *            Gibt an, ob ausgeführte Sammellastschriften angezeigt werden\n     *            sollen.\n     *            <ul>\n     *            <li><tt>True</tt>: nur ausgeführte werden angezeigt</li>\n     *            <li><tt>False</tt>: nur <em>nicht</em> ausgeführte werden\n     *            angezeigt</li>\n     *            <li><tt>null</tt>: Filterkriterium wird ignoriert</li>\n     *            </ul>\n     * @return Sammellastschriften, die dem angegebenen Filterkriterium\n     *         entsprechen\n     */\n    ArrayList<BeitragSammelLastschrift> findSammelLastschriften(\n            @Param(\"ausgefuehrt\") Boolean ausgefuehrt);\n\n    /**\n     * Liefert alle Lastschriften (inkl. der zugehörigen Mandate), die in einer\n     * Sammellastschrift enthalten sind.\n     * \n     * @param sammelLastschriftId\n     *            ID der Sammellastschrift\n     * @return enthaltenen (Einzel-)Lastschriften\n     */\n    ArrayList<DataLastschriftMandat> getLastschriften(int sammelLastschriftId);\n\n    /**\n     * Liefert alle Rechnungen, die in einer Sammellastschrift enthalten sind.\n     * Das heißt es werden die zugeordneten Rechnungen aller enthaltenen\n     * Einzellastschriften vereinigt.\n     * \n     * @param sammelLastschriftId\n     *            ID der Sammellastschrift\n     * @return zugeordnete Rechnungen\n     */\n    List<BeitragRechnung> getRechnungenInSammelLastschrift(\n            int sammelLastschriftId);\n\n    /**\n     * Fügt eine Prenotification in die Datenbank ein. Die\n     * <tt>prenotificationId</tt> der neu eingefügten Prenotification ist\n     * anschließend im Objekt gespeichert, das als Parameter übergeben wurde.\n     * \n     * @param pre\n     *            Daten der einzufügenden Prenotification\n     */\n    void insertPrenotification(BeitragPrenotification pre);\n\n    /**\n     * Überprüft, ob die aktuellste (nach Ausstellungsdatum), als regelmäßig\n     * gekennzeichnete Prenotification im Betrag mit den Parametern\n     * übereinstimmt. In diesem Fall muss also keine neue erstellt werden.\n     * \n     * @param mandatId\n     *            ID des Mandats\n     * @param betrag\n     *            benötigter Lastschriftbetrag\n     * @return <tt>true</tt>, falls die aktuellste Prenotification den passenden\n     *         Betrag enthält\n     */\n    boolean existsValidPrenotification(@Param(\"mandatId\") int mandatId,\n            @Param(\"betrag\") BigDecimal betrag);\n\n    /**\n     * Fasst eine Prenotification und das dazugehörige Mandat zusammen.\n     */\n    @Getter\n    public class DataPrenotificationMandat {\n        private BeitragPrenotification prenotification;\n        private BeitragSepaMandat mandat;\n    }\n\n    /**\n     * Liefert eine Prenotification und das dazugehörige Mandat.\n     * \n     * @param prenotificationId\n     *            ID der Prenotification\n     * @return Prenotification mit Mandat\n     */\n    DataPrenotificationMandat getPrenotificationMitMandat(int prenotificationId);\n}\nsrc/nami/beitrag/db/LastschriftenMapper.java\n@Getter\npublic static class DataRechnungMitglied {\n    private BeitragMitglied mitglied;\n    private BeitragRechnung rechnung;\n}\n", "answers": ["                BeitragSammelLastschrift sammelLastschrift;"], "length": 2526, "dataset": "repobench-p_e", "language": "java", "all_classes": null, "_id": "baa8de84ab065678542bc7eba856ad8688646323f9d230bd"}
{"input": "import kademlia.message.Receiver;\nimport java.io.IOException;\nimport kademlia.JKademliaNode;\nimport kademlia.KadConfiguration;\nimport kademlia.KadServer;\nimport kademlia.KademliaNode;\nimport kademlia.exceptions.RoutingException;\nimport kademlia.message.AcknowledgeMessage;\nimport kademlia.message.ConnectMessage;\nimport kademlia.message.Message;\nimport kademlia.node.Node;\n/**\n * @author Joshua Kissoon\n * @created 20140218\n * @desc Operation that handles connecting to an existing Kademlia network using a bootstrap node\n */\npackage kademlia.operation;\n\n\npublic class ConnectOperation implements Operation, Receiver\n{\n\n    public static final int MAX_CONNECT_ATTEMPTS = 5;       // Try 5 times to connect to a node\n\n    private final KadServer server;\n    private final KademliaNode localNode;", "context": "src/kademlia/KadConfiguration.java\npublic interface KadConfiguration\n{\n\n    /**\n     * @return Interval in milliseconds between execution of RestoreOperations.\n     */\n    public long restoreInterval();\n\n    /**\n     * If no reply received from a node in this period (in milliseconds)\n     * consider the node unresponsive.\n     *\n     * @return The time it takes to consider a node unresponsive\n     */\n    public long responseTimeout();\n\n    /**\n     * @return Maximum number of milliseconds for performing an operation.\n     */\n    public long operationTimeout();\n\n    /**\n     * @return Maximum number of concurrent messages in transit.\n     */\n    public int maxConcurrentMessagesTransiting();\n\n    /**\n     * @return K-Value used throughout Kademlia\n     */\n    public int k();\n\n    /**\n     * @return Size of replacement cache.\n     */\n    public int replacementCacheSize();\n\n    /**\n     * @return # of times a node can be marked as stale before it is actually removed.\n     */\n    public int stale();\n\n    /**\n     * Creates the folder in which this node data is to be stored.\n     *\n     * @param ownerId\n     *\n     * @return The folder path\n     */\n    public String getNodeDataFolder(String ownerId);\n\n    /**\n     * @return Whether we're in a testing or production system.\n     */\n    public boolean isTesting();\n}\nsrc/kademlia/JKademliaNode.java\npublic class JKademliaNode implements KademliaNode\n{\n\n    /* Kademlia Attributes */\n    private final String ownerId;\n\n    /* Objects to be used */\n    private final transient Node localNode;\n    private final transient KadServer server;\n    private final transient KademliaDHT dht;\n    private transient KademliaRoutingTable routingTable;\n    private final int udpPort;\n    private transient KadConfiguration config;\n\n    /* Timer used to execute refresh operations */\n    private transient Timer refreshOperationTimer;\n    private transient TimerTask refreshOperationTTask;\n\n    /* Factories */\n    private final transient MessageFactory messageFactory;\n\n    /* Statistics */\n    private final transient KadStatistician statistician;\n\n    \n    {\n        statistician = new Statistician();\n    }\n\n    /**\n     * Creates a Kademlia DistributedMap using the specified name as filename base.\n     * If the id cannot be read from disk the specified defaultId is used.\n     * The instance is bootstraped to an existing network by specifying the\n     * address of a bootstrap node in the network.\n     *\n     * @param ownerId      The Name of this node used for storage\n     * @param localNode    The Local Node for this Kad instance\n     * @param udpPort      The UDP port to use for routing messages\n     * @param dht          The DHT for this instance\n     * @param config\n     * @param routingTable\n     *\n     * @throws IOException If an error occurred while reading id or local map\n     *                     from disk <i>or</i> a network error occurred while\n     *                     attempting to bootstrap to the network\n     * */\n    public JKademliaNode(String ownerId, Node localNode, int udpPort, KademliaDHT dht, KademliaRoutingTable routingTable, KadConfiguration config) throws IOException\n    {\n        this.ownerId = ownerId;\n        this.udpPort = udpPort;\n        this.localNode = localNode;\n        this.dht = dht;\n        this.config = config;\n        this.routingTable = routingTable;\n        this.messageFactory = new MessageFactory(this, this.dht, this.config);\n        this.server = new KadServer(udpPort, this.messageFactory, this.localNode, this.config, this.statistician);\n        this.startRefreshOperation();\n    }\n\n    @Override\n    public final void startRefreshOperation()\n    {\n        this.refreshOperationTimer = new Timer(true);\n        refreshOperationTTask = new TimerTask()\n        {\n            @Override\n            public void run()\n            {\n                try\n                {\n                    /* Runs a DHT RefreshOperation  */\n                    JKademliaNode.this.refresh();\n                }\n                catch (IOException e)\n                {\n                    System.err.println(\"KademliaNode: Refresh Operation Failed; Message: \" + e.getMessage());\n                }\n            }\n        };\n        refreshOperationTimer.schedule(refreshOperationTTask, this.config.restoreInterval(), this.config.restoreInterval());\n    }\n\n    @Override\n    public final void stopRefreshOperation()\n    {\n        /* Close off the timer tasks */\n        this.refreshOperationTTask.cancel();\n        this.refreshOperationTimer.cancel();\n        this.refreshOperationTimer.purge();\n    }\n\n    public JKademliaNode(String ownerId, Node node, int udpPort, KademliaRoutingTable routingTable, KadConfiguration config) throws IOException\n    {\n        this(\n                ownerId,\n                node,\n                udpPort,\n                new DHT(ownerId, config),\n                routingTable,\n                config\n        );\n    }\n\n    public JKademliaNode(String ownerId, Node node, int udpPort, KadConfiguration config) throws IOException\n    {\n        this(\n                ownerId,\n                node,\n                udpPort,\n                new JKademliaRoutingTable(node, config),\n                config\n        );\n    }\n\n    public JKademliaNode(String ownerId, KademliaId defaultId, int udpPort) throws IOException\n    {\n        this(\n                ownerId,\n                new Node(defaultId, InetAddress.getLocalHost(), udpPort),\n                udpPort,\n                new DefaultConfiguration()\n        );\n    }\n\n    /**\n     * Load Stored state using default configuration\n     *\n     * @param ownerId The ID of the owner for the stored state\n     *\n     * @return A Kademlia instance loaded from a stored state in a file\n     *\n     * @throws java.io.FileNotFoundException\n     * @throws java.lang.ClassNotFoundException\n     */\n    public static JKademliaNode loadFromFile(String ownerId) throws FileNotFoundException, IOException, ClassNotFoundException\n    {\n        return JKademliaNode.loadFromFile(ownerId, new DefaultConfiguration());\n    }\n\n    /**\n     * Load Stored state\n     *\n     * @param ownerId The ID of the owner for the stored state\n     * @param iconfig Configuration information to work with\n     *\n     * @return A Kademlia instance loaded from a stored state in a file\n     *\n     * @throws java.io.FileNotFoundException\n     * @throws java.lang.ClassNotFoundException\n     */\n    public static JKademliaNode loadFromFile(String ownerId, KadConfiguration iconfig) throws FileNotFoundException, IOException, ClassNotFoundException\n    {\n        DataInputStream din;\n\n        /**\n         * @section Read Basic Kad data\n         */\n        din = new DataInputStream(new FileInputStream(getStateStorageFolderName(ownerId, iconfig) + File.separator + \"kad.kns\"));\n        JKademliaNode ikad = new JsonSerializer<JKademliaNode>().read(din);\n\n        /**\n         * @section Read the routing table\n         */\n        din = new DataInputStream(new FileInputStream(getStateStorageFolderName(ownerId, iconfig) + File.separator + \"routingtable.kns\"));\n        KademliaRoutingTable irtbl = new JsonRoutingTableSerializer(iconfig).read(din);\n\n        /**\n         * @section Read the node state\n         */\n        din = new DataInputStream(new FileInputStream(getStateStorageFolderName(ownerId, iconfig) + File.separator + \"node.kns\"));\n        Node inode = new JsonSerializer<Node>().read(din);\n\n        /**\n         * @section Read the DHT\n         */\n        din = new DataInputStream(new FileInputStream(getStateStorageFolderName(ownerId, iconfig) + File.separator + \"dht.kns\"));\n        KademliaDHT idht = new JsonDHTSerializer().read(din);\n        idht.setConfiguration(iconfig);\n\n        return new JKademliaNode(ownerId, inode, ikad.getPort(), idht, irtbl, iconfig);\n    }\n\n    @Override\n    public Node getNode()\n    {\n        return this.localNode;\n    }\n\n    @Override\n    public KadServer getServer()\n    {\n        return this.server;\n    }\n\n    @Override\n    public KademliaDHT getDHT()\n    {\n        return this.dht;\n    }\n\n    @Override\n    public KadConfiguration getCurrentConfiguration()\n    {\n        return this.config;\n    }\n\n    @Override\n    public synchronized final void bootstrap(Node n) throws IOException, RoutingException\n    {\n        long startTime = System.nanoTime();\n        Operation op = new ConnectOperation(this.server, this, n, this.config);\n        op.execute();\n        long endTime = System.nanoTime();\n        this.statistician.setBootstrapTime(endTime - startTime);\n    }\n\n    @Override\n    public int put(KadContent content) throws IOException\n    {\n        return this.put(new JKademliaStorageEntry(content));\n    }\n\n    @Override\n    public int put(JKademliaStorageEntry entry) throws IOException\n    {\n        StoreOperation sop = new StoreOperation(this.server, this, entry, this.dht, this.config);\n        sop.execute();\n\n        /* Return how many nodes the content was stored on */\n        return sop.numNodesStoredAt();\n    }\n\n    @Override\n    public void putLocally(KadContent content) throws IOException\n    {\n        this.dht.store(new JKademliaStorageEntry(content));\n    }\n\n    @Override\n    public JKademliaStorageEntry get(GetParameter param) throws NoSuchElementException, IOException, ContentNotFoundException\n    {\n        if (this.dht.contains(param))\n        {\n            /* If the content exist in our own DHT, then return it. */\n            return this.dht.get(param);\n        }\n\n        /* Seems like it doesn't exist in our DHT, get it from other Nodes */\n        long startTime = System.nanoTime();\n        ContentLookupOperation clo = new ContentLookupOperation(server, this, param, this.config);\n        clo.execute();\n        long endTime = System.nanoTime();\n        this.statistician.addContentLookup(endTime - startTime, clo.routeLength(), clo.isContentFound());\n        return clo.getContentFound();\n    }\n\n    @Override\n    public void refresh() throws IOException\n    {\n        new KadRefreshOperation(this.server, this, this.dht, this.config).execute();\n    }\n\n    @Override\n    public String getOwnerId()\n    {\n        return this.ownerId;\n    }\n\n    @Override\n    public int getPort()\n    {\n        return this.udpPort;\n    }\n\n    @Override\n    public void shutdown(final boolean saveState) throws IOException\n    {\n        /* Shut down the server */\n        this.server.shutdown();\n\n        this.stopRefreshOperation();\n\n        /* Save this Kademlia instance's state if required */\n        if (saveState)\n        {\n            /* Save the system state */\n            this.saveKadState();\n        }\n    }\n\n    @Override\n    public void saveKadState() throws IOException\n    {\n        DataOutputStream dout;\n\n        /**\n         * @section Store Basic Kad data\n         */\n        dout = new DataOutputStream(new FileOutputStream(getStateStorageFolderName(this.ownerId, this.config) + File.separator + \"kad.kns\"));\n        new JsonSerializer<JKademliaNode>().write(this, dout);\n\n        /**\n         * @section Save the node state\n         */\n        dout = new DataOutputStream(new FileOutputStream(getStateStorageFolderName(this.ownerId, this.config) + File.separator + \"node.kns\"));\n        new JsonSerializer<Node>().write(this.localNode, dout);\n\n        /**\n         * @section Save the routing table\n         * We need to save the routing table separate from the node since the routing table will contain the node and the node will contain the routing table\n         * This will cause a serialization recursion, and in turn a Stack Overflow\n         */\n        dout = new DataOutputStream(new FileOutputStream(getStateStorageFolderName(this.ownerId, this.config) + File.separator + \"routingtable.kns\"));\n        new JsonRoutingTableSerializer(this.config).write(this.getRoutingTable(), dout);\n\n        /**\n         * @section Save the DHT\n         */\n        dout = new DataOutputStream(new FileOutputStream(getStateStorageFolderName(this.ownerId, this.config) + File.separator + \"dht.kns\"));\n        new JsonDHTSerializer().write(this.dht, dout);\n\n    }\n\n    /**\n     * Get the name of the folder for which a content should be stored\n     *\n     * @return String The name of the folder to store node states\n     */\n    private static String getStateStorageFolderName(String ownerId, KadConfiguration iconfig)\n    {\n        /* Setup the nodes storage folder if it doesn't exist */\n        String path = iconfig.getNodeDataFolder(ownerId) + File.separator + \"nodeState\";\n        File nodeStateFolder = new File(path);\n        if (!nodeStateFolder.isDirectory())\n        {\n            nodeStateFolder.mkdir();\n        }\n        return nodeStateFolder.toString();\n    }\n\n    @Override\n    public KademliaRoutingTable getRoutingTable()\n    {\n        return this.routingTable;\n    }\n\n    @Override\n    public KadStatistician getStatistician()\n    {\n        return this.statistician;\n    }\n\n    /**\n     * Creates a string containing all data about this Kademlia instance\n     *\n     * @return The string representation of this Kad instance\n     */\n    @Override\n    public String toString()\n    {\n        StringBuilder sb = new StringBuilder(\"\\n\\nPrinting Kad State for instance with owner: \");\n        sb.append(this.ownerId);\n        sb.append(\"\\n\\n\");\n\n        sb.append(\"\\n\");\n        sb.append(\"Local Node\");\n        sb.append(this.localNode);\n        sb.append(\"\\n\");\n\n        sb.append(\"\\n\");\n        sb.append(\"Routing Table: \");\n        sb.append(this.getRoutingTable());\n        sb.append(\"\\n\");\n\n        sb.append(\"\\n\");\n        sb.append(\"DHT: \");\n        sb.append(this.dht);\n        sb.append(\"\\n\");\n\n        sb.append(\"\\n\\n\\n\");\n\n        return sb.toString();\n    }\n}\nsrc/kademlia/message/AcknowledgeMessage.java\npublic class AcknowledgeMessage implements Message\n{\n\n    private Node origin;\n    public static final byte CODE = 0x01;\n\n    public AcknowledgeMessage(Node origin)\n    {\n        this.origin = origin;\n    }\n\n    public AcknowledgeMessage(DataInputStream in) throws IOException\n    {\n        this.fromStream(in);\n    }\n\n    @Override\n    public final void fromStream(DataInputStream in) throws IOException\n    {\n        this.origin = new Node(in);\n    }\n\n    @Override\n    public void toStream(DataOutputStream out) throws IOException\n    {\n        origin.toStream(out);\n    }\n\n    public Node getOrigin()\n    {\n        return this.origin;\n    }\n\n    @Override\n    public byte code()\n    {\n        return CODE;\n    }\n\n    @Override\n    public String toString()\n    {\n        return \"AcknowledgeMessage[origin=\" + origin.getNodeId() + \"]\";\n    }\n}\nsrc/kademlia/message/Receiver.java\npublic interface Receiver\n{\n\n    /**\n     * Message is received, now handle it\n     *\n     * @param conversationId The ID of this conversation, used for further conversations\n     * @param incoming       The incoming\n     *\n     * @throws java.io.IOException\n     */\n    public void receive(Message incoming, int conversationId) throws IOException;\n\n    /**\n     * If no reply is received in <code>MessageServer.TIMEOUT</code> seconds for the\n     * message with communication id <code>comm</code>, the MessageServer calls this method\n     *\n     * @param conversationId The conversation ID of this communication\n     *\n     * @throws IOException if an I/O error occurs\n     * */\n    public void timeout(int conversationId) throws IOException;\n}\nsrc/kademlia/KadServer.java\npublic class KadServer\n{\n\n    /* Maximum size of a Datagram Packet */\n    private static final int DATAGRAM_BUFFER_SIZE = 64 * 1024;      // 64KB\n\n    /* Basic Kad Objects */\n    private final transient KadConfiguration config;\n\n    /* Server Objects */\n    private final DatagramSocket socket;\n    private transient boolean isRunning;\n    private final Map<Integer, Receiver> receivers;\n    private final Timer timer;      // Schedule future tasks\n    private final Map<Integer, TimerTask> tasks;    // Keep track of scheduled tasks\n\n    private final Node localNode;\n\n    /* Factories */\n    private final KademliaMessageFactory messageFactory;\n\n    private final KadStatistician statistician;\n\n    \n    {\n        isRunning = true;\n        this.tasks = new HashMap<>();\n        this.receivers = new HashMap<>();\n        this.timer = new Timer(true);\n    }\n\n    /**\n     * Initialize our KadServer\n     *\n     * @param udpPort      The port to listen on\n     * @param mFactory     Factory used to create messages\n     * @param localNode    Local node on which this server runs on\n     * @param config\n     * @param statistician A statistician to manage the server statistics\n     *\n     * @throws java.net.SocketException\n     */\n    public KadServer(int udpPort, KademliaMessageFactory mFactory, Node localNode, KadConfiguration config, KadStatistician statistician) throws SocketException\n    {\n        this.config = config;\n        this.socket = new DatagramSocket(udpPort);\n        this.localNode = localNode;\n        this.messageFactory = mFactory;\n        this.statistician = statistician;\n\n        /* Start listening for incoming requests in a new thread */\n        this.startListener();\n    }\n\n    /**\n     * Starts the listener to listen for incoming messages\n     */\n    private void startListener()\n    {\n        new Thread()\n        {\n            @Override\n            public void run()\n            {\n                listen();\n            }\n        }.start();\n    }\n\n    /**\n     * Sends a message\n     *\n     * @param msg  The message to send\n     * @param to   The node to send the message to\n     * @param recv The receiver to handle the response message\n     *\n     * @return Integer The communication ID of this message\n     *\n     * @throws IOException\n     * @throws kademlia.exceptions.KadServerDownException\n     */\n    public synchronized int sendMessage(Node to, Message msg, Receiver recv) throws IOException, KadServerDownException\n    {\n        if (!isRunning)\n        {\n            throw new KadServerDownException(this.localNode + \" - Kad Server is not running.\");\n        }\n\n        /* Generate a random communication ID */\n        int comm = new Random().nextInt();\n\n        /* If we have a receiver */\n        if (recv != null)\n        {\n            try\n            {\n                /* Setup the receiver to handle message response */\n                receivers.put(comm, recv);\n                TimerTask task = new TimeoutTask(comm, recv);\n                timer.schedule(task, this.config.responseTimeout());\n                tasks.put(comm, task);\n            }\n            catch (IllegalStateException ex)\n            {\n                /* The timer is already cancelled so we cannot do anything here really */\n            }\n        }\n\n        /* Send the message */\n        sendMessage(to, msg, comm);\n\n        return comm;\n    }\n\n    /**\n     * Method called to reply to a message received\n     *\n     * @param to   The Node to send the reply to\n     * @param msg  The reply message\n     * @param comm The communication ID - the one received\n     *\n     * @throws java.io.IOException\n     */\n    public synchronized void reply(Node to, Message msg, int comm) throws IOException\n    {\n        if (!isRunning)\n        {\n            throw new IllegalStateException(\"Kad Server is not running.\");\n        }\n        sendMessage(to, msg, comm);\n    }\n\n    /**\n     * Internal sendMessage method called by the public sendMessage method after a communicationId is generated\n     */\n    private void sendMessage(Node to, Message msg, int comm) throws IOException\n    {\n        /* Use a try-with resource to auto-close streams after usage */\n        try (ByteArrayOutputStream bout = new ByteArrayOutputStream(); DataOutputStream dout = new DataOutputStream(bout);)\n        {\n            /* Setup the message for transmission */\n            dout.writeInt(comm);\n            dout.writeByte(msg.code());\n            msg.toStream(dout);\n            dout.close();\n\n            byte[] data = bout.toByteArray();\n\n            if (data.length > DATAGRAM_BUFFER_SIZE)\n            {\n                throw new IOException(\"Message is too big\");\n            }\n\n            /* Everything is good, now create the packet and send it */\n            DatagramPacket pkt = new DatagramPacket(data, 0, data.length);\n            pkt.setSocketAddress(to.getSocketAddress());\n            socket.send(pkt);\n\n            /* Lets inform the statistician that we've sent some data */\n            this.statistician.sentData(data.length);\n        }\n    }\n\n    /**\n     * Listen for incoming messages in a separate thread\n     */\n    private void listen()\n    {\n        try\n        {\n            while (isRunning)\n            {\n                try\n                {\n                    /* Wait for a packet */\n                    byte[] buffer = new byte[DATAGRAM_BUFFER_SIZE];\n                    DatagramPacket packet = new DatagramPacket(buffer, buffer.length);\n                    socket.receive(packet);\n\n                    /* Lets inform the statistician that we've received some data */\n                    this.statistician.receivedData(packet.getLength());\n\n                    if (this.config.isTesting())\n                    {\n                        /**\n                         * Simulating network latency\n                         * We pause for 1 millisecond/100 bytes\n                         */\n                        int pause = packet.getLength() / 100;\n                        try\n                        {\n                            Thread.sleep(pause);\n                        }\n                        catch (InterruptedException ex)\n                        {\n\n                        }\n                    }\n\n                    /* We've received a packet, now handle it */\n                    try (ByteArrayInputStream bin = new ByteArrayInputStream(packet.getData(), packet.getOffset(), packet.getLength());\n                            DataInputStream din = new DataInputStream(bin);)\n                    {\n\n                        /* Read in the conversation Id to know which handler to handle this response */\n                        int comm = din.readInt();\n                        byte messCode = din.readByte();\n\n                        Message msg = messageFactory.createMessage(messCode, din);\n                        din.close();\n\n                        /* Get a receiver for this message */\n                        Receiver receiver;\n                        if (this.receivers.containsKey(comm))\n                        {\n                            /* If there is a reciever in the receivers to handle this */\n                            synchronized (this)\n                            {\n                                receiver = this.receivers.remove(comm);\n                                TimerTask task = (TimerTask) tasks.remove(comm);\n                                if (task != null)\n                                {\n                                    task.cancel();\n                                }\n                            }\n                        }\n                        else\n                        {\n                            /* There is currently no receivers, try to get one */\n                            receiver = messageFactory.createReceiver(messCode, this);\n                        }\n\n                        /* Invoke the receiver */\n                        if (receiver != null)\n                        {\n                            receiver.receive(msg, comm);\n                        }\n                    }\n                }\n                catch (IOException e)\n                {\n                    //this.isRunning = false;\n                    System.err.println(\"Server ran into a problem in listener method. Message: \" + e.getMessage());\n                }\n            }\n        }\n        finally\n        {\n            if (!socket.isClosed())\n            {\n                socket.close();\n            }\n            this.isRunning = false;\n        }\n    }\n\n    /**\n     * Remove a conversation receiver\n     *\n     * @param comm The id of this conversation\n     */\n    private synchronized void unregister(int comm)\n    {\n        receivers.remove(comm);\n        this.tasks.remove(comm);\n    }\n\n    /**\n     * Stops listening and shuts down the server\n     */\n    public synchronized void shutdown()\n    {\n        this.isRunning = false;\n        this.socket.close();\n        timer.cancel();\n    }\n\n    /**\n     * Task that gets called by a separate thread if a timeout for a receiver occurs.\n     * When a reply arrives this task must be canceled using the <code>cancel()</code>\n     * method inherited from <code>TimerTask</code>. In this case the caller is\n     * responsible for removing the task from the <code>tasks</code> map.\n     * */\n    class TimeoutTask extends TimerTask\n    {\n\n        private final int comm;\n        private final Receiver recv;\n\n        public TimeoutTask(int comm, Receiver recv)\n        {\n            this.comm = comm;\n            this.recv = recv;\n        }\n\n        @Override\n        public void run()\n        {\n            if (!KadServer.this.isRunning)\n            {\n                return;\n            }\n\n            try\n            {\n                unregister(comm);\n                recv.timeout(comm);\n            }\n            catch (IOException e)\n            {\n                System.err.println(\"Cannot unregister a receiver. Message: \" + e.getMessage());\n            }\n        }\n    }\n\n    public void printReceivers()\n    {\n        for (Integer r : this.receivers.keySet())\n        {\n            System.out.println(\"Receiver for comm: \" + r + \"; Receiver: \" + this.receivers.get(r));\n        }\n    }\n\n    public boolean isRunning()\n    {\n        return this.isRunning;\n    }\n\n}\nsrc/kademlia/KademliaNode.java\npublic interface KademliaNode\n{\n\n    /**\n     * Schedule the recurring refresh operation\n     */\n    public void startRefreshOperation();\n\n    /**\n     * Stop the recurring refresh operation\n     */\n    public void stopRefreshOperation();\n\n    /**\n     * @return Node The local node for this system\n     */\n    public Node getNode();\n\n    /**\n     * @return The KadServer used to send/receive messages\n     */\n    public KadServer getServer();\n\n    /**\n     * @return The DHT for this kad instance\n     */\n    public KademliaDHT getDHT();\n\n    /**\n     * @return The current KadConfiguration object being used\n     */\n    public KadConfiguration getCurrentConfiguration();\n\n    /**\n     * Connect to an existing peer-to-peer network.\n     *\n     * @param n The known node in the peer-to-peer network\n     *\n     * @throws RoutingException      If the bootstrap node could not be contacted\n     * @throws IOException           If a network error occurred\n     * @throws IllegalStateException If this object is closed\n     * */\n    public void bootstrap(Node n) throws IOException, RoutingException;\n\n    /**\n     * Stores the specified value under the given key\n     * This value is stored on K nodes on the network, or all nodes if there are > K total nodes in the network\n     *\n     * @param content The content to put onto the DHT\n     *\n     * @return Integer How many nodes the content was stored on\n     *\n     * @throws java.io.IOException\n     *\n     */\n    public int put(KadContent content) throws IOException;\n\n    /**\n     * Stores the specified value under the given key\n     * This value is stored on K nodes on the network, or all nodes if there are > K total nodes in the network\n     *\n     * @param entry The StorageEntry with the content to put onto the DHT\n     *\n     * @return Integer How many nodes the content was stored on\n     *\n     * @throws java.io.IOException\n     *\n     */\n    public int put(JKademliaStorageEntry entry) throws IOException;\n\n    /**\n     * Store a content on the local node's DHT\n     *\n     * @param content The content to put on the DHT\n     *\n     * @throws java.io.IOException\n     */\n    public void putLocally(KadContent content) throws IOException;\n\n    /**\n     * Get some content stored on the DHT\n     *\n     * @param param The parameters used to search for the content\n     *\n     * @return DHTContent The content\n     *\n     * @throws java.io.IOException\n     * @throws kademlia.exceptions.ContentNotFoundException\n     */\n    public JKademliaStorageEntry get(GetParameter param) throws NoSuchElementException, IOException, ContentNotFoundException;\n\n    /**\n     * Allow the user of the System to call refresh even out of the normal Kad refresh timing\n     *\n     * @throws java.io.IOException\n     */\n    public void refresh() throws IOException;\n\n    /**\n     * @return String The ID of the owner of this local network\n     */\n    public String getOwnerId();\n\n    /**\n     * @return Integer The port on which this kad instance is running\n     */\n    public int getPort();\n\n    /**\n     * Here we handle properly shutting down the Kademlia instance\n     *\n     * @param saveState Whether to save the application state or not\n     *\n     * @throws java.io.FileNotFoundException\n     */\n    public void shutdown(final boolean saveState) throws IOException;\n\n    /**\n     * Saves the node state to a text file\n     *\n     * @throws java.io.FileNotFoundException\n     */\n    public void saveKadState() throws IOException;\n\n    /**\n     * @return The routing table for this node.\n     */\n    public KademliaRoutingTable getRoutingTable();\n\n    /**\n     * @return The statistician that manages all statistics\n     */\n    public KadStatistician getStatistician();\n}\nsrc/kademlia/message/Message.java\npublic interface Message extends Streamable\n{\n\n    /**\n     * The unique code for the message type, used to differentiate all messages\n     * from each other. Since this is of <code>byte</code> type there can\n     * be at most 256 different message types.\n     *\n     * @return byte A unique code representing the message type\n     * */\n    public byte code();\n}\nsrc/kademlia/node/Node.java\npublic class Node implements Streamable, Serializable\n{\n\n    private KademliaId nodeId;\n    private InetAddress inetAddress;\n    private int port;\n    private final String strRep;\n\n    public Node(KademliaId nid, InetAddress ip, int port)\n    {\n        this.nodeId = nid;\n        this.inetAddress = ip;\n        this.port = port;\n        this.strRep = this.nodeId.toString();\n    }\n\n    /**\n     * Load the Node's data from a DataInput stream\n     *\n     * @param in\n     *\n     * @throws IOException\n     */\n    public Node(DataInputStream in) throws IOException\n    {\n        this.fromStream(in);\n        this.strRep = this.nodeId.toString();\n    }\n\n    /**\n     * Set the InetAddress of this node\n     *\n     * @param addr The new InetAddress of this node\n     */\n    public void setInetAddress(InetAddress addr)\n    {\n        this.inetAddress = addr;\n    }\n\n    /**\n     * @return The NodeId object of this node\n     */\n    public KademliaId getNodeId()\n    {\n        return this.nodeId;\n    }\n\n    /**\n     * Creates a SocketAddress for this node\n     *\n     * @return\n     */\n    public InetSocketAddress getSocketAddress()\n    {\n        return new InetSocketAddress(this.inetAddress, this.port);\n    }\n\n    @Override\n    public void toStream(DataOutputStream out) throws IOException\n    {\n        /* Add the NodeId to the stream */\n        this.nodeId.toStream(out);\n\n        /* Add the Node's IP address to the stream */\n        byte[] a = inetAddress.getAddress();\n        if (a.length != 4)\n        {\n            throw new RuntimeException(\"Expected InetAddress of 4 bytes, got \" + a.length);\n        }\n        out.write(a);\n\n        /* Add the port to the stream */\n        out.writeInt(port);\n    }\n\n    @Override\n    public final void fromStream(DataInputStream in) throws IOException\n    {\n        /* Load the NodeId */\n        this.nodeId = new KademliaId(in);\n\n        /* Load the IP Address */\n        byte[] ip = new byte[4];\n        in.readFully(ip);\n        this.inetAddress = InetAddress.getByAddress(ip);\n\n        /* Read in the port */\n        this.port = in.readInt();\n    }\n\n    @Override\n    public boolean equals(Object o)\n    {\n        if (o instanceof Node)\n        {\n            Node n = (Node) o;\n            if (n == this)\n            {\n                return true;\n            }\n            return this.getNodeId().equals(n.getNodeId());\n        }\n        return false;\n    }\n\n    @Override\n    public int hashCode()\n    {\n        return this.getNodeId().hashCode();\n    }\n\n    @Override\n    public String toString()\n    {\n        return this.getNodeId().toString();\n    }\n}\nsrc/kademlia/message/ConnectMessage.java\npublic class ConnectMessage implements Message\n{\n\n    private Node origin;\n    public static final byte CODE = 0x02;\n\n    public ConnectMessage(Node origin)\n    {\n        this.origin = origin;\n    }\n\n    public ConnectMessage(DataInputStream in) throws IOException\n    {\n        this.fromStream(in);\n    }\n\n    @Override\n    public final void fromStream(DataInputStream in) throws IOException\n    {\n        this.origin = new Node(in);\n    }\n\n    @Override\n    public void toStream(DataOutputStream out) throws IOException\n    {\n        origin.toStream(out);\n    }\n\n    public Node getOrigin()\n    {\n        return this.origin;\n    }\n\n    @Override\n    public byte code()\n    {\n        return CODE;\n    }\n\n    @Override\n    public String toString()\n    {\n        return \"ConnectMessage[origin NodeId=\" + origin.getNodeId() + \"]\";\n    }\n}\nsrc/kademlia/exceptions/RoutingException.java\npublic class RoutingException extends IOException\n{\n\n    public RoutingException()\n    {\n        super();\n    }\n\n    public RoutingException(String message)\n    {\n        super(message);\n    }\n}\n", "answers": ["    private final Node bootstrapNode;"], "length": 3536, "dataset": "repobench-p_e", "language": "java", "all_classes": null, "_id": "26eec314bf72b6e3629b958f58e6dae01e8b6d3c3515566c"}
{"input": "package com.cosium.openapi.annotation_processor;\nimport static java.util.Objects.requireNonNull;\nimport com.cosium.logging.annotation_processor.AbstractLoggingProcessor;\nimport com.cosium.openapi.annotation_processor.code.CodeGenerator;\nimport com.cosium.openapi.annotation_processor.code.CodeGeneratorFactory;\nimport com.cosium.openapi.annotation_processor.file.FileManager;\nimport com.cosium.openapi.annotation_processor.file.FileManagerFactory;\nimport com.cosium.openapi.annotation_processor.loader.DefaultServiceLoader;\nimport com.cosium.openapi.annotation_processor.loader.ServiceLoader;\nimport com.cosium.openapi.annotation_processor.model.ParsedPath;\nimport com.cosium.openapi.annotation_processor.option.IOptions;\nimport com.cosium.openapi.annotation_processor.option.OptionsBuilder;\nimport com.cosium.openapi.annotation_processor.parser.PathParser;\nimport com.cosium.openapi.annotation_processor.parser.PathParserFactory;\nimport com.cosium.openapi.annotation_processor.parser.spring.SpringParserFactory;\nimport com.cosium.openapi.annotation_processor.specification.SpecificationGenerator;\nimport com.cosium.openapi.annotation_processor.specification.SpecificationGeneratorFactory;\nimport com.google.auto.service.AutoService;\nimport io.swagger.models.Swagger;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Set;\nimport java.util.concurrent.atomic.AtomicInteger;\nimport java.util.concurrent.atomic.AtomicReference;\nimport java.util.stream.Collectors;\nimport javax.annotation.processing.Messager;\nimport javax.annotation.processing.ProcessingEnvironment;\nimport javax.annotation.processing.Processor;\nimport javax.annotation.processing.RoundEnvironment;\nimport javax.lang.model.SourceVersion;\nimport javax.lang.model.element.Element;\nimport javax.lang.model.element.TypeElement;\nimport javax.lang.model.util.Elements;\nimport javax.lang.model.util.Types;\nimport javax.tools.Diagnostic;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n\n\n/**\n * Created on 12/07/17.\n *\n * @author Reda.Housni-Alaoui\n */\n@AutoService(Processor.class)\npublic class OpenAPIProcessor extends AbstractLoggingProcessor {\n\n    private static final Logger LOG = LoggerFactory.getLogger(OpenAPIProcessor.class);\n\n    private final AtomicInteger roundNumber = new AtomicInteger();\n    private final List<PathParserFactory> parserFactories = new ArrayList<>();\n    private final OptionsBuilder optionsBuilder = new OptionsBuilder();", "context": "src/main/java/com/cosium/openapi/annotation_processor/parser/spring/SpringParserFactory.java\npublic class SpringParserFactory implements PathParserFactory {\n\n    @Override\n    public String getSupportedAnnotation() {\n        return RequestMapping.class.getCanonicalName();\n    }\n\n    @Override\n    public PathParser build(Types typeUtils, Elements elementUtils) {\n        return new SpringParser(new PropertyUtils(typeUtils, elementUtils), new AnnotationUtils(typeUtils));\n    }\n}\nsrc/main/java/com/cosium/openapi/annotation_processor/option/IOptions.java\n@Value.Immutable\npublic interface IOptions {\n\n    /**\n     * @return The base package that will be used to generate all resources\n     */\n    String baseGenerationPackage();\n\n    /**\n     * @return The specification generator options\n     */\n    ISpecificationGeneratorOptions specificationGenerator();\n\n    /**\n     * @return The code generator options\n     */\n    ICodeGeneratorOptions codeGenerator();\n\n}\nsrc/main/java/com/cosium/openapi/annotation_processor/loader/ServiceLoader.java\npublic interface ServiceLoader {\n\n    /**\n     * @param serviceType The service type\n     * @param <T> The service type\n     * @return All the available instances for the service type\n     */\n    <T> List<T> load(Class<T> serviceType);\n\n}\nsrc/main/java/com/cosium/openapi/annotation_processor/parser/PathParser.java\npublic interface PathParser {\n\n    /**\n     * @param annotatedElement The element from which to parse the paths.\n     *                         This element is annoted with {@link PathParserFactory#getSupportedAnnotation()}.\n     * @return A list of parsed paths\n     */\n    List<ParsedPath> parse(Element annotatedElement);\n\n}\nsrc/main/java/com/cosium/openapi/annotation_processor/specification/SpecificationGenerator.java\npublic interface SpecificationGenerator {\n\n    /**\n     * Generate specification for given paths\n     *\n     * @param parsedPaths The parsed paths\n     * @param roundDescriptor The descriptor of the current round\n     * @return The generated specification\n     */\n    Swagger generate(List<ParsedPath> parsedPaths, RoundDescriptor roundDescriptor);\n\n}\nsrc/main/java/com/cosium/openapi/annotation_processor/loader/DefaultServiceLoader.java\npublic class DefaultServiceLoader implements ServiceLoader {\n\n    private final Map<Class<?>, List<?>> servicesByType = Collections.synchronizedMap(new HashMap<>());\n\n    @SuppressWarnings(\"unchecked\")\n    @Override\n    public <T> List<T> load(Class<T> serviceType) {\n        servicesByType.computeIfAbsent(serviceType, this::doLoad);\n        return (List<T>) servicesByType.get(serviceType);\n    }\n\n    private <T> List<T> doLoad(Class<T> serviceType) {\n        try (InputStream inputStream = getClass().getResourceAsStream(\"/META-INF/services/\" + serviceType.getCanonicalName())) {\n            return readLines(inputStream)\n                    .stream()\n                    .map(this::classForName)\n                    .map(this::newInstance)\n                    .map(serviceType::cast)\n                    .collect(Collectors.toList());\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        }\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    private List<String> readLines(InputStream inputStream) throws IOException {\n        return IOUtils.readLines(inputStream);\n    }\n\n    private Class<?> classForName(String name) {\n        try {\n            return Class.forName(name);\n        } catch (ClassNotFoundException e) {\n            throw new RuntimeException(e);\n        }\n    }\n\n    private <T> T newInstance(Class<T> type) {\n        try {\n            return type.newInstance();\n        } catch (InstantiationException | IllegalAccessException e) {\n            throw new RuntimeException(e);\n        }\n    }\n}\nsrc/main/java/com/cosium/openapi/annotation_processor/file/FileManager.java\npublic interface FileManager {\n\n    FileObject getResource(CharSequence relativeName) throws NoSuchFileException;\n\n    FileObject getResource(CharSequence pkg, CharSequence relativeName) throws NoSuchFileException;\n\n    FileObject createResource(CharSequence relativeName);\n\n    FileObject createResource(CharSequence pkg, CharSequence relativeName);\n\n}\nsrc/main/java/com/cosium/openapi/annotation_processor/model/ParsedPath.java\npublic class ParsedPath {\n\n    private final String pathTemplate;\n    private final Path path;\n\n    @JsonCreator\n    public ParsedPath(@JsonProperty(\"pathTemplate\") String pathTemplate, @JsonProperty(\"path\") Path path) {\n        requireNonNull(pathTemplate);\n        requireNonNull(path);\n\n        this.pathTemplate = pathTemplate;\n        this.path = path;\n    }\n\n    public String getPathTemplate() {\n        return pathTemplate;\n    }\n\n    public Path getPath() {\n        return path;\n    }\n\n    @Override\n    public String toString() {\n        return new ToStringBuilder(this)\n                .append(\"pathTemplate\", pathTemplate)\n                .toString();\n    }\n\n    @Override\n    public boolean equals(Object o) {\n        if (this == o) return true;\n        if (o == null || getClass() != o.getClass()) return false;\n\n        ParsedPath that = (ParsedPath) o;\n\n        return pathTemplate.equals(that.pathTemplate);\n    }\n\n    @Override\n    public int hashCode() {\n        return pathTemplate.hashCode();\n    }\n}\nsrc/main/java/com/cosium/openapi/annotation_processor/code/CodeGenerator.java\npublic interface CodeGenerator {\n\n    /**\n     * Generates code from specification\n     * @param swagger The specification to generate code from\n     */\n    void generate(Swagger swagger);\n\n}\nsrc/main/java/com/cosium/openapi/annotation_processor/option/OptionsBuilder.java\npublic class OptionsBuilder {\n\n    private static final String PREFIX = \"com.cosium.openapi.\";\n\n    private static final String BASE_GENERATION_PACKAGE = PREFIX + \"generation_package\";\n\n    private static final String SPECIFICATION_GENERATOR_PREFIX = PREFIX + \"specification_generator.\";\n\n    private static final String SPECIFICATION_GENERATOR_TITLE_OPTION = SPECIFICATION_GENERATOR_PREFIX + \"title\";\n    private static final String SPECIFICATION_GENERATOR_BASE_PATH_OPTION = SPECIFICATION_GENERATOR_PREFIX + \"base_path\";\n    private static final String SPECIFICATION_GENERATOR_PRODUCES_OPTION = SPECIFICATION_GENERATOR_PREFIX + \"produces\";\n    private static final String SPECIFICATION_GENERATOR_CONSUMES_OPTION = SPECIFICATION_GENERATOR_PREFIX + \"consumes\";\n\n    private static final String CODE_GENERATOR_PREFIX = PREFIX + \"code_generator.\";\n\n    private static final String CODE_GENERATOR_LANGUAGES_OPTIONS = CODE_GENERATOR_PREFIX + \"languages\";\n    private static final String CODE_GENERATOR_ONE_GENERATION_FOLDER_PER_LANGUAGE_OPTIONS = CODE_GENERATOR_PREFIX + \"one_generation_folder_per_language\";\n\n    private static final String APPLICATION_JSON = \"application/json\";\n\n    public Set<String> getSupportedOptions() {\n        return new LinkedHashSet<>(Arrays.asList(\n                BASE_GENERATION_PACKAGE,\n                SPECIFICATION_GENERATOR_TITLE_OPTION,\n                SPECIFICATION_GENERATOR_BASE_PATH_OPTION,\n                SPECIFICATION_GENERATOR_PRODUCES_OPTION,\n                SPECIFICATION_GENERATOR_CONSUMES_OPTION,\n                CODE_GENERATOR_LANGUAGES_OPTIONS,\n                CODE_GENERATOR_ONE_GENERATION_FOLDER_PER_LANGUAGE_OPTIONS\n        ));\n    }\n\n    public IOptions build(Map<String, String> options) {\n        return Options\n                .builder()\n                .baseGenerationPackage(options.getOrDefault(BASE_GENERATION_PACKAGE, \"com.cosium.openapi.generated\"))\n                .specificationGenerator(buildSpecificationGenerator(options))\n                .codeGenerator(buildCodeGenerator(options))\n                .build();\n    }\n\n    private ISpecificationGeneratorOptions buildSpecificationGenerator(Map<String, String> options) {\n        SpecificationGeneratorOptions.BuildFinal documentatorOptionsBuilder = SpecificationGeneratorOptions\n                .builder()\n                .title(options.getOrDefault(SPECIFICATION_GENERATOR_TITLE_OPTION, StringUtils.EMPTY))\n                .basePath(options.getOrDefault(SPECIFICATION_GENERATOR_BASE_PATH_OPTION, \"/\"));\n\n        Stream.of(StringUtils\n                .split(StringUtils.defaultIfBlank(options.get(SPECIFICATION_GENERATOR_CONSUMES_OPTION), APPLICATION_JSON), \",\"))\n                .forEach(documentatorOptionsBuilder::addConsumes);\n        Stream.of(StringUtils\n                .split(StringUtils.defaultIfBlank(options.get(SPECIFICATION_GENERATOR_PRODUCES_OPTION), APPLICATION_JSON), \",\"))\n                .forEach(documentatorOptionsBuilder::addProduces);\n        return documentatorOptionsBuilder.build();\n    }\n\n    private ICodeGeneratorOptions buildCodeGenerator(Map<String, String> options) {\n        CodeGeneratorOptions.BuildFinal builder = CodeGeneratorOptions.builder()\n                .oneGenerationFolderPerLanguage(Boolean.valueOf(\n                        options.getOrDefault(CODE_GENERATOR_ONE_GENERATION_FOLDER_PER_LANGUAGE_OPTIONS, String.valueOf(true))\n                ));\n\n        ofNullable(options.get(CODE_GENERATOR_LANGUAGES_OPTIONS))\n                .filter(StringUtils::isNotBlank)\n                .map(s -> StringUtils.split(s, \",\"))\n                .ifPresent(builder::addLanguages);\n\n        return builder.build();\n    }\n\n}\nsrc/main/java/com/cosium/openapi/annotation_processor/specification/SpecificationGeneratorFactory.java\npublic class SpecificationGeneratorFactory {\n\n    private final AtomicReference<Swagger> runtimeCache = new AtomicReference<>();\n    private final ISpecificationGeneratorOptions options;\n\n    public SpecificationGeneratorFactory(ISpecificationGeneratorOptions options) {\n        requireNonNull(options);\n        this.options = options;\n    }\n\n\n    public SpecificationGenerator build(FileManager fileManager) {\n        return new WriterSpecificationGenerator(\n                new IncrementalSpecificationGenerator(\n                        new BasicSpecificationGenerator(\n                                runtimeCache,\n                                options\n                        ), fileManager\n                ), fileManager\n        );\n    }\n\n}\nsrc/main/java/com/cosium/openapi/annotation_processor/code/CodeGeneratorFactory.java\npublic class CodeGeneratorFactory {\n\n    private final ICodeGeneratorOptions options;\n    private final ServiceLoader serviceLoader;\n\n    public CodeGeneratorFactory(ICodeGeneratorOptions options, ServiceLoader serviceLoader) {\n        requireNonNull(options);\n        requireNonNull(serviceLoader);\n        this.options = options;\n        this.serviceLoader = serviceLoader;\n    }\n\n    public CodeGenerator build(FileManager fileManager) {\n        return new DefaultCodeGenerator(\n                options,\n                serviceLoader,\n                fileManager\n        );\n    }\n\n}\nsrc/main/java/com/cosium/openapi/annotation_processor/file/FileManagerFactory.java\npublic class FileManagerFactory {\n\n    private final Filer filer;\n    private final String baseGenerationPackage;\n\n    public FileManagerFactory(Filer filer, String baseGenerationPackage) {\n        requireNonNull(filer);\n        requireNonNull(baseGenerationPackage);\n        this.filer = filer;\n        this.baseGenerationPackage = baseGenerationPackage;\n    }\n\n    public FileManager build(String packageName, Collection<Element> originatingElements) {\n        return new DefaultFileManager(baseGenerationPackage + \".\" + packageName, filer, originatingElements);\n    }\n\n}\nsrc/main/java/com/cosium/openapi/annotation_processor/parser/PathParserFactory.java\npublic interface PathParserFactory {\n\n    /**\n     * @return The supported annotation qualified name\n     */\n    String getSupportedAnnotation();\n\n    /**\n     * @return A new parser\n     */\n    PathParser build(Types typeUtils, Elements elementUtils);\n\n}\n", "answers": ["    private final ServiceLoader serviceLoader = new DefaultServiceLoader();"], "length": 925, "dataset": "repobench-p_e", "language": "java", "all_classes": null, "_id": "169dde6aa6a69a92950ca0529c356ab33884183329f88574"}
{"input": "from struct import (pack, unpack)\nfrom thrift.protocol.TBinaryProtocol import TBinaryProtocolAcceleratedFactory\nfrom ..asynchronous import (\n  AsyncResult,\n  NoopTimeout\n)\nfrom ..compat import BytesIO\nfrom ..constants import (\n  ChannelState,\n  SinkProperties,\n  SinkRole\n)\nfrom ..message import (\n  ChannelConcurrencyError,\n  ClientError,\n  Deadline,\n  MethodCallMessage,\n  MethodReturnMessage,\n  TimeoutError\n)\nfrom ..sink import (\n  SinkProvider,\n  SocketTransportSinkProvider,\n  ClientMessageSink,\n)\nfrom ..varz import (\n  AggregateTimer,\n  AverageTimer,\n  Rate,\n  Source,\n  VarzBase,\n)\nfrom .serializer import MessageSerializer\nimport time\nimport gevent\n\n\nclass SocketTransportSink(ClientMessageSink):\n  \"\"\"A sink to transport thrift method calls over a socket.\n\n  This sink does not support processing multiple messages in parallel and will\n  raise an exception if it detects it is about to.\n  \"\"\"\n  class Varz(VarzBase):\n    \"\"\"\n    messages_sent - The number of messages sent over this sink.\n    messages_recv - The number of messages received over this sink.\n    send_time - The aggregate amount of time spent sending data.\n    recv_time - The aggregate amount of time spend receiving data.\n    send_latency - The average amount of time taken to send a message.\n    recv_latency - The average amount of time taken to receive a message\n                   (once a response has reached the client).\n    transport_latency - The average amount of time taken to perform a full\n                        method call transaction (send data, wait for response,\n                        read response).\n    \"\"\"\n\n    _VARZ_BASE_NAME = 'scales.thrift.SocketTransportSink'\n    _VARZ = {\n      'messages_sent': Rate,\n      'messages_recv': Rate,\n      'send_time': AggregateTimer,\n      'recv_time': AggregateTimer,\n      'send_latency': AverageTimer,\n      'recv_latency': AverageTimer,\n      'transport_latency': AverageTimer\n    }\n\n  def __init__(self, socket, source):\n    super(SocketTransportSink, self).__init__()\n    self._socket = socket\n    self._state = ChannelState.Idle\n    socket_source = '%s:%d' % (self._socket.host, self._socket.port)\n    self._varz = self.Varz(Source(service=source, endpoint=socket_source))\n    self._processing = None\n    self._open_result = None\n\n  def Open(self):\n    if not self._open_result:\n      self._open_result = AsyncResult()\n      self._open_result.SafeLink(self._OpenImpl)\n    return self._open_result\n\n  def _OpenImpl(self):\n    try:\n      self._socket.open()\n      self._state = ChannelState.Open\n    except Exception:\n      self._Fault('Open failed')\n      raise\n\n  def Close(self):\n    self._state = ChannelState.Closed\n    self._socket.close()\n    self._open_result = None\n    if self._processing:\n      p, self._processing = self._processing, None\n      p.kill(block=False)\n\n  @property\n  def state(self):\n    if self._socket.isOpen():\n      return ChannelState.Open\n    else:\n      return self._state\n\n  def _Fault(self, reason):\n    \"\"\"Shutdown the sink and signal.\n\n    Args:\n      reason - The reason the shutdown occurred.  May be an exception or string.\n    \"\"\"\n    if self.state == ChannelState.Closed:\n      return\n\n    self.Close()\n    if not isinstance(reason, Exception):\n      reason = Exception(str(reason))\n    self._on_faulted.Set(reason)\n\n  def _AsyncProcessTransaction(self, data, sink_stack, deadline):\n    \"\"\"Process a method call/response transaction.\n    When complete the response is dispatched to the sink_stack.\n\n    Args:\n      data - The data to send.\n      sink_stack - The sink_stack for this method call.\n      deadline - The absolute deadline the transaction must complete by.\n    \"\"\"\n    with self._varz.transport_latency.Measure():\n      gtimeout = None\n      try:\n        if deadline:\n          timeout = deadline - time.time()\n          if timeout < 0:\n            raise gevent.Timeout()\n          gtimeout = gevent.Timeout.start_new(timeout)\n        else:\n          gtimeout = NoopTimeout()\n\n        with self._varz.send_time.Measure():\n          with self._varz.send_latency.Measure():\n            self._socket.write(data)\n        self._varz.messages_sent()\n\n        sz, = unpack('!i', self._socket.readAll(4))\n        with self._varz.recv_time.Measure():\n          with self._varz.recv_latency.Measure():\n            buf = BytesIO(self._socket.readAll(sz))\n        self._varz.messages_recv()\n\n        gtimeout.cancel()\n        self._processing = None\n        gevent.spawn(self._ProcessReply, buf, sink_stack)\n      except gevent.Timeout: # pylint: disable=E0712\n", "context": "scales/thrift/serializer.py\nclass MessageSerializer(object):\n  \"\"\"A serializer that can serialize and deserialize thrift method calls.\n\n  This relies on the generated thrift args and return value classes created\n  by the thrift compiler to do the serialization/deserialization.\n  \"\"\"\n  def __init__(\n      self,\n      service_cls,\n      protocol_factory=TBinaryProtocolAcceleratedFactory()):\n    \"\"\"\n    Args:\n      service_cls - The thrift generated interface class.\n      protocol_factory - A class implementing getProtocol(...).  By default,\n       TBinaryProtocolAcceleratedFactory is used.\n    \"\"\"\n    self._protocol_factory = protocol_factory\n    self._seq_id = 0\n    self._service_modules = [sys.modules[c.__module__]\n                             for c in inspect.getmro(service_cls)\n                             if not c is object]\n    if len(self._service_modules) == 1:\n      self._FindClass = self._FindClassNoInheritance\n    else:\n      self._attr_cache = {}\n      self._FindClass = self._FindClassInheritance\n\n  def _FindClassInheritance(self, name):\n    cls = self._attr_cache.get(name)\n    if cls:\n      return cls\n\n    for m in self._service_modules:\n      cls = getattr(m, name, None)\n      if cls:\n        self._attr_cache[name] = cls\n        return cls\n    return None\n\n  def _FindClassNoInheritance(self, name):\n    return getattr(self._service_modules[0], name, None)\n\n  def SerializeThriftCall(self, msg, buf):\n    \"\"\"Serialize a MethodCallMessage to a stream\n\n    Args:\n      msg - The MethodCallMessage to serialize.\n      buf - The buffer to serialize into.\n    \"\"\"\n    thrift_buffer = TMemoryBuffer()\n    thrift_buffer._buffer = buf\n    protocol = self._protocol_factory.getProtocol(thrift_buffer)\n    method, args, kwargs = msg.method, msg.args, msg.kwargs\n    is_one_way = self._FindClass('%s_result' % method) is None\n    args_cls = self._FindClass('%s_args' % method)\n    if not args_cls:\n      raise AttributeError('Unable to find args class for method %s' % method)\n\n    protocol.writeMessageBegin(\n        msg.method,\n        TMessageType.ONEWAY if is_one_way else TMessageType.CALL,\n        self._seq_id)\n    thrift_args = args_cls(*args, **kwargs)\n    thrift_args.write(protocol)\n    protocol.writeMessageEnd()\n\n  def DeserializeThriftCall(self, buf):\n    \"\"\"Deserialize a stream and context to a MethodReturnMessage.\n\n    Args:\n      buf - The buffer.\n      ctx - The context from serialization.\n\n    Returns:\n      A MethodCallMessage.\n    \"\"\"\n\n    thrift_buffer = TMemoryBuffer()\n    thrift_buffer._buffer = buf\n    protocol = self._protocol_factory.getProtocol(thrift_buffer)\n\n    (fn_name, msg_type, seq_id) = protocol.readMessageBegin()\n    if msg_type == TMessageType.EXCEPTION:\n      x = TApplicationException()\n      x.read(protocol)\n      protocol.readMessageEnd()\n      return MethodReturnMessage(error=x)\n\n    result_cls = self._FindClass('%s_result' % fn_name)\n    if result_cls:\n      result = result_cls()\n      result.read(protocol)\n    else:\n      result = None\n    protocol.readMessageEnd()\n\n    if not result:\n      return MethodReturnMessage()\n    if getattr(result, 'success', None) is not None:\n      return MethodReturnMessage(return_value=result.success)\n\n    result_spec = getattr(result_cls, 'thrift_spec', None)\n    if result_spec:\n      exceptions = result_spec[1:]\n      for e in exceptions:\n        attr_val = getattr(result, e[2], None)\n        if attr_val is not None:\n          return MethodReturnMessage(error=attr_val)\n\n    return MethodReturnMessage(TApplicationException(\n      TApplicationException.MISSING_RESULT, \"%s failed: unknown result\" % fn_name))\nscales/message.py\nclass Deadline(object):\r\n  KEY = \"__Deadline\"\r\n  EVENT_KEY = \"__Deadline_Event\"\r\n\r\n  def __init__(self, timeout):\r\n    \"\"\"\r\n    Args:\r\n      timeout - The timeout in seconds\r\n    \"\"\"\r\n    import  time\r\n    self._ts = Long(time.time()) * 1000000000 # Nanoseconds\r\n    self._timeout = Long(timeout * 1000000000)\r\nscales/message.py\nclass MethodReturnMessage(Message):\r\n  \"\"\"A message representing the return value from a service call.\"\"\"\r\n  __slots__ = ('return_value', 'error', 'stack')\r\n\r\n  def __init__(self, return_value=None, error=None):\r\n    \"\"\"\r\n    Args:\r\n      return_value - The return value of the call, or None\r\n      error - The error that occurred during processing, or None.\r\n              If not None, the current stack will be captured and included.\r\n    \"\"\"\r\n    super(MethodReturnMessage, self).__init__()\r\n    self.return_value = return_value\r\n    self.error = error\r\n    if error:\r\n      exc_info = sys.exc_info()\r\n      if len(exc_info) != 3 or exc_info[2] is None:\r\n        try:\r\n          raise ZeroDivisionError\r\n        except ZeroDivisionError:\r\n          tb = sys.exc_info()[2]\r\n          frame = tb.tb_frame.f_back\r\n      else:\r\n        tb = exc_info[2]\r\n        while tb.tb_next is not None:\r\n          tb = tb.tb_next\r\n        frame = tb.tb_frame\r\n\r\n      stack = traceback.format_list(traceback.extract_stack(frame))\r\n      stack = stack + traceback.format_exception_only(error.__class__, error)\r\n      self.stack = stack\r\n      # Prevent circular references\r\n      del frame\r\n      del tb\r\n    else:\r\n      self.stack = None\r\nscales/sink.py\ndef SinkProvider(sink_cls, role=None, **defaults):\n  \"\"\"Factory for creating simple sink providers.\n\n  Args:\n    sink_cls - The type of sink to provide.\n  Returns:\n    A SinkProvider that provides sinks of type 'sink_cls'.\n  \"\"\"\n  field_names = ' '.join(defaults.keys())\n  params_cls = namedtuple('Params', field_names)\n\n  def CreateSink(self, properties):\n    return self.SINK_CLASS(self.next_provider, self.sink_properties, properties)\n\n  def sink_class(self):\n    return self.SINK_CLASS\n\n  provider = type(\n    sink_cls.__name__ + 'Provider',\n    (SinkProviderBase, ),\n    {\n      'SINK_CLASS': sink_cls,\n      'PARAMS_CLASS': params_cls,\n      'Role': role,\n      'CreateSink': CreateSink,\n      'sink_class': property(sink_class),\n      '_defaults': defaults\n    }\n  )\n  return provider\nscales/asynchronous.py\nclass AsyncResult(g_AsyncResult):\n  @staticmethod\n  def WhenAll(ars):\n    \"\"\"Returns an AsyncResult representing the state of all AsyncResults passed.\n\n    Args:\n      ars - An enumerable of AsyncResults.\n    Returns:\n      An AsyncResult representing the completion of all ars passed in.  When all\n      complete, the AsyncResult will be set to an array of the results of each\n      AsyncResult, in the order they were enumerated in.\n      If any AsyncResult fails, the return result will fail.\n    \"\"\"\n\n    ret = AsyncResult()\n    num_ars = len(ars)\n    total = [num_ars]\n    results = [None] * num_ars\n    def complete(_n, _ar):\n      if _ar.exception:\n        ret.set_exception(_ar.exception)\n      elif not ret.ready():\n        total[0] -= 1\n        results[_n] = _ar.value\n        if total[0] == 0:\n          ret.set(results)\n\n    for n, ar in enumerate(ars):\n      ar.rawlink(functools.partial(complete, n))\n    return ret\n\n  @staticmethod\n  def WhenAny(ars):\n    \"\"\"Returns an AsyncResult representing the state of any AsyncResult passed in.\n    The return value represents the state of the first AsyncResult to complete, or,\n    if all fail, the last to fail.\n\n    Args:\n      ars - An enumerable of AsyncResults.\n    Returns:\n      An AsyncResult representing the state of the first AsyncResult to complete.\n      The AsyncResult's value will be set to the value of the first result to\n      complete, or, if all fail, the exception thrown by the last to fail.\n    \"\"\"\n    ready_ars = [ar for ar in ars if ar.ready()]\n    if ready_ars:\n      return ready_ars[0]\n\n    ret = AsyncResult()\n    total = [len(ars)]\n    def complete(_ar):\n      total[0] -= 1\n      if total[0] == 0 and _ar.exception:\n        ret.set_exception(_ar.exception)\n      elif not ret.ready() and _ar.successful():\n        ret.set(_ar.value)\n\n    for ar in ars:\n      ar.rawlink(complete)\n    return ret\n\n  @staticmethod\n  def FromValue(val):\n    if val is None:\n      return AsyncResult.Complete()\n    else:\n      ar = AsyncResult()\n      ar.set(val)\n      return ar\n\n  @staticmethod\n  def Complete():\n    \"\"\"Return an AsyncResult that has completed.\"\"\"\n    return _COMPLETE\n\n  @staticmethod\n  def CompleteIn(n):\n    \"\"\"Returns an AsyncResult that completes in <n> seconds\n\n    Args:\n      n - The number of seconds to wait before completing.\n    \"\"\"\n    ar = AsyncResult()\n    def helper():\n      ar.set()\n    g = Greenlet(helper)\n    g.start_later(float(n))\n    return ar\n\n  def _SafeLinkHelper(self, fn):\n    try:\n      self.set(fn())\n    except:\n      self.set_exception(sys.exc_info()[1])\n\n  def SafeLink(self, fn):\n    \"\"\"Propagate the result of calling fn() on a new greenlet to ar\n\n    Args:\n      ar - An AsyncResult.\n      fn - The function to execute.\n    \"\"\"\n    gevent.spawn(self._SafeLinkHelper, fn)\n\n  def ContinueWith(self, fn, on_hub=True):\n    cw_ar = AsyncResult()\n    def continue_with_callback(_ar):\n      def run():\n        try:\n          val = fn(_ar)\n          cw_ar.set(val)\n        except:\n          cw_ar.set_exception(sys.exc_info()[1])\n      if on_hub:\n        run()\n      else:\n        gevent.spawn(run)\n    self.rawlink(continue_with_callback)\n    return cw_ar\n\n  def Map(self, fn):\n    def mapper(_):\n      if self.exception:\n        return self\n      else:\n        return fn(self.value)\n    return self.ContinueWith(mapper).Unwrap()\n\n  def _UnwrapHelper(self, target):\n    if self.ready():\n      # We're ready, propagate the result\n      if self.exception:\n        target.set_exception(self.exception)\n      else:\n        if isinstance(self.value, AsyncResult):\n          self.value._UnwrapHelper(target)\n        else:\n          target.set(self.value)\n    else:\n      self.rawlink(\n        functools.partial(AsyncResult._UnwrapHelper, target=target))\n\n  def Unwrap(self):\n    unwrapped_ar = AsyncResult()\n    self._UnwrapHelper(unwrapped_ar)\n    return unwrapped_ar\n\n  @staticmethod\n  def TryGet(val):\n    if isinstance(val, AsyncResult):\n      return val.get()\n    else:\n      return val\n\n  @staticmethod\n  def Run(fn):\n    ar = AsyncResult()\n    ar.SafeLink(fn)\n    return ar\n\n  @staticmethod\n  def RunInline(fn):\n    ar = AsyncResult()\n    ar._SafeLinkHelper(fn)\n    return ar\nscales/message.py\nclass TimeoutError(Exception):\r\n  def __init__(self):\r\n    super(TimeoutError, self).__init__(\r\n      'The call did not complete within the specified timeout '\r\n      'and has been aborted.')\r\nscales/message.py\nclass ChannelConcurrencyError(Exception): pass\r\nscales/sink.py\ndef SocketTransportSinkProvider(sink_cls):\n  class _SocketTransportSinkProvider(SinkProviderBase):\n    SINK_CLS = sink_cls\n    Role = SinkRole.Transport\n\n    def CreateSink(self, properties):\n      server = properties[SinkProperties.Endpoint]\n      service = properties[SinkProperties.Label]\n      sock = ScalesSocket(server.host, server.port)\n      healthy_sock = VarzSocketWrapper(sock, service)\n      sink = self.SINK_CLS(healthy_sock, service)\n      return sink\n\n    @property\n    def sink_class(self):\n      return self.SINK_CLS\n\n  return _SocketTransportSinkProvider\nscales/constants.py\nclass ChannelState(object):\r\n  Idle = 1\r\n  Open = 2\r\n  Busy = 3\r\n  Closed = 4\r\nscales/varz.py\nclass VarzType(object):\nclass Source(object):\nclass VarzMetric(object):\nclass Gauge(VarzMetric): VARZ_TYPE = VarzType.Gauge\nclass Rate(VarzMetric): VARZ_TYPE = VarzType.Rate\nclass AverageRate(VarzMetric): VARZ_TYPE = VarzType.AverageRate\nclass Counter(Rate): VARZ_TYPE = VarzType.Counter\nclass VarzTimerBase(VarzMetric):\nclass AverageTimer(VarzTimerBase): VARZ_TYPE = VarzType.AverageTimer\nclass AggregateTimer(VarzTimerBase): VARZ_TYPE = VarzType.AggregateTimer\nclass VarzMeta(type):\nclass _VarzBase(object):\nclass _SampleSet(object):\nclass VarzReceiver(object):\nclass VarzAggregator(object):\n  class _Agg(object):\nclass VarzSocketWrapper(object):\n  class Varz(VarzBase):\nclass MonoClock(object):\nclass Ema(object):\n  def __init__(self, method=None, service=None, endpoint=None, client_id=None):\n  def to_tuple(self):\n  def to_dict(self):\n  def __cmp__(self, other):\n  def __hash__(self):\n  def _Adapt(fn):\n    def __Adapt(metric, source, amount=1):\n  def __init__(self, metric, source):\n  def __call__(self, *args):\n  def ForSource(self, source):\n  def Measure(self, source=None):\n  def __new__(mcs, name, bases, dct):\ndef VerifySource(source):\n  def __init__(self, source):\n  def __getattr__(self, item):\n  def __init__(self, max_size, data=None, p=.1):\n  def Sample(self, value):\n  def RegisterMetric(metric, varz_type):\n  def IncrementVarz(source, metric, amount=1):\n  def SetVarz(source, metric, value):\n  def RecordPercentileSample(cls, source, metric, value):\ndef DefaultKeySelector(k):\n    def __init__(self):\n  def CalculatePercentile(values, pct):\n  def _Downsample(lst, target_size):\n  def Aggregate(varz, metrics, key_selector=None):\n  def __init__(self, socket, varz_tag):\n  def host(self):\n  def port(self):\n  def isOpen(self):\n  def read(self, sz):\n  def recv_into(self, buf, sz):\n  def flush(self):\n  def write(self, buff):\n  def open(self):\n  def close(self):\n  def readAll(self, sz):\n  def __init__(self):\n  def Sample(self):\n  def __init__(self, window):\n  def Update(self, ts, sample):\n  VARZ_TYPE = None\nclass Gauge(VarzMetric): VARZ_TYPE = VarzType.Gauge\nclass Rate(VarzMetric): VARZ_TYPE = VarzType.Rate\nclass AverageRate(VarzMetric): VARZ_TYPE = VarzType.AverageRate\nclass Counter(Rate): VARZ_TYPE = VarzType.Counter\nclass AverageTimer(VarzTimerBase): VARZ_TYPE = VarzType.AverageTimer\nclass AggregateTimer(VarzTimerBase): VARZ_TYPE = VarzType.AggregateTimer\n  _VARZ = {}\n  _VARZ_BASE_NAME = None\n  VARZ_METRICS = {}\n  VARZ_DATA = defaultdict(lambda: defaultdict(int))\n  VARZ_PERCENTILES = [.5, .90, .99, .999, .9999]\n  _MAX_PERCENTILE_SIZE = 1000\n  MAX_AGG_AGE = 5 * 60\n    _VARZ_BASE_NAME = 'scales.socket'\n    _VARZ = {\n      'bytes_recv': Rate,\n      'bytes_sent': Rate,\n      'num_connections': Counter,\n      'tests_failed': Counter,\n      'connects': Rate,\n      'open_latency': AverageTimer\n    }\nscales/message.py\nclass MethodCallMessage(Message):\r\n  \"\"\"A MethodCallMessage represents a method being invoked on a service.\"\"\"\r\n  __slots__ = ('service', 'method', 'args', 'kwargs')\r\n\r\n  def __init__(self, service, method, args, kwargs):\r\n    \"\"\"\r\n    Args:\r\n      service - The service this method call is intended for.\r\n      method - The method on the service.\r\n      args - The args passed to the method call.\r\n      kwargs-  The kwargs passed to the method call.\r\n    \"\"\"\r\n    super(MethodCallMessage, self).__init__()\r\n    self.service = service\r\n    self.method = method\r\n    self.args = args\r\n    self.kwargs = kwargs\r\nscales/sink.py\nclass ClientMessageSink(MessageSink):\n  \"\"\"ClientMessageSinks take a message, stream, and headers and perform\n  processing on them.\n  \"\"\"\n  __slots__ = '_on_faulted',\n  Role = None\n  Builder = None\n\n  def __init__(self):\n    self._on_faulted = Observable()\n    super(ClientMessageSink, self).__init__()\n\n  @property\n  def state(self):\n    return self.next_sink.state\n\n  @property\n  def is_open(self):\n    \"\"\"Returns True if the sink is Idle, Open, or Busy\"\"\"\n    return self.state <= ChannelState.Busy\n\n  @property\n  def is_closed(self):\n    \"\"\"Returns True if the sink is Closed.\"\"\"\n    return self.state == ChannelState.Closed\n\n  @property\n  def is_ready(self):\n    \"\"\"Returns True if the channel is open, eg ready to process messages.\"\"\"\n    return self.state == ChannelState.Open\n\n  @property\n  def on_faulted(self):\n    return self._on_faulted\n\n  def Open(self):\n    if self.next_sink:\n      return self.next_sink.Open()\n    else:\n      return AsyncResult.Complete()\n\n  def Close(self):\n    if self.next_sink:\n      self.next_sink.Close()\n\n  @abstractmethod\n  def AsyncProcessRequest(self, sink_stack, msg, stream, headers):\n    \"\"\"Process a request message, stream, and headers.\n\n    Args:\n      sink_stack - The SinkStack representing the processing state of the message.\n                   Implementors should push their sink onto this stack before\n                   forwarding the message in order to participate in processing\n                   the response.\n      msg - The message being processed.\n      stream - A serialized version of the message.\n      headers - Any additional headers to be sent.\n    \"\"\"\n    raise NotImplementedError()\n\n  @abstractmethod\n  def AsyncProcessResponse(self, sink_stack, context, stream, msg):\n    \"\"\"Process a response stream.\n\n    Args:\n      sink_stack - The SinkStack representing the processing state of the message.\n                   Implementors should call sink_stack.AsyncProcessMessage(...)\n                   to forward the message to the next sink.\n      context - The context that was pushed onto the stack in AsyncProcessRequest.\n      stream - The stream representing the serialized response.\n    \"\"\"\n    raise NotImplementedError()\nscales/compat.py\n\nscales/constants.py\nclass SinkRole(object):\r\n  Transport = 'transport'\r\n  Pool = 'pool'\r\n  LoadBalancer = 'loadbalancer'\r\n  Formatter = 'formatter'\r\nscales/constants.py\nclass SinkProperties(object):\r\n  Endpoint = 'endpoint'\r\n  ServiceInterface = 'service_iface'\r\n  Label = 'label'\r\nscales/asynchronous.py\nclass NoopTimeout(object):\n  def start(self): pass\n  def cancel(self): pass\nscales/message.py\nclass ClientError(Exception): pass\r\n", "answers": ["        err = TimeoutError()"], "length": 2130, "dataset": "repobench-p_e", "language": "python", "all_classes": null, "_id": "d6e0fb2ff8a19c243e92b7242fece0adc5425bb8cc6722a6"}
{"input": "import sys\nimport os\nimport glob\nimport datetime\nimport copy\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n    import argparse\n    import RMS.ConfigReader as cr\nfrom RMS.Astrometry.Conversions import raDec2Vector, vector2RaDec, datetime2JD, jd2Date, raDec2AltAz, \\\n    geocentricToApparentRadiantAndVelocity, EARTH_CONSTANTS\nfrom RMS.Formats.FFfile import filenameToDatetime\nfrom RMS.Formats.FTPdetectinfo import readFTPdetectinfo\nfrom RMS.Formats.Showers import loadShowers, generateActivityDiagram, makeShowerColors\nfrom RMS.Math import vectNorm, angularSeparation, angularSeparationVect, isAngleBetween, \\\n    sphericalPointFromHeadingAndDistance, cartesianToPolar\nfrom RMS.Routines.GreatCircle import fitGreatCircle, greatCircle, greatCirclePhase\nfrom RMS.Routines.SolarLongitude import jd2SolLonSteyaert\nfrom RMS.Routines.AllskyPlot import AllSkyPlot\n    beg_azim, beg_alt = raDec2AltAz(beg_ra, beg_dec, meteor_obj.jdt_ref, meteor_obj.lat, meteor_obj.lon)\n    beg_vect_horiz = raDec2Vector(beg_azim, beg_alt)\n\n    # Compute end point vector in alt/az\n    end_ra, end_dec = vector2RaDec(meteor_obj.end_vect)\n    end_azim, end_alt = raDec2AltAz(end_ra, end_dec, meteor_obj.jdt_ref, meteor_obj.lat, meteor_obj.lon)\n    end_vect_horiz = raDec2Vector(end_azim, end_alt)\n\n    # Compute radiant vector in alt/az\n    radiant_azim, radiant_alt = raDec2AltAz(shower.ra, shower.dec, meteor_obj.jdt_ref, meteor_obj.lat, \\\n        meteor_obj.lon)\n    radiant_vector_horiz = raDec2Vector(radiant_azim, radiant_alt)\n\n\n    # Reject the pairing if the radiant is below the horizon\n    if radiant_alt < 0:\n        return -1\n\n\n    # Get distance from Earth's centre to the position given by geographical coordinates for the \n    #   observer's latitude\n    earth_radius = EARTH.EQUATORIAL_RADIUS/np.sqrt(1.0 - (EARTH.E**2)*np.sin(np.radians(config.latitude))**2)\n\n    # Compute the distance from Earth's centre to the station (including the sea level height of the station)\n    re_dist = earth_radius + config.elevation\n\n    ### ###\n\n\n    # Compute the distance the meteor traversed during its duration (meters)\n    dist = shower.v_init*meteor_obj.duration\n\n    # Compute the angle between the begin and the end point of the meteor (rad)\n    ang_beg_end = np.arccos(np.dot(vectNorm(beg_vect_horiz), vectNorm(end_vect_horiz)))\n\n    # Compute the angle between the radiant vector and the begin point (rad)\n    ang_beg_rad = np.arccos(np.dot(vectNorm(radiant_vector_horiz), -vectNorm(beg_vect_horiz)))\n\n\n    # Compute the distance from the station to the begin point (meters)\n    dist_beg = dist*np.sin(ang_beg_rad)/np.sin(ang_beg_end)\n\n\n    # Compute the height using the law of cosines\n    ht  = np.sqrt(dist_beg**2 + re_dist**2 - 2*dist_beg*re_dist*np.cos(np.radians(90 + meteor_obj.beg_alt)))\n    ht -= earth_radius\n    ht  = abs(ht)\n\n\n    return ht\n\n\n\n\ndef showerAssociation(config, ftpdetectinfo_list, shower_code=None, show_plot=False, save_plot=False, \\\n    plot_activity=False):\n    \"\"\" Do single station shower association based on radiant direction and height. \n    \n    Arguments:\n        config: [Config instance]\n        ftpdetectinfo_list: [list] A list of paths to FTPdetectinfo files.\n\n    Keyword arguments:\n        shower_code: [str] Only use this one shower for association (e.g. ETA, PER, SDA). None by default,\n            in which case all active showers will be associated.\n        show_plot: [bool] Show the plot on the screen. False by default.\n        save_plot: [bool] Save the plot in the folder with FTPdetectinfos. False by default.\n        plot_activity: [bool] Whether to plot the shower activity plot of not. False by default.\n\n    Return:\n        associations, shower_counts: [tuple]\n            - associations: [dict] A dictionary where the FF name and the meteor ordinal number on the FF\n                file are keys, and the associated Shower object are values.\n            - shower_counts: [list] A list of shower code and shower count pairs.\n    \"\"\"\n\n    # Load the list of meteor showers\n    shower_list = loadShowers(config.shower_path, config.shower_file_name)\n\n\n    # Load FTPdetectinfos\n    meteor_data = []\n    for ftpdetectinfo_path in ftpdetectinfo_list:\n\n        if not os.path.isfile(ftpdetectinfo_path):\n            print('No such file:', ftpdetectinfo_path)\n            continue\n\n        meteor_data += readFTPdetectinfo(*os.path.split(ftpdetectinfo_path))\n\n    if not len(meteor_data):\n        return {}, []\n\n\n    # Dictionary which holds FF names as keys and meteor measurements + associated showers as values\n    associations = {}\n\n    for meteor in meteor_data:\n\n        ff_name, cam_code, meteor_No, n_segments, fps, hnr, mle, binn, px_fm, rho, phi, meteor_meas = meteor\n\n        # Skip very short meteors\n        if len(meteor_meas) < 4:\n            continue\n\n        # Check if the data is calibrated\n        if not meteor_meas[0][0]:\n            print('Data is not calibrated! Meteors cannot be associated to showers!')\n            break\n\n\n        # Init container for meteor observation\n        meteor_obj = MeteorSingleStation(cam_code, config.latitude, config.longitude, ff_name)\n\n        # Infill the meteor structure\n        for entry in meteor_meas:\n            \n            calib_status, frame_n, x, y, ra, dec, azim, elev, inten, mag = entry\n\n            # Compute the Julian data of every point\n", "context": "RMS/Math.py\ndef angularSeparationVect(vect1, vect2):\n    \"\"\" Calculates angle between vectors in radians. \"\"\"\n\n    return np.abs(np.arccos(np.dot(vect1, vect2)))\nRMS/Math.py\n@np.vectorize\ndef sphericalPointFromHeadingAndDistance(ra1, dec1, heading, distance):\n    \"\"\" Given RA and Dec, a heading and angular distance, compute coordinates of the point.\n\n    Arguments:\n        ra1: [float] Right Ascension (deg).\n        dec1: [float] Declination (deg).\n        heading: [float] Heading +E of due N in degrees (deg).\n        distance: [float] Distance (deg).\n\n    Return:\n        ra, dec: [float] Coordinates of the new point (deg)\n\n    \"\"\"\n\n    ra1 = np.radians(ra1)\n    dec1 = np.radians(dec1)\n    heading = np.radians(heading)\n    distance = np.radians(distance)\n\n    # Compute the new declination\n    dec = np.arcsin(np.sin(dec1)*np.cos(distance) + np.cos(dec1)*np.sin(distance)*np.cos(heading))\n\n    # Compute the new RA\n    dra = np.arctan2(np.sin(heading)*np.sin(distance)*np.cos(dec1), np.cos(distance) \\\n        - np.sin(dec1)*np.sin(dec))\n    ra = (ra1 - dra + np.pi)%(2*np.pi) - np.pi\n\n\n    return np.degrees(ra)%360, np.degrees(dec)\nRMS/Astrometry/Conversions.py\ndef raDec2AltAz(ra, dec, jd, lat, lon):\n    \"\"\" Calculate the reference azimuth and altitude of the centre of the FOV from the given RA/Dec.\n    Arguments:\n        ra:  [float] Right ascension in degrees.\n        dec: [float] Declination in degrees.\n        jd: [float] Reference Julian date.\n        lat: [float] Latitude +N in degrees.\n        lon: [float] Longitude +E in degrees.\n    Return:\n        (azim, elev): [tuple of float]: Azimuth and elevation (degrees).\n    \"\"\"\n    ra = np.radians(ra)\n    dec = np.radians(dec)\n    lat = np.radians(lat)\n    lon = np.radians(lon)\n\n    # Compute azim and elev using a fast cython function\n    if isinstance(ra, float) or isinstance(ra, int) or isinstance(ra, np.float64):\n        azim, elev = cyraDec2AltAz(ra, dec, jd, lat, lon)\n\n    elif isinstance(ra, np.ndarray):\n        # Compute it for numpy arrays\n        azim, elev = cyraDec2AltAz_vect(ra, dec, jd, lat, lon)\n\n    else:\n        raise TypeError(\"ra must be a number or np.ndarray, given: {}\".format(type(ra)))\n\n    return np.degrees(azim), np.degrees(elev)\nRMS/Math.py\ndef angularSeparation(ra1, dec1, ra2, dec2):\n    \"\"\" Calculates the angle between two points on a sphere. \n    \n    Arguments:\n        ra1: [float] Right ascension 1 (radians).\n        dec1: [float] Declination 1 (radians).\n        ra2: [float] Right ascension 2 (radians).\n        dec2: [float] Declination 2 (radians).\n\n    Return:\n        [float] Angle between two coordinates (radians).\n    \"\"\"\n\n    # Classical method\n    return np.arccos(np.sin(dec1)*np.sin(dec2) + np.cos(dec1)*np.cos(dec2)*np.cos(ra2 - ra1))\n\n    # # Compute the angular separation using the haversine formula\n    # #   Source: https://idlastro.gsfc.nasa.gov/ftp/pro/astro/gcirc.pro\n    # deldec2 = (dec2 - dec1)/2.0\n    # delra2 =  (ra2 - ra1)/2.0\n    # sindis = np.sqrt(np.sin(deldec2)*np.sin(deldec2) \\\n    #     + np.cos(dec1)*np.cos(dec2)*np.sin(delra2)*np.sin(delra2))\n    # dis = 2.0*np.arcsin(sindis) \n\n    # return dis\nRMS/Astrometry/Conversions.py\ndef jd2Date(jd, UT_corr=0, dt_obj=False):\n    \"\"\" Converts the given Julian date to (year, month, day, hour, minute, second, millisecond) tuple.\n    Arguments:\n        jd: [float] Julian date\n    Keyword arguments:\n        UT_corr: [float] UT correction in hours (difference from local time to UT)\n        dt_obj: [bool] returns a datetime object if True. False by default.\n    Return:\n        (year, month, day, hour, minute, second, millisecond)\n    \"\"\"\n\n    dt = timedelta(days=jd)\n\n    try:\n        date = dt + JULIAN_EPOCH - J2000_JD + timedelta(hours=UT_corr)\n\n    # If the date is out of range (i.e. before year 1) use year 1. This is the limitation in the datetime\n    # library. Time handling should be switched to astropy.time\n    except OverflowError:\n        date = datetime(MINYEAR, 1, 1, 0, 0, 0)\n\n    # Return a datetime object if dt_obj == True\n    if dt_obj:\n        return date\n\n    return date.year, date.month, date.day, date.hour, date.minute, date.second, date.microsecond/1000.0\nRMS/Routines/SolarLongitude.py\ndef jd2SolLonSteyaert(jd):\n    \"\"\" Convert the given Julian date to solar longitude, J2000.0 epoch. Chris Steyaert method.\n\n    Reference: Steyaert, C. (1991). Calculating the solar longitude 2000.0. WGN, Journal of the International \n        Meteor Organization, 19, 31-34.\n\n    Arguments:\n        jd: [float] julian date\n\n    Return:\n        [float] solar longitude in radians, J2000.0 epoch\n\n    \"\"\"\n\n    # Define time constants\n    A0 = [334166, 3489, 350, 342, 314, 268, 234, 132, 127, 120, 99, 90, 86, 78, 75, 51, 49, 36, 32, 28, 27, \n        24, 21, 21, 20, 16, 13, 13]\n\n    B0 = [4.669257, 4.6261, 2.744, 2.829, 3.628, 4.418, 6.135, 0.742, 2.037, 1.110, 5.233, 2.045, 3.508, \n        1.179, 2.533, 4.58, 4.21, 2.92, 5.85, 1.90, 0.31, 0.34, 4.81, 1.87, 2.46, 0.83, 3.41, 1.08]\n\n    C0 = [6283.07585, 12566.1517, 5753.385, 3.523, 77713.771, 7860.419, 3930.210, 11506.77, 529.691, 1577.344, \n        5884.927, 26.298, 398.149, 5223.694, 5507.553, 18849.23, 775.52, 0.07, 11790.63, 796.3, 10977.08, \n        5486.78, 2544.31, 5573.14, 6069.78, 213.3, 2942.46, 20.78]\n\n    A1 = [20606, 430, 43]\n    B1 = [2.67823, 2.635, 1.59]\n    C1 = [6283.07585, 12566.152, 3.52]\n\n    A2 = [872, 29]\n    B2 = [1.073, 0.44]\n    C2 = [6283.07585, 12566.15]\n\n    A3 = 29\n    B3 = 5.84\n    C3 = 6283.07585\n\n    # Number of millennia since 2000\n    T = (jd - 2451545.0)/365250.0\n\n    # Mean solar longitude\n    L0 = 4.8950627 + 6283.07585*T - 0.0000099*T**2\n\n    # Wrap L0 to [0, 2pi] range\n    L0 = L0%(2*np.pi)\n\n    # Periodical terms\n    S0 = np.sum([A0[i]*np.cos((B0[i] + C0[i]*T)%(2*np.pi)) for i in range(28)])\n    S1 = np.sum([A1[i]*np.cos((B1[i] + C1[i]*T)%(2*np.pi)) for i in range(3)])\n    S2 = np.sum([A2[i]*np.cos((B2[i] + C2[i]*T)%(2*np.pi)) for i in range(2)])\n    S3 = A3*np.cos((B3 + C3*T)%(2*np.pi))\n\n    # Solar longitude of J2000.0\n    L = L0 + (S0 + S1*T + S2*T**2 + S3*T**3)*1e-7\n\n    # Bound to solar longitude to the [0, 2pi] range\n    L = L%(2*np.pi)\n\n    return L\nRMS/Math.py\ndef vectNorm(vect):\n    \"\"\" Convert a given vector to a unit vector. \"\"\"\n\n    return vect/vectMag(vect)\nRMS/Astrometry/Conversions.py\ndef vector2RaDec(eci):\n    \"\"\" Convert Earth-centered intertial vector to right ascension and declination.\n    Arguments:\n        eci: [3 element ndarray] Vector coordinates in Earth-centered inertial system\n    Return:\n        (ra, dec): [tuple of floats] right ascension and declinaton (degrees)\n    \"\"\"\n\n    # Normalize the ECI coordinates\n    eci = vectNorm(eci)\n\n    # Calculate declination\n    dec = np.arcsin(eci[2])\n\n    # Calculate right ascension\n    ra = np.arctan2(eci[1], eci[0])%(2*np.pi)\n\n    return np.degrees(ra), np.degrees(dec)\nRMS/Math.py\ndef cartesianToPolar(x, y, z):\n    \"\"\" Converts 3D cartesian coordinates to polar coordinates. \n\n    Arguments:\n        x: [float] Px coordinate.\n        y: [float] Py coordinate.\n        z: [float] Pz coordinate.\n\n    Return:\n        (theta, phi): [float] Polar angles in radians (inclination, azimuth).\n\n    \"\"\"\n\n    theta = np.arccos(z)\n    phi = np.arctan2(y, x)\n\n    return theta, phi\nRMS/Astrometry/Conversions.py\ndef raDec2Vector(ra, dec):\n    \"\"\" Convert stellar equatorial coordinates to a vector with X, Y and Z components.\n    @param ra: [float] right ascension in degrees\n    @param dec: [float] declination in degrees\n    @return (x, y, z): [tuple of floats]\n    \"\"\"\n\n    ra_rad = math.radians(ra)\n    dec_rad = math.radians(dec)\n\n    xt = math.cos(dec_rad)*math.cos(ra_rad)\n    yt = math.cos(dec_rad)*math.sin(ra_rad)\n    zt = math.sin(dec_rad)\n\n    return xt, yt, zt\nRMS/Formats/FFfile.py\ndef filenameToDatetime(file_name):\n    \"\"\" Converts FF bin file name to a datetime object.\n\n    Arguments:\n        file_name: [str] Name of a FF file.\n\n    Return:\n        [datetime object] Date and time of the first frame in the FF file.\n\n    \"\"\"\n\n    # e.g.  FF499_20170626_020520_353_0005120.bin\n    # or FF_CA0001_20170626_020520_353_0005120.fits\n\n    file_name = file_name.split('_')\n\n    # Check the number of list elements, and the new fits format has one more underscore\n    i = 0\n    if len(file_name[0]) == 2:\n        i = 1\n\n    date = file_name[i + 1]\n    year = int(date[:4])\n    month = int(date[4:6])\n    day = int(date[6:8])\n\n    time = file_name[i + 2]\n    hour = int(time[:2])\n    minute = int(time[2:4])\n    seconds = int(time[4:6])\n\n    ms = int(file_name[i + 3])\n\n\n    return datetime.datetime(year, month, day, hour, minute, seconds, ms*1000)\nRMS/Astrometry/Conversions.py\ndef datetime2JD(dt, UT_corr=0.0):\n    \"\"\" Converts a datetime object to Julian date.\n    Arguments:\n        dt: [datetime object]\n    Keyword arguments:\n        UT_corr: [float] UT correction in hours (difference from local time to UT)\n    Return:\n        jd: [float] Julian date\n    \"\"\"\n\n    return date2JD(dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second, dt.microsecond/1000.0,\n                   UT_corr=UT_corr)\nRMS/Astrometry/Conversions.py\ndef geocentricToApparentRadiantAndVelocity(ra_g, dec_g, vg, lat, lon, elev, jd, include_rotation=True):\n    \"\"\" Converts the geocentric into apparent meteor radiant and velocity. The conversion is not perfect\n        as the zenith attraction correction should be done after the radiant has been derotated for Earth's\n        velocity, but it's precise to about 0.1 deg.\n\n    Arguments:\n        ra_g: [float] Geocentric right ascension (deg).\n        dec_g: [float] Declination (deg).\n        vg: [float] Geocentric velocity (m/s).\n        lat: [float] State vector latitude (deg)\n        lon: [float] State vector longitude (deg).\n        ele: [float] State vector elevation (meters).\n        jd: [float] Julian date.\n    Keyword arguments:\n        include_rotation: [bool] Whether the velocity should be corrected for Earth's rotation.\n            True by default.\n    Return:\n        (ra, dec, v_init): Apparent radiant (deg) and velocity (m/s).\n    \"\"\"\n\n    # Compute ECI coordinates of the meteor state vector\n    state_vector = geo2Cartesian(lat, lon, elev, jd)\n\n    eci_x, eci_y, eci_z = state_vector\n\n    # Assume that the velocity at infinity corresponds to the initial velocity\n    v_init = np.sqrt(vg**2 + (2*6.67408*5.9722)*1e13/vectMag(state_vector))\n\n    # Calculate the geocentric latitude (latitude which considers the Earth as an elipsoid) of the reference\n    # trajectory point\n    lat_geocentric = np.degrees(math.atan2(eci_z, math.sqrt(eci_x**2 + eci_y**2)))\n\n    ### Uncorrect for zenith attraction ###\n\n    # Compute the radiant in the local coordinates\n    azim, elev = raDec2AltAz(ra_g, dec_g, jd, lat_geocentric, lon)\n\n    # Compute the zenith angle\n    eta = np.radians(90.0 - elev)\n\n    # Numerically correct for zenith attraction\n    diff = 10e-5\n    zc = eta\n    while diff > 10e-6:\n        # Update the zenith distance\n        zc -= diff\n\n        # Calculate the zenith attraction correction\n        delta_zc = 2*math.atan((v_init - vg)*math.tan(zc/2.0)/(v_init + vg))\n        diff = zc + delta_zc - eta\n\n    # Compute the uncorrected geocentric radiant for zenith attraction\n    ra, dec = altAz2RADec(azim, 90.0 - np.degrees(zc), jd, lat_geocentric, lon)\n\n    ### ###\n\n    # Apply the rotation correction\n    if include_rotation:\n        # Calculate the velocity of the Earth rotation at the position of the reference trajectory point (m/s)\n        v_e = 2*math.pi*vectMag(state_vector)*math.cos(np.radians(lat_geocentric))/86164.09053\n\n        # Calculate the equatorial coordinates of east from the reference position on the trajectory\n        azimuth_east = 90.0\n        altitude_east = 0\n        ra_east, dec_east = altAz2RADec(azimuth_east, altitude_east, jd, lat, lon)\n\n        # Compute the radiant vector in ECI coordinates of the apparent radiant\n        v_ref_vect = v_init*np.array(raDec2Vector(ra, dec))\n\n        v_ref_nocorr = np.zeros(3)\n\n        # Calculate the derotated reference velocity vector/radiant\n        v_ref_nocorr[0] = v_ref_vect[0] + v_e*np.cos(np.radians(ra_east))\n        v_ref_nocorr[1] = v_ref_vect[1] + v_e*np.sin(np.radians(ra_east))\n        v_ref_nocorr[2] = v_ref_vect[2]\n\n        # Compute the radiant without Earth's rotation included\n        ra_norot, dec_norot = vector2RaDec(vectNorm(v_ref_nocorr))\n        v_init_norot = vectMag(v_ref_nocorr)\n\n        ra = ra_norot\n        dec = dec_norot\n        v_init = v_init_norot\n\n    return ra, dec, v_init\nRMS/Math.py\ndef isAngleBetween(left, ang, right):\n    \"\"\" Checks if ang is between the angle on the left and right. \n    \n    Arguments:\n        left: [float] Left (counter-clockwise) angle (radians).\n        ang: [float] Angle to check (radians),\n        right: [float] Right (clockwise) angle (radiant).\n\n    Return:\n        [bool] True if the angle is in between, false otherwise.\n    \"\"\"\n\n    if right - left < 0:\n        right = right - left + 2*np.pi\n    else:\n        right = right - left\n\n\n    if ang - left < 0:\n        ang = ang - left + 2*np.pi\n    else:\n        ang = ang - left\n\n\n    return ang < right\nRMS/Astrometry/Conversions.py\nclass EARTH_CONSTANTS(object):\n    \"\"\" Holds Earth's shape and physical parameters. \"\"\"\n\n    def __init__(self):\n\n        # Earth elipsoid parameters in meters (source: WGS84, the GPS standard)\n        self.EQUATORIAL_RADIUS = 6378137.0\n        self.POLAR_RADIUS = 6356752.314245\n        self.E = math.sqrt(1.0 - self.POLAR_RADIUS**2/self.EQUATORIAL_RADIUS**2)\n        self.RATIO = self.EQUATORIAL_RADIUS/self.POLAR_RADIUS\n        self.SQR_DIFF = self.EQUATORIAL_RADIUS**2 - self.POLAR_RADIUS**2\nRMS/Routines/AllskyPlot.py\nclass AllSkyPlot(object):\n\tdef __init__(self, ax_handle=None):\n\n\t\tself.ra0 = 180.0\n\n\n\t\tif ax_handle is None:\n\t\t\t\n\t\t\tself.fig = plt.figure()\n\t\t\tself.ax = self.fig.add_subplot(1, 1, 1, facecolor='black')\n\n\t\telse:\n\t\t\tself.ax = ax_handle\n\t\t\tself.fig = plt.gcf()\n\n\t\t# Set background color\n\t\tself.fig.patch.set_facecolor('black')\n\n\t\t# Set equal aspect ratio\n\t\tself.ax.set_aspect('equal')\n\n\t\t# # Set tick color\n\t\t# self.ax.tick_params(axis='x', colors='0.5')\n\t\t# self.ax.tick_params(axis='y', colors='0.5')\n\n\t\t# Turn off ticks\n\t\tself.ax.tick_params(labeltop=False, labelright=False, labelbottom=False, labelleft=False)\n\n\t\tself.plotGrid()\n\n\n\tdef raDec2XY(self, ra, dec):\n\n\t\t# Compute projected coordinates\n\t\tx = ((180 - ra)%360 - self.ra0)*np.cos(np.radians(dec))\n\t\ty = dec\n\n\t\treturn x, y\n\n\n\tdef plot(self, ra_array, dec_array, max_break_deg=30, **kwargs):\n\n\t\t# If there are more than one point, check for 0/360 wraparounds in RA\n\t\tif isinstance(ra_array, list) or isinstance(ra_array, np.ndarray):\n\n\t\t\tra_array = np.array(ra_array)\n\t\t\tra_array = (180 - ra_array)%360\n\t\t\tdec_array = np.array(dec_array)\n\n\t\t\tcoord_list = []\n\n\t\t\t# Find large breaks in RA and plot them separately\n\t\t\tra_diff = np.abs(ra_array[:-1] - ra_array[1:])\n\t\t\tbreak_indices = np.where(ra_diff > max_break_deg)[0]\n\n\t\t\tif not len(break_indices):\n\t\t\t\tcoord_list = [[ra_array, dec_array]]\n\n\t\t\telse:\n\t\t\t\tprev_break_idx = 0\n\t\t\t\tfor break_idx in break_indices:\n\t\t\t\t\tra_temp = ra_array[prev_break_idx:break_idx + 1]\n\t\t\t\t\tdec_temp = dec_array[prev_break_idx:break_idx + 1]\n\n\t\t\t\t\tprev_break_idx = break_idx + 1\n\n\t\t\t\t\tcoord_list.append([ra_temp, dec_temp])\n\n\t\t\t\tcoord_list.append([ra_array[break_idx + 1:], dec_array[break_idx + 1:]])\n\n\t\telse:\n\t\t\tcoord_list = [[180 - ra_array, dec_array]]\n\n\n\t\t# Plot all segments\n\t\tfor i, (ra_temp, dec_temp) in enumerate(coord_list):\n\t\t\tx, y = self.raDec2XY(180 - ra_temp, dec_temp)\n\n\t\t\t# Make sure that all plotted lines have the same color\n\t\t\tif i > 0:\n\t\t\t\tcolor = plt_handle[0].get_color()\n\n\t\t\t\t# Add color to kwargs\n\t\t\t\tif 'color' not in kwargs:\n\t\t\t\t\tkwargs['color'] = color\n\t\t\t\n\n\t\t\tplt_handle = self.ax.plot(x, y, **kwargs)\n\n\n\n\tdef scatter(self, ra_array, dec_array, **kwargs):\n\n\t\tx, y = self.raDec2XY(ra_array, dec_array)\n\t\tself.ax.scatter(x, y, **kwargs)\n\n\n\n\n\tdef plotGrid(self, step=15):\n\n\n\t\t# Plot a meridian and parallel grid\n\t\tra_grid = np.sort(np.append(np.arange(0, 360 + step, step), [180.0001]))\n\t\tdec_grid = np.arange(-90, 90 + step, step)\n\n\n\t\t# Plot meridians\n\t\tfor ra in ra_grid[:-1]:\n\n\t\t\t# Increase number of points for meridian plot so they are smoother\n\t\t\tstep_finer = step/5\n\t\t\tdec_arr = np.arange(-90, 90 + step_finer, step_finer)\n\n\t\t\tra_temp = np.zeros_like(dec_arr) + ra\n\n\t\t\tx_grid, y_grid = self.raDec2XY(ra_temp, dec_arr)\n\n\t\t\tself.ax.plot(x_grid, y_grid, linestyle='dotted', alpha=0.5, color='silver')\n\n\n\t\t# Plot parallels\n\t\tfor dec in dec_grid:\n\n\t\t\tdec_temp = np.zeros_like(ra_grid) + dec\n\n\t\t\tself.plot(ra_grid, dec_temp, linestyle='dotted', alpha=0.5, color='silver')\n\n\n\t\t# Plot dec ticks\n\t\tfor dec in dec_grid[::2]:\n\n\t\t\tx, y = self.raDec2XY(0, dec)\n\n\t\t\tif dec > 0:\n\t\t\t\tva = 'bottom'\n\n\t\t\telse:\n\t\t\t\tva = 'top'\n\n\t\t\tself.ax.text(x, y, \"{:+d}$^\\circ$\".format(dec), color='0.5', ha='center', va=va, size=7)\n\n\n\t\t# Plot every other RA tick and skip 0 and 360\n\t\tra_ticks = np.sort(np.append(np.arange(0, 360, 2*step), [180.0001]))\n\t\tfor ra in ra_ticks:\n\n\t\t\t# Offset RA so 0 starts in the middle and increases to the left\n\t\t\t#ra_text = (180 - ra)%360\n\n\t\t\tx, y = self.raDec2XY(ra, 0)\n\t\t\tself.ax.text(x, y, \"{:+d}$^\\circ$\".format(int(ra)), color='0.5', ha='center', va='top', size=7)\n\n\n\n\n\t\n\tdef beautify(self):\n\n\t\tself.ax.set_xlim([-180, 180])\n\t\tself.ax.set_ylim([-90, 90])\n\n\t\tself.fig.tight_layout()\n\n\n\tdef show(self):\n\n\t\tself.beautify()\n\t\tplt.show()\n", "answers": ["            jd = datetime2JD(filenameToDatetime(ff_name) + datetime.timedelta(seconds=float(frame_n)/fps))"], "length": 2640, "dataset": "repobench-p_e", "language": "python", "all_classes": null, "_id": "3a25616cb5ea03ff9da80a9e67105c5d33a5d27f5481b048"}
{"input": "from collections import defaultdict\nfrom tqdm import tqdm\nfrom ..utils import (Bunch,\n                     get_excerpts,\n                     chunk_bounds,\n                     data_chunk,\n                     _concatenate,\n                     )\nfrom ..kwik.mea import (_channels_per_group,\n                        _probe_adjacency_list,\n                        )\nfrom .detect import Thresholder, compute_threshold, FloodFillDetector\nfrom .filter import Filter\nfrom .pca import PCA\nfrom .store import SpikeDetektStore\nfrom .waveform import WaveformExtractor\nimport logging\nimport numpy as np\n        # Create the return arrays.\n        groups = np.array(groups, dtype=np.int32)\n        assert groups.shape == (n_spikes,)\n        assert groups.dtype == np.int32\n\n        samples = np.array(samples, dtype=np.float64)\n        assert samples.shape == (n_spikes,)\n        assert samples.dtype == np.float64\n\n        # These are lists of arrays of various shapes (because of various\n        # groups).\n        waveforms = _array_list(waveforms)\n        assert waveforms.shape == (n_spikes,)\n        assert waveforms.dtype == np.object\n\n        masks = _array_list(masks)\n        assert masks.dtype == np.object\n        assert masks.shape == (n_spikes,)\n\n        # Reorder the spikes.\n        idx = np.argsort(samples)\n        groups = groups[idx]\n        samples = samples[idx]\n        waveforms = waveforms[idx]\n        masks = masks[idx]\n\n        # Remove spikes in the overlapping bands.\n        # WARNING: add s_start to spike_samples, because spike_samples\n        # is relative to the start of the chunk.\n        # It is important to add s_start and not keep_start, because of\n        # edge effects between overlapping chunks.\n        s_start = s_start or 0\n        (keep_start, keep_end) = keep_bounds\n        idx = _keep_spikes(samples + s_start, (keep_start, keep_end))\n\n        # Split the data according to the channel groups.\n        split = _split_spikes(groups, idx=idx, spike_samples=samples,\n                              waveforms=waveforms, masks=masks)\n        # split: {group: {'spike_samples': ..., 'waveforms':, 'masks':}}\n\n        # Assert that spike samples are increasing.\n        for group in split:\n            samples = split[group]['spike_samples']\n            if samples is not None:\n                assert np.all(np.diff(samples) >= 0)\n\n        return split\n\n    def waveform_pcs(self, waveforms, masks):\n        \"\"\"Compute waveform principal components.\n\n        Returns\n        -------\n\n        pcs : array\n            An `(n_features, n_samples, n_channels)` array.\n\n        \"\"\"\n        pca = self._create_pca()\n        if waveforms is None or not len(waveforms):\n            return\n        assert (waveforms.shape[0], waveforms.shape[2]) == masks.shape\n        return pca.fit(waveforms, masks)\n\n    def features(self, waveforms, pcs):\n        \"\"\"Extract features from waveforms.\n\n        Returns\n        -------\n\n        features : array\n            An `(n_spikes, n_channels, n_features)` array.\n\n        \"\"\"\n        pca = self._create_pca()\n        out = pca.transform(waveforms, pcs=pcs)\n        assert out.dtype == np.float32\n        return out\n\n    # Chunking\n    # -------------------------------------------------------------------------\n\n    def iter_chunks(self, n_samples):\n        \"\"\"Iterate over chunks.\"\"\"\n        rate = self._kwargs['sample_rate']\n        chunk_size = int(self._kwargs['chunk_size_seconds'] * rate)\n        overlap = int(self._kwargs['chunk_overlap_seconds'] * rate)\n        for chunk_idx, bounds in enumerate(chunk_bounds(n_samples, chunk_size,\n                                                        overlap=overlap)):\n            yield Bunch(bounds=bounds,\n                        s_start=bounds[0],\n                        s_end=bounds[1],\n                        keep_start=bounds[2],\n                        keep_end=bounds[3],\n                        keep_bounds=(bounds[2:4]),\n                        key=bounds[2],\n                        chunk_idx=chunk_idx,\n                        )\n\n    def n_chunks(self, n_samples):\n        \"\"\"Number of chunks.\"\"\"\n        return len(list(self.iter_chunks(n_samples)))\n\n    def chunk_keys(self, n_samples):\n        return [chunk.key for chunk in self.iter_chunks(n_samples)]\n\n    # Output data\n    # -------------------------------------------------------------------------\n\n    def output_data(self):\n        \"\"\"Bunch of values to be returned by the algorithm.\"\"\"\n        sc = self._store.spike_counts\n        chunk_keys = self._store.chunk_keys\n\n        # NOTE: deal with multiple recordings.\n        samples = self._store.spike_samples()\n\n        s = {}\n        r = {}\n        for group in self._groups:\n", "context": "klusta/kwik/mea.py\ndef _probe_adjacency_list(probe):\n    \"\"\"Return an adjacency list of a whole probe.\"\"\"\n    cgs = probe['channel_groups'].values()\n    graphs = [cg['graph'] for cg in cgs]\n    edges = list(itertools.chain(*graphs))\n    adjacency_list = _edges_to_adjacency_list(edges)\n    return adjacency_list\nklusta/utils.py\ndef _concatenate(arrs):\n    if arrs is None:\n        return\n    arrs = [_as_array(arr) for arr in arrs if arr is not None]\n    if not arrs:\n        return\n    return np.concatenate(arrs, axis=0)\nklusta/kwik/mea.py\ndef _channels_per_group(probe):\n    groups = probe['channel_groups'].keys()\n    return {group: probe['channel_groups'][group]['channels']\n            for group in groups}\nklusta/utils.py\ndef data_chunk(data, chunk, with_overlap=False):\n    \"\"\"Get a data chunk.\"\"\"\n    assert isinstance(chunk, tuple)\n    if len(chunk) == 2:\n        i, j = chunk\n    elif len(chunk) == 4:\n        if with_overlap:\n            i, j = chunk[:2]\n        else:\n            i, j = chunk[2:]\n    else:\n        raise ValueError(\"'chunk' should have 2 or 4 elements, \"\n                         \"not {0:d}\".format(len(chunk)))\n    return data[i:j, ...]\nklusta/traces/detect.py\nclass Thresholder(object):\n    \"\"\"Threshold traces to detect spikes.\n\n    Parameters\n    ----------\n\n    mode : str\n        `'positive'`, `'negative'`, or `'both'`.\n    thresholds : dict\n        A `{str: float}` mapping for multiple thresholds (e.g. `weak`\n        and `strong`).\n\n    Example\n    -------\n\n    ```python\n    thres = Thresholder('positive', thresholds=(.1, .2))\n    crossings = thres(traces)\n    ```\n\n    \"\"\"\n    def __init__(self,\n                 mode=None,\n                 thresholds=None,\n                 ):\n        assert mode in ('positive', 'negative', 'both')\n        if isinstance(thresholds, (float, int, np.ndarray)):\n            thresholds = {'default': thresholds}\n        if thresholds is None:\n            thresholds = {}\n        assert isinstance(thresholds, dict)\n        self._mode = mode\n        self._thresholds = thresholds\n\n    def transform(self, data):\n        \"\"\"Return `data`, `-data`, or `abs(data)` depending on the mode.\"\"\"\n        if self._mode == 'positive':\n            return data\n        elif self._mode == 'negative':\n            return -data\n        elif self._mode == 'both':\n            return np.abs(data)\n\n    def detect(self, data_t, threshold=None):\n        \"\"\"Perform the thresholding operation.\"\"\"\n        # Accept dictionary of thresholds.\n        if isinstance(threshold, (list, tuple)):\n            return {name: self(data_t, threshold=name)\n                    for name in threshold}\n        # Use the only threshold by default (if there is only one).\n        if threshold is None:\n            assert len(self._thresholds) == 1\n            threshold = list(self._thresholds.keys())[0]\n        # Fetch the threshold from its name.\n        if isinstance(threshold, string_types):\n            assert threshold in self._thresholds\n            threshold = self._thresholds[threshold]\n        # threshold = float(threshold)\n        # Threshold the data.\n        return data_t > threshold\n\n    def __call__(self, data, threshold=None):\n        # Transform the data according to the mode.\n        data_t = self.transform(data)\n        return self.detect(data_t, threshold=threshold)\nklusta/traces/store.py\nclass SpikeDetektStore(ArrayStore):\n    \"\"\"Store the following items:\n\n    * filtered\n    * components\n    * spike_samples\n    * features\n    * masks\n\n    \"\"\"\n    def __init__(self, root_dir, groups=None, chunk_keys=None):\n        super(SpikeDetektStore, self).__init__(root_dir)\n        self._groups = groups\n        self._chunk_keys = chunk_keys\n        self._spike_counts = SpikeCounts(groups=groups, chunk_keys=chunk_keys)\n\n    def _rel_path(self, name=None, chunk_key=None, group=None):\n        assert chunk_key >= 0\n        assert group is None or group >= 0\n        assert isinstance(name, string_types)\n        group = group if group is not None else 'all'\n        return 'group_{group}/{name}/chunk_{chunk:d}.npy'.format(\n            chunk=chunk_key, name=name, group=group)\n\n    @property\n    def groups(self):\n        return self._groups\n\n    @property\n    def chunk_keys(self):\n        return self._chunk_keys\n\n    def _iter(self, group=None, name=None):\n        for chunk_key in self.chunk_keys:\n            yield self.load(group=group, chunk_key=chunk_key, name=name)\n\n    def spike_samples(self, group=None):\n        if group is None:\n            return {group: self.spike_samples(group) for group in self._groups}\n        return self.concatenate(self._iter(group=group, name='spike_samples'))\n\n    def features(self, group=None):\n        \"\"\"Yield chunk features.\"\"\"\n        if group is None:\n            return {group: self.features(group) for group in self._groups}\n        return self._iter(group=group, name='features')\n\n    def masks(self, group=None):\n        \"\"\"Yield chunk masks.\"\"\"\n        if group is None:\n            return {group: self.masks(group) for group in self._groups}\n        return self._iter(group=group, name='masks')\n\n    @property\n    def spike_counts(self):\n        return self._spike_counts\n\n    def append(self, group=None, chunk_key=None,\n               spike_samples=None, features=None, masks=None,\n               spike_offset=0):\n        if spike_samples is None or len(spike_samples) == 0:\n            return\n        n = len(spike_samples)\n        assert features.shape[0] == n\n        assert masks.shape[0] == n\n        spike_samples = spike_samples + spike_offset\n        assert np.all(np.diff(spike_samples) >= 0)\n\n        self.store(group=group, chunk_key=chunk_key,\n                   name='features', data=features)\n        self.store(group=group, chunk_key=chunk_key,\n                   name='masks', data=masks)\n        self.store(group=group, chunk_key=chunk_key,\n                   name='spike_samples', data=spike_samples)\n        self._spike_counts.append(group=group, chunk_key=chunk_key, count=n)\n\n    def concatenate(self, arrays):\n        return _concatenate(arrays)\n\n    def delete_all(self, name):\n        \"\"\"Delete all files for a given data name.\"\"\"\n        for group in self._groups:\n            for chunk_key in self._chunk_keys:\n                super(SpikeDetektStore, self).delete(name=name, group=group,\n                                                     chunk_key=chunk_key)\nklusta/utils.py\nclass Bunch(dict):\n    \"\"\"A dict with additional dot syntax.\"\"\"\n    def __init__(self, *args, **kwargs):\n        super(Bunch, self).__init__(*args, **kwargs)\n        self.__dict__ = self\n\n    def copy(self):\n        return Bunch(super(Bunch, self).copy())\nklusta/utils.py\ndef chunk_bounds(n_samples, chunk_size, overlap=0):\n    \"\"\"Return chunk bounds.\n    Chunks have the form:\n        [ overlap/2 | chunk_size-overlap | overlap/2 ]\n        s_start   keep_start           keep_end     s_end\n    Except for the first and last chunks which do not have a left/right\n    overlap.\n    This generator yields (s_start, s_end, keep_start, keep_end).\n    \"\"\"\n    s_start = 0\n    s_end = chunk_size\n    keep_start = s_start\n    keep_end = s_end - overlap // 2\n    yield s_start, s_end, keep_start, keep_end\n\n    while s_end - overlap + chunk_size < n_samples:\n        s_start = s_end - overlap\n        s_end = s_start + chunk_size\n        keep_start = keep_end\n        keep_end = s_end - overlap // 2\n        if s_start < s_end:\n            yield s_start, s_end, keep_start, keep_end\n\n    s_start = s_end - overlap\n    s_end = n_samples\n    keep_start = keep_end\n    keep_end = s_end\n    if s_start < s_end:\n        yield s_start, s_end, keep_start, keep_end\nklusta/traces/filter.py\nclass Filter(object):\n    \"\"\"Multichannel bandpass filter.\n\n    The filter is applied on every column of a 2D array.\n\n    Example\n    -------\n\n    ```python\n    fil = Filter(rate=20000., low=5000., high=15000., order=4)\n    traces_f = fil(traces)\n    ```\n\n    \"\"\"\n    def __init__(self, rate=None, low=None, high=None, order=None):\n        self._filter = bandpass_filter(rate=rate,\n                                       low=low,\n                                       high=high,\n                                       order=order,\n                                       )\n\n    def __call__(self, data):\n        return apply_filter(data, filter=self._filter)\nklusta/traces/detect.py\ndef compute_threshold(arr, single_threshold=True, std_factor=None):\n    \"\"\"Compute the threshold(s) of filtered traces.\n\n    Parameters\n    ----------\n\n    arr : ndarray\n        Filtered traces, shape `(n_samples, n_channels)`.\n    single_threshold : bool\n        Whether there should be a unique threshold for all channels, or\n        one threshold per channel.\n    std_factor : float or 2-tuple\n        The threshold in unit of signal std. Two values can be specified\n        for multiple thresholds (weak and strong).\n\n    Returns\n    -------\n\n    thresholds : ndarray\n        A `(2,)` or `(2, n_channels)` array with the thresholds.\n\n    \"\"\"\n    assert arr.ndim == 2\n    ns, nc = arr.shape\n\n    assert std_factor is not None\n    if isinstance(std_factor, (int, float)):\n        std_factor = (std_factor, std_factor)\n    assert isinstance(std_factor, (tuple, list))\n    assert len(std_factor) == 2\n    std_factor = np.array(std_factor)\n\n    if not single_threshold:\n        std_factor = std_factor[:, None]\n\n    # Get the median of all samples in all excerpts, on all channels.\n    if single_threshold:\n        median = np.median(np.abs(arr))\n    # Or independently for each channel.\n    else:\n        median = np.median(np.abs(arr), axis=0)\n\n    # Compute the threshold from the median.\n    std = median / .6745\n    threshold = std_factor * std\n    assert isinstance(threshold, np.ndarray)\n\n    if single_threshold:\n        assert threshold.ndim == 1\n        assert len(threshold) == 2\n    else:\n        assert threshold.ndim == 2\n        assert threshold.shape == (2, nc)\n    return threshold\nklusta/traces/waveform.py\nclass WaveformExtractor(object):\n    \"\"\"Extract waveforms after data filtering and spike detection.\"\"\"\n    def __init__(self,\n                 extract_before=None,\n                 extract_after=None,\n                 weight_power=None,\n                 thresholds=None,\n                 channels_per_group=None,\n                 ):\n        self._extract_before = extract_before\n        self._extract_after = extract_after\n        self._weight_power = weight_power if weight_power is not None else 1.\n        self._thresholds = thresholds or {}\n        self._channels_per_group = channels_per_group\n        # mapping channel => channels in the shank\n        self._dep_channels = {i: channels\n                              for channels in channels_per_group.values()\n                              for i in channels}\n        self._channel_groups = {i: g\n                                for g, channels in channels_per_group.items()\n                                for i in channels}\n\n    def _component(self, component, data=None, n_samples=None):\n        comp_s = component[:, 0]  # shape: (component_size,)\n        comp_ch = component[:, 1]  # shape: (component_size,)\n        channel = comp_ch[0]\n        if channel not in self._dep_channels:\n            raise RuntimeError(\"Channel `{}` appears to be dead and should \"\n                               \"have been excluded from the threshold \"\n                               \"crossings.\".format(channel))\n        channels = self._dep_channels[channel]\n        group = self._channel_groups[comp_ch[0]]\n\n        # Get the temporal window around the waveform.\n        s_min, s_max = (comp_s.min() - 3), (comp_s.max() + 4)\n        s_min = max(s_min, 0)\n        s_max = min(s_max, n_samples)\n        assert s_min < s_max\n\n        return Bunch(comp_s=comp_s,\n                     comp_ch=comp_ch,\n                     s_min=s_min,\n                     s_max=s_max,\n                     channels=channels,\n                     group=group,\n                     )\n\n    def _normalize(self, x):\n        x = np.asarray(x)\n        tw = self._thresholds['weak']\n        ts = self._thresholds['strong']\n        return np.clip((x - tw) / (ts - tw), 0, 1)\n\n    def _comp_wave(self, data_t, comp):\n        comp_s, comp_ch = comp.comp_s, comp.comp_ch\n        s_min, s_max = comp.s_min, comp.s_max\n        nc = data_t.shape[1]\n        # Data on weak threshold crossings. shape: (some_length, nc)\n        wave = np.zeros((s_max - s_min, nc), dtype=data_t.dtype)\n        # The sample where the peak is reached, on each channel.\n        wave[comp_s - s_min, comp_ch] = data_t[comp_s, comp_ch]\n        return wave\n\n    def masks(self, data_t, wave, comp):\n        nc = data_t.shape[1]\n        channels = comp.channels\n        comp_ch = comp.comp_ch\n        s_min = comp.s_min\n\n        # Binary mask. shape: (nc,)\n        masks_bin = np.zeros(nc, dtype=np.bool)\n        masks_bin[np.unique(comp_ch)] = 1\n\n        # Find the peaks (relative to the start of the chunk). shape: (nc,)\n        peaks = np.argmax(wave, axis=0) + s_min\n        # Peak values on each channel. shape: (nc,)\n        peaks_values = data_t[peaks, np.arange(0, nc)] * masks_bin\n\n        # Compute the float masks.\n        masks_float = self._normalize(peaks_values)\n        # Keep shank channels.\n        masks_float = masks_float[channels]\n        return masks_float\n\n    def spike_sample_aligned(self, wave, comp):\n        s_min, s_max = comp.s_min, comp.s_max\n        # Compute the fractional peak.\n        wave_n = self._normalize(wave)\n        wave_n_p = np.power(wave_n, self._weight_power)\n        u = np.arange(s_max - s_min)[:, np.newaxis]\n        # Spike aligned time relative to the start of the chunk.\n        s_aligned = np.sum(wave_n_p * u) / np.sum(wave_n_p) + s_min\n        return s_aligned\n\n    def extract(self, data, s_aligned, channels=None):\n        s = int(s_aligned)\n        # Get block of given size around peak sample.\n        waveform = _get_padded(data,\n                               s - self._extract_before - 1,\n                               s + self._extract_after + 2)\n        return waveform[:, channels]  # Keep shank channels.\n\n    def align(self, waveform, s_aligned):\n        s = int(s_aligned)\n        sb, sa = self._extract_before, self._extract_after\n        # Perform interpolation around the fractional peak.\n        old_s = np.arange(s - sb - 1, s + sa + 2)\n        new_s = np.arange(s - sb + 0, s + sa + 0) + (s_aligned - s)\n        try:\n            f = interp1d(old_s, waveform, bounds_error=True,\n                         kind='cubic', axis=0)\n        except ValueError:\n            logger.warn(\"Interpolation error at time {0:d}\".format(s))\n            return waveform\n        return f(new_s)\n\n    def set_thresholds(self, **kwargs):\n        self._thresholds.update(kwargs)\n\n    def __call__(self, component=None, data=None, data_t=None):\n        assert data.shape == data_t.shape\n        comp = self._component(component,\n                               data=data,\n                               n_samples=data_t.shape[0],\n                               )\n        channels = comp.channels\n\n        wave = self._comp_wave(data_t, comp)\n        masks = self.masks(data_t, wave, comp)\n        s_aligned = self.spike_sample_aligned(wave, comp)\n\n        waveform_unaligned = self.extract(data, s_aligned, channels=channels)\n        waveform_aligned = self.align(waveform_unaligned, s_aligned)\n\n        assert waveform_aligned.ndim == 2\n        assert masks.ndim == 1\n        assert waveform_aligned.shape[1] == masks.shape[0]\n\n        return comp.group, s_aligned, waveform_aligned, masks\nklusta/utils.py\ndef get_excerpts(data, n_excerpts=None, excerpt_size=None):\n    assert n_excerpts is not None\n    assert excerpt_size is not None\n    if len(data) < n_excerpts * excerpt_size:\n        return data\n    elif n_excerpts == 0:\n        return data[:0]\n    elif n_excerpts == 1:\n        return data[:excerpt_size]\n    out = np.concatenate([data_chunk(data, chunk)\n                          for chunk in excerpts(len(data),\n                                                n_excerpts=n_excerpts,\n                                                excerpt_size=excerpt_size)])\n    assert len(out) <= n_excerpts * excerpt_size\n    return out\nklusta/traces/pca.py\nclass PCA(object):\n    \"\"\"Apply PCA to waveforms.\"\"\"\n    def __init__(self, n_pcs=None):\n        self._n_pcs = n_pcs\n        self._pcs = None\n\n    def fit(self, waveforms, masks=None):\n        \"\"\"Compute the PCs of waveforms.\n\n        Parameters\n        ----------\n\n        waveforms : ndarray\n            Shape: `(n_spikes, n_samples, n_channels)`\n        masks : ndarray\n            Shape: `(n_spikes, n_channels)`\n\n        \"\"\"\n        self._pcs = _compute_pcs(waveforms, n_pcs=self._n_pcs, masks=masks)\n        return self._pcs\n\n    def transform(self, waveforms, pcs=None):\n        \"\"\"Project waveforms on the PCs.\n\n        Parameters\n        ----------\n\n        waveforms : ndarray\n            Shape: `(n_spikes, n_samples, n_channels)`\n\n        \"\"\"\n        if pcs is None:\n            pcs = self._pcs\n        # Need to call fit() if the pcs are None here.\n        if pcs is not None:\n            return _project_pcs(waveforms, pcs)\nklusta/traces/detect.py\nclass FloodFillDetector(object):\n    \"\"\"Detect spikes in weak and strong threshold crossings.\n\n    Parameters\n    ----------\n\n    probe_adjacency_list : dict\n        A dict `{channel: [neighbors]}`.\n    join_size : int\n        The number of samples defining the tolerance in time for\n        finding connected components\n\n    Example\n    -------\n\n    ```python\n    det = FloodFillDetector(probe_adjacency_list=...,\n                            join_size=...)\n    components = det(weak_crossings, strong_crossings)\n    ```\n\n    `components` is a list of `(n, 2)` int arrays with the sample and channel\n    for every sample in the component.\n\n    \"\"\"\n    def __init__(self, probe_adjacency_list=None, join_size=None,\n                 channels_per_group=None):\n        self._adjacency_list = probe_adjacency_list\n        self._join_size = join_size\n        self._channels_per_group = channels_per_group\n\n    def __call__(self, weak_crossings=None, strong_crossings=None):\n        weak_crossings = np.asarray(weak_crossings, np.bool)\n        strong_crossings = np.asarray(strong_crossings, np.bool)\n        all_channels = sorted([item for sublist\n                              in self._channels_per_group.values()\n                              for item in sublist])\n\n        cc = connected_components(weak_crossings=weak_crossings,\n                                  strong_crossings=strong_crossings,\n                                  probe_adjacency_list=self._adjacency_list,\n                                  channels=all_channels,\n                                  join_size=self._join_size,\n                                  )\n        # cc is a list of list of pairs (sample, channel)\n        return [np.array(c) for c in cc]\n", "answers": ["            spikes = _concatenate(samples[group])"], "length": 2166, "dataset": "repobench-p_e", "language": "python", "all_classes": null, "_id": "e8ad768522188255a99ba4610f9077423b54dc653a1b122c"}
{"input": "import numpy as np\nimport nibabel as nib\nimport argparse as ap\nimport matplotlib.pyplot as plt\nfrom tfce_mediation.pyfunc import convert_mni_object, convert_fs, convert_gifti, convert_ply, convert_fslabel, save_waveform, save_stl, save_fs, save_ply, convert_redtoyellow, convert_bluetolightblue, convert_mpl_colormaps, convert_fsannot, convert_voxel\nfrom tfce_mediation.tm_io import savemgh_v2\n\t\thelp=\"Input a gifti surface file (e.g., --i_gifti average.surf.gii)\", \n\t\tnargs=1, \n\t\tmetavar=('*.surf.gii'))\n\tigroup.add_argument(\"-i_mni\", \"--inputmniobj\",\n\t\thelp=\"Input a MNI object file (e.g., --i_mni l_hemi.obj)\", \n\t\tnargs=1, \n\t\tmetavar=('*.obj'))\n\tigroup.add_argument(\"-i_ply\", \"--inputply\",\n\t\thelp=\"Input a MNI object file (e.g., --i_ply l_hemi.ply). Note, vertex colors will be stripped.\", \n\t\tnargs=1, \n\t\tmetavar=('*.ply'))\n\t# voxel to surface conversion\n\tigroup.add_argument(\"-i_voxel\", \"--inputvoxel\",\n\t\thelp=\"Input a voxel (nifti, minc, mgh). e.g. -i_voxel tstat_pFWER_corr.nii.gz\", \n\t\tnargs=1)\n\tap.add_argument(\"-vol\", \"--specifyvolume\",\n\t\thelp=\"Specify volume if -i_voxel is 4D (the first volume is 0)\", \n\t\tnargs=1)\n\tap.add_argument(\"-vt\", \"--voxelthreshold\",\n\t\thelp=\"Apply a threshold to -i_voxel image (zeros everything below value). e.g. -vt 0.95\", \n\t\tnargs=1)\n\tap.add_argument(\"-vb\", \"--voxelbackbone\",\n\t\thelp=\"Add binary mask backbone to the voxel image. e.g. -vb binary_mask_skeleton.nii.gz\", \n\t\tnargs=1)\n\tap.add_argument(\"-vp\", \"--paintvoxelsurface\",\n\t\thelp=\"Must be used with -i_voxel and -o_ply options. Input the sigificance threshold (low and high), and either: red-yellow (r_y), blue-lightblue (b_lb) or any matplotlib colorschemes (https://matplotlib.org/examples/color/colormaps_reference.html). Note, thresholds must be postive. e.g., -vp 0.95 1 r_y\", \n\t\tnargs=3, \n\t\tmetavar=('float','float', 'colormap'))\n\tap.add_argument(\"-ov_mgh\", \"--outputvoxelsurfmgh\",\n\t\thelp=\"Output a mgh file of the surface. This is only useful for visualization purposes.\", \n\t\tnargs=1)\n\n\togroup = ap.add_mutually_exclusive_group(required=True)\n\togroup.add_argument(\"-o_fs\", \"--outputfreesurfer\",\n\t\thelp=\"Output file name for freesurfer surface (e.g., -o_fs lh.32k.midthickness)\", \n\t\tnargs=1, \n\t\tmetavar=('*'))\n\togroup.add_argument(\"-o_obj\", \"--outputwaveform\",\n\t\thelp=\"Output file name for waveform object file for visualization with blender (or any other 3D viewer). This is NOT an MNI object file.\", \n\t\tnargs=1, \n\t\tmetavar=('*'))\n\togroup.add_argument(\"-o_stl\", \"--outputstl\",\n\t\thelp=\"Output file name for STereoLithography (STL) object file for visualization with blender (or any other 3D viewer).\", \n\t\tnargs=1, \n\t\tmetavar=('*'))\n\togroup.add_argument(\"-o_ply\", \"--outputply\",\n\t\thelp=\"Output file name for Polygon File Format (PYL) object file for visualization with blender (or any other 3D viewer).\", \n\t\tnargs=1, \n\t\tmetavar=('*'))\n\n\tap.add_argument(\"-p\", \"--paintsurface\",\n\t\thelp=\"Projects surface file onto a ply mesh for visualization of results using a 3D viewer. Must be used with -o_ply option. Input the surface file (*.mgh), the sigificance threshold (low and high), and either: red-yellow (r_y), blue-lightblue (b_lb) or any matplotlib colorschemes (https://matplotlib.org/examples/color/colormaps_reference.html). Note, thresholds must be postive. e.g., -p image.mgh 0.95 1 r_y\", \n\t\tnargs=4, \n\t\tmetavar=('*.mgh','float','float', 'colormap'))\n\tap.add_argument(\"-s\", \"--paintsecondsurface\",\n\t\thelp=\"Projects a second surface file onto a ply mesh for visualization of resutls using a 3D viewer. Must be used with -o_ply and -p options. Input the surface file (*.mgh), the sigificance threshold (low and high), and either: red-yellow (r_y), blue-lightblue (b_lb) or any matplotlib colorschemes (https://matplotlib.org/examples/color/colormaps_reference.html). Note, thresholds must be postive. e.g., -s negimage.mgh 0.95 1 b_lb\", \n\t\tnargs=4, \n\t\tmetavar=('*.mgh','float','float', 'colormap'))\n\n\tap.add_argument(\"-l\", \"--paintfslabel\",\n\t\thelp=\"Projects freesurface label file onto a ply mesh for visualization of resutls using a 3D viewer. Must be used with -o_ply option. Input the label (*.label or *.label-????) and either: red-yellow (r_y), blue-lightblue (b_lb) or any matplotlib colorschemes (https://matplotlib.org/examples/color/colormaps_reference.html). More than one label can be included. e.g. -l label1.label rainbow label2.label Reds\", \n\t\tnargs='+', \n\t\tmetavar=('*.label colormap'))\n\tap.add_argument(\"-a\", \"--paintfsannot\",\n\t\thelp=\"Projects freesurface annotation file onto a ply mesh for visualization of resutls using a 3D viewer. Must be used with -o_ply option. The legend is outputed\", \n\t\tnargs=1, \n\t\tmetavar=('*.annot'))\n\n\n\treturn ap\n\ndef run(opts):\n\t#input\n\tif opts.inputfreesurfer:\n\t\tv,f = convert_fs(str(opts.inputfreesurfer[0]))\n\tif opts.inputgifti:\n\t\tv,f = convert_gifti(str(opts.inputgifti[0]))\n\tif opts.inputmniobj:\n\t\tv,f = convert_mni_object(str(opts.inputmniobj[0]))\n\tif opts.inputply:\n\t\tv,f,_ = convert_ply(str(opts.inputply[0]))\n\tif opts.inputvoxel:\n\t\timg = nib.load(opts.inputvoxel[0])\n\t\timg_data = img.get_data()\n\t\tif img_data.ndim > 3:\n\t\t\tif opts.specifyvolume:\n\t\t\t\timg_data = img_data[:,:,:,int(opts.specifyvolume[0])]\n\t\t\telse:\n\t\t\t\tprint(\"Error: -i_voxel is a 4D image. Use -vol to specify the volume of interest.\")\n\t\t\t\tquit()\n\t\tif opts.voxelthreshold and opts.voxelbackbone:\n\t\t\tmask = nib.load(opts.voxelbackbone[0]).get_data()\n\t\t\tv,f,values = convert_voxel(img_data, affine = img.affine, threshold = float(opts.voxelthreshold[0]), data_mask = mask)\n\t\telif opts.voxelthreshold:\n\t\t\tv,f,values = convert_voxel(img_data, affine = img.affine, threshold = float(opts.voxelthreshold[0]))\n\t\telif opts.voxelbackbone: \n\t\t\tmask = nib.load(opts.voxelbackbone[0]).get_data()\n\t\t\tv,f,values = convert_voxel(img_data, affine = img.affine, data_mask = mask)\n\t\telse:\n\t\t\tv,f,values = convert_voxel(img_data, affine = img.affine)\n\t#output\n\tif opts.outputfreesurfer:\n\t\tsave_fs(v,f, opts.outputfreesurfer[0])\n\tif opts.outputwaveform:\n\t\tsave_waveform(v,f, opts.outputwaveform[0])\n\tif opts.outputstl:\n\t\tsave_stl(v,f, opts.outputstl[0])\n\tif opts.outputply:\n\t\t# get the matplotlib colormaps\n\t\tcolormaps = np.array(plt.colormaps(),dtype=np.str)\n\t\tif opts.paintsurface:\n\t\t\timg = nib.load(opts.paintsurface[0])\n\t\t\timg_data = img.get_data()\n\t\t\tif img_data.ndim > 3:\n\t\t\t\tprint(\"Error: input file can only contain one subject\")\n\t\t\t\tquit()\n\t\t\timg_data = img_data[:,0,0]\n\t\t\tif (str(opts.paintsurface[3]) == 'r_y') or (str(opts.paintsurface[3]) == 'red-yellow'):\n\t\t\t\tout_color_array = convert_redtoyellow(np.array(( float(opts.paintsurface[1]),float(opts.paintsurface[2]) )), img_data)\n\t\t\telif (str(opts.paintsurface[3]) == 'b_lb') or (str(opts.paintsurface[3]) == 'blue-lightblue'):\n", "context": "tfce_mediation/pyfunc.py\ndef convert_bluetolightblue(threshold, img_data, baseColour=[227,218,201], save_colorbar = True):\n\tcolor_array = np.zeros((img_data.shape[0],3))\n\tcolor_cutoffs = np.linspace(threshold[0],threshold[1],256)\n\tcolored_img_data = np.zeros_like(img_data)\n\tcV=0\n\tfor k in img_data:\n\t\tcolored_img_data[cV] = np.searchsorted(color_cutoffs, k, side=\"left\")\n\t\tcV+=1\n\tcolor_array[:,1]=np.copy(colored_img_data)\n\tcolor_array[:,2]=255\n\tcolor_array[img_data<threshold[0]] = baseColour\n\tcolor_array[img_data>threshold[1]] = [0,255,255]\n\n\tcmap_name = 'blue_lightblue'\n\tcmap_array = np.array(( np.zeros(256), np.linspace(0,255,256), (np.ones(256)*255))).T\n\tblb_cmap = colors.ListedColormap(cmap_array/255)\n\tif save_colorbar:\n\t\twrite_colorbar(threshold, blb_cmap, cmap_name, 'png')\n\t\tplt.clf()\n\treturn color_array\ntfce_mediation/pyfunc.py\ndef save_waveform(v,f, outname):\n\tif not outname.endswith('obj'):\n\t\toutname += '.obj'\n\toutname=check_outname(outname)\n\twith open(outname, \"a\") as o:\n\t\tfor i in range(len(v)):\n\t\t\to.write(\"v %1.6f %1.6f %1.6f\\n\" % (v[i,0],v[i,1], v[i,2]) )\n\t\tfor j in range(len(f)):\n\t\t\to.write(\"f %d %d %d\\n\" % (f[j,0],f[j,1], f[j,2]) )\n\t\to.close()\ntfce_mediation/pyfunc.py\ndef convert_ply(name_ply):\n\telement = []\n\tsize = []\n\tvertex_info = []\n\tvertex_dtype = []\n\tface_dtype = []\n\tface_info = []\n\tvertex_property = 0\n\tface_property = 0\n\tply_ascii = False\n\n\tobj = open(name_ply)\n\treader = obj.readline().strip().split()\n\tfirstword = reader[0]\n\n\t# READ HEADER\n\twhile firstword != 'end_header':\n\t\treader = obj.readline().strip().split()\n\t\tfirstword = reader[0]\n\t\tif firstword == 'format':\n\t\t\tply_format = reader[1]\n\t\t\tif ply_format == 'binary_little_endian':\n\t\t\t\tply_format = '<'\n\t\t\telif ply_format == 'binary_big_endian':\n\t\t\t\tply_format = '>'\n\t\t\telse:\n\t\t\t\tply_ascii = True\n\t\tif firstword == 'element':\n\t\t\telement.append((reader[1]))\n\t\t\tsize.append((reader[2]))\n\t\t\tif reader[1] == 'vertex':\n\t\t\t\tvertex_property = 1\n\t\t\telse:\n\t\t\t\tvertex_property = 0\n\t\t\tif reader[1] == 'face':\n\t\t\t\tface_property = 1\n\t\t\telse:\n\t\t\t\tface_property = 0\n\t\tif reader[0] == 'property':\n\t\t\tif vertex_property == 1:\n\t\t\t\tvertex_dtype.append((reader[1]))\n\t\t\t\tvertex_info.append((reader[2]))\n\t\t\telif face_property == 1:\n\t\t\t\tface_dtype.append((reader[2]))\n\t\t\t\tface_dtype.append((reader[3]))\n\t\t\t\tface_info.append((reader[4]))\n\t\t\telse:\n\t\t\t\tprint(\"Unknown property\")\n\n\t# READ ELEMENTS\n\tfor e in range(len(element)):\n\t\t# VERTEX DATA\n\t\tif element[e] == 'vertex':\n\t\t\tv = np.zeros((int(size[e]), 3), dtype=np.float32)\n\t\t\tc = np.zeros((int(size[e]), 3), dtype=np.uint8)\n\t\t\tif ply_ascii:\n\t\t\t\tfor i in range(int(size[e])):\n\t\t\t\t\treader = obj.readline().strip().split()\n\t\t\t\t\tv[i, 0] = np.array(reader[0]).astype(np.float)\n\t\t\t\t\tv[i, 1] = np.array(reader[1]).astype(np.float)\n\t\t\t\t\tv[i, 2] = np.array(reader[2]).astype(np.float)\n\t\t\t\t\tif len(vertex_info) == 6:\n\t\t\t\t\t\tc[i, 0] = np.array(reader[3]).astype(np.uint8)\n\t\t\t\t\t\tc[i, 1] = np.array(reader[4]).astype(np.uint8)\n\t\t\t\t\t\tc[i, 2] = np.array(reader[5]).astype(np.uint8)\n\t\t\telse:\n\t\t\t\tstruct_fmt = ply_format\n\t\t\t\tfor i in range(len(vertex_dtype)):\n\t\t\t\t\tif vertex_dtype[i] == 'float':\n\t\t\t\t\t\tstruct_fmt += 'f'\n\t\t\t\t\tif vertex_dtype[i] == 'uchar':\n\t\t\t\t\t\tstruct_fmt += 'B'\n\t\t\t\t\tif vertex_dtype[i] == 'int':\n\t\t\t\t\t\tstruct_fmt += 'i'\n\t\t\t\tstruct_len = struct.calcsize(struct_fmt)\n\t\t\t\tstruct_unpack = struct.Struct(struct_fmt).unpack_from\n\t\t\t\tvcounter = 0\n\t\t\t\twhile vcounter != int(size[e]):\n\t\t\t\t\tif len(vertex_dtype) > 3:\n\t\t\t\t\t\ts = struct_unpack(obj.read(struct_len))\n\t\t\t\t\t\tv[vcounter] = s[:3]\n\t\t\t\t\t\tc[vcounter] = s[3:]\n\t\t\t\t\t\tvcounter += 1\n\t\t\t\t\telse:\n\t\t\t\t\t\ts = struct_unpack(obj.read(struct_len))\n\t\t\t\t\t\tv[vcounter] = s[:3]\n\t\t\t\t\t\tvcounter += 1\n\t\t# FACE DATA\n\t\tif element[e] == 'face':\n\t\t\tif ply_ascii:\n\t\t\t\treader = obj.readline().strip().split()\n\t\t\t\tnumf = int(reader[0])\n\t\t\t\tf = np.zeros((int(size[e]), numf), dtype=np.int32)\n\t\t\t\tf[0] = reader[1:]\n\t\t\t\tfcounter = 1\n\t\t\t\twhile fcounter != int(size[e]):\n\t\t\t\t\treader = obj.readline().strip().split()\n\t\t\t\t\tf[fcounter] = reader[1:]\n\t\t\t\t\tfcounter += 1\n\t\t\telse:\n\t\t\t\tif face_dtype[0] == 'uchar':\n\t\t\t\t\tfcounter = 0\n\t\t\t\t\twhile fcounter != int(size[e]):\n\t\t\t\t\t\tstruct_unpack = struct.Struct(ply_format + 'B').unpack_from\n\t\t\t\t\t\tnumf = struct_unpack(obj.read(1))[0]\n\t\t\t\t\t\t# creates empty face array if it doesn't exists\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tf\n\t\t\t\t\t\texcept NameError:\n\t\t\t\t\t\t\tf = np.zeros((int(size[e]), numf), dtype=np.int32)\n\t\t\t\t\t\tstruct_fmt = ply_format\n\t\t\t\t\t\tfor i in range(int(numf)):\n\t\t\t\t\t\t\tstruct_fmt += 'i'\n\t\t\t\t\t\tstruct_len = struct.calcsize(struct_fmt)\n\t\t\t\t\t\tstruct_unpack = struct.Struct(struct_fmt).unpack_from\n\t\t\t\t\t\ts = struct_unpack(obj.read(struct_len))\n\t\t\t\t\t\tf[fcounter] = s\n\t\t\t\t\t\tfcounter += 1\n\treturn (v, f, c)\ntfce_mediation/pyfunc.py\ndef convert_fsannot(annot_name):\n\tlabels, ctab, names  = nib.freesurfer.read_annot(annot_name)\n\tlabels[labels==-1] = 0 # why is this necessary?????\n\tcolor_array = ctab[labels]\n\tcolor_array = color_array[:,:3]\n\twrite_annot_legend(ctab, names, annot_name)\n\treturn color_array\ntfce_mediation/pyfunc.py\ndef save_stl(v,f, outname):\n\tif not outname.endswith('stl'):\n\t\toutname += '.stl'\n\toutname=check_outname(outname)\n\tv = np.array(v, dtype=np.float32, order = \"C\")\n\tf = np.array(f, dtype=np.int32, order = \"C\")\n\ttris = v[f]\n\tn = np.cross( tris[::,1 ] - tris[::,0]  , tris[::,2 ] - tris[::,0] )\n\tn = normalize_v3(n)\n\twith open(outname, \"a\") as o:\n\t\to.write(\"solid surface\\n\")\n\t\tfor i in range(tris.shape[0]):\n\t\t\to.write(\"facet normal %1.6f %1.6f %1.6f\\n\"% (n[i,0],n[i,0],n[i,0]))\n\t\t\to.write(\"outer loop\\n\")\n\t\t\to.write(\"vertex %1.6f %1.6f %1.6f\\n\" % (tris[i,0,0],tris[i,0,1],tris[i,0,2]))\n\t\t\to.write(\"vertex %1.6f %1.6f %1.6f\\n\" % (tris[i,1,0],tris[i,1,1],tris[i,1,2]))\n\t\t\to.write(\"vertex %1.6f %1.6f %1.6f\\n\" % (tris[i,2,0],tris[i,2,1],tris[i,2,2]))\n\t\t\to.write(\"endloop\\n\")\n\t\t\to.write(\"endfacet\\n\")\n\t\to.write(\"endfacet\\n\")\n\t\to.close()\ntfce_mediation/pyfunc.py\ndef convert_voxel(img_data, affine = None, threshold = None, data_mask = None, absthreshold = None):\n\t\"\"\"\n\tConverts a voxel image to a surface including outputs voxel values to paint vertex surface.\n\t\n\tParameters\n\t----------\n\timg_data : array\n\t\timage array\n\taffine : array\n\t\t affine [4x4] to convert vertices values to native space (Default = None)\n\tdata_mask : array\n\t\tuse a mask to create a surface backbone (Default = None)\n\tthreshold : float\n\t\tthreshold for output of voxels (Default = None)\n\tabsthreshold : float\n\t\tthreshold for output of abs(voxels) (Default = None)\n\t\t\n\tReturns\n\t-------\n\t\tv : array\n\t\t\tvertices\n\t\tf : array\n\t\t\tfaces\n\t\tvalues : array\n\t\t\tscalar values\n\t\n\t\"\"\"\n\ttry:\n\t\tfrom skimage import measure\n\texcept:\n\t\tprint(\"Error skimage is required\")\n\t\tquit()\n\n\tif threshold is not None:\n\t\tprint(\"Zeroing data less than threshold = %1.2f\" % threshold)\n\t\timg_data[img_data<threshold] = 0\n\tif absthreshold is not None:\n\t\tprint(\"Zeroing absolute values less than threshold = %1.2f\" % absthreshold)\n\t\timg_data[np.abs(img_data)<absthreshold] = 0\n\tif data_mask is not None:\n\t\tprint(\"Including mask\")\n\t\tdata_mask *= .1\n\t\tdata_mask[img_data!=0] = img_data[img_data!=0]\n\t\tdel img_data\n\t\timg_data = np.copy(data_mask)\n\ttry:\n\t\tv, f, _, values = measure.marching_cubes_lewiner(img_data)\n\t\tif affine is not None:\n\t\t\tprint(\"Applying affine transformation\")\n\t\t\tv = nib.affines.apply_affine(affine,v)\n\texcept:\n\t\tprint(\"No voxels above threshold\")\n\t\tv = f = values = []\n\treturn v, f, values\ntfce_mediation/pyfunc.py\ndef convert_gifti(gifti_surface):\n\timg = nib.load(gifti_surface)\n\tv, f = img.darrays[0].data, img.darrays[1].data\n\treturn v, f\ntfce_mediation/pyfunc.py\ndef convert_fslabel(name_fslabel):\n\tobj = open(name_fslabel)\n\treader = obj.readline().strip().split()\n\treader = np.array(obj.readline().strip().split())\n\tif reader.ndim == 1:\n\t\tnum_vertex = reader[0].astype(np.int)\n\telse:\n\t\tprint('Error reading header')\n\tv_id = np.zeros((num_vertex)).astype(np.int)\n\tv_ras = np.zeros((num_vertex,3)).astype(np.float)\n\tv_value = np.zeros((num_vertex)).astype(np.float)\n\tfor i in range(num_vertex):\n\t\treader = obj.readline().strip().split()\n\t\tv_id[i] = np.array(reader[0]).astype(np.int)\n\t\tv_ras[i] = np.array(reader[1:4]).astype(np.float)\n\t\tv_value[i] = np.array(reader[4]).astype(np.float)\n\treturn (v_id, v_ras, v_value)\ntfce_mediation/pyfunc.py\ndef save_ply(v, f, outname, color_array=None, output_binary=True):\n\t# check file extension\n\tif not outname.endswith('ply'):\n\t\tif output_binary:\n\t\t\toutname += '.ply'\n\t\telse:\n\t\t\toutname += '.ascii.ply'\n\toutname=check_outname(outname)\n\n\t# write header \n\theader = (\"ply\\n\")\n\tif output_binary:\n\t\theader += (\"format binary_%s_endian 1.0\\n\" % (sys.byteorder))\n\t\tif sys.byteorder == 'little':\n\t\t\toutput_fmt = '<'\n\t\telse:\n\t\t\toutput_fmt = '>'\n\telse:\n\t\theader += (\"format ascii 1.0\\n\")\n\theader += (\"comment made with TFCE_mediation\\n\")\n\theader += (\"element vertex %d\\n\" % len(v))\n\theader += (\"property float x\\n\")\n\theader += (\"property float y\\n\")\n\theader += (\"property float z\\n\")\n\tif color_array is not None:\n\t\theader += (\"property uchar red\\n\")\n\t\theader += (\"property uchar green\\n\")\n\t\theader += (\"property uchar blue\\n\")\n\theader += (\"element face %d\\n\" % len(f))\n\theader += (\"property list uchar int vertex_index\\n\")\n\theader += (\"end_header\\n\")\n\n\t# write to file\n\twith open(outname, \"a\") as o:\n\t\to.write(header)\n\t\tfor i in range(len(v)):\n\t\t\tif output_binary:\n\t\t\t\tif color_array is not None:\n\t\t\t\t\to.write(\n\t\t\t\t\t\tstruct.pack(output_fmt + 'fffBBB', v[i, 0], v[i, 1], v[i, 2], color_array[i, 0], color_array[i, 1], color_array[i, 2]))\n\t\t\t\telse:\n\t\t\t\t\to.write(struct.pack(output_fmt + 'fff', v[i, 0], v[i, 1], v[i, 2]))\n\t\t\telse:\n\t\t\t\tif color_array is not None:\n\t\t\t\t\to.write(\"%1.6f %1.6f %1.6f %d %d %d\\n\" % (v[i, 0], v[i, 1], v[i, 2], color_array[i, 0], color_array[i, 1], color_array[i, 2]))\n\t\t\t\telse:\n\t\t\t\t\to.write(\"%1.6f %1.6f %1.6f\\n\" % (v[i, 0], v[i, 1], v[i, 2]))\n\t\tfor j in range(len(f)):\n\t\t\tif output_binary:\n\t\t\t\to.write(struct.pack('<Biii', 3, f[j, 0], f[j, 1], f[j, 2]))\n\t\t\telse:\n\t\t\t\to.write(\"3 %d %d %d\\n\" % (f[j, 0], f[j, 1], f[j, 2]))\ntfce_mediation/pyfunc.py\ndef convert_fs(fs_surface):\n\tv, f = nib.freesurfer.read_geometry(fs_surface)\n\treturn v, f\ntfce_mediation/pyfunc.py\ndef convert_mpl_colormaps(threshold,img_data, cmapName, baseColour=[227,218,201], save_colorbar = True):\n\tcmapFunc = plt.get_cmap(str(cmapName))\n\tcolor_array = np.zeros((img_data.shape[0],3))\n\tcolor_cutoffs = np.linspace(threshold[0],threshold[1],256)\n\tcV=0\n\tfor k in img_data:\n\t\ttemp_ = np.array(cmapFunc(np.searchsorted(color_cutoffs, k, side=\"left\")))*255\n\t\tcolor_array[cV,:] = ((np.around(temp_[0]), np.around(temp_[1]), np.around(temp_[2])))\n\t\tcV+=1\n\tcolor_array[img_data<threshold[0]] = baseColour\n\ttemp_ = np.array(cmapFunc(np.searchsorted(color_cutoffs, color_cutoffs[255], side=\"left\")))*255 # safer\n\tcolor_array[img_data>=threshold[1]] = ((int(temp_[0]), int(temp_[1]), int(temp_[2])))\n\tif save_colorbar:\n\t\twrite_colorbar(threshold, cmapFunc, cmapName, 'png')\n\t\tplt.clf()\n\treturn color_array\ntfce_mediation/tm_io.py\ndef savemgh_v2(image_array, index, imagename, affine=None):\n\tif not imagename.endswith('mgh'):\n\t\timagename += '.mgh'\n\toutdata = image_array.astype(np.float32, order = \"C\")\n\tif image_array.ndim == 1:\n\t\timgout = np.zeros((index.shape[0],index.shape[1],index.shape[2]))\n\t\timgout[index]=outdata\n\telif image_array.shape[1] > 1:\n\t\timgout = np.zeros((index.shape[0],index.shape[1],index.shape[2],image_array.shape[1]))\n\t\timgout[index]=outdata\n\telse:\n\t\timgout = np.zeros((index.shape[0],index.shape[1],index.shape[2]))\n\t\timgout[index]=outdata[:,0]\n\tnib.save(nib.freesurfer.mghformat.MGHImage(imgout.astype(np.float32, order = \"C\"),affine=affine),imagename)\ntfce_mediation/pyfunc.py\ndef convert_redtoyellow(threshold,img_data, baseColour=[227,218,201], save_colorbar = True):\n\tcolor_array = np.zeros((img_data.shape[0],3))\n\tcolor_cutoffs = np.linspace(threshold[0],threshold[1],256)\n\tcolored_img_data = np.zeros_like(img_data)\n\tcV=0\n\tfor k in img_data:\n\t\tcolored_img_data[cV] = np.searchsorted(color_cutoffs, k, side=\"left\")\n\t\tcV+=1\n\tcolor_array[:,0]=255\n\tcolor_array[:,1]=np.copy(colored_img_data)\n\tcolor_array[img_data<threshold[0]] = baseColour\n\tcolor_array[img_data>threshold[1]] = [255,255,0]\n\n\tcmap_name = 'red_yellow'\n\tcmap_array = np.array(( (np.ones(256)*255), np.linspace(0,255,256), np.zeros(256))).T\n\trl_cmap = colors.ListedColormap(cmap_array/255)\n\tif save_colorbar:\n\t\twrite_colorbar(threshold, rl_cmap, cmap_name, 'png')\n\t\tplt.clf()\n\treturn color_array\ntfce_mediation/pyfunc.py\ndef convert_mni_object(obj_file):\n\t# adapted from Jon Pipitone's script https://gist.github.com/pipitone/8687804\n\tobj = open(obj_file)\n\t_, _, _, _, _, _, numpoints = obj.readline().strip().split()\n\tnumpoints = int(numpoints)\n\tvertices=[]\n\tnormals=[]\n\ttriangles=[]\n\n\tfor i in range(numpoints):\n\t\tx, y, z = list(map(float,obj.readline().strip().split())) \n\t\tvertices.append((x, y, z))\n\tassert obj.readline().strip() == \"\"\n\t# numpoints normals as (x,y,z)\n\tfor i in range(numpoints):\n\t\tnormals.append(tuple(map(float,obj.readline().strip().split())))\n\n\tassert obj.readline().strip() == \"\"\n\tnt=int(obj.readline().strip().split()[0]) # number of triangles\n\t_, _, _, _, _ = obj.readline().strip().split()\n\tassert obj.readline().strip() == \"\"\n\t# rest of the file is a list of numbers\n\tpoints = list(map(int, \"\".join(obj.readlines()).strip().split()))\n\tpoints = points[nt:]\t# ignore these.. (whatever they are)\n\tfor i in range(nt): \n\t\ttriangles.append((points.pop(0), points.pop(0), points.pop(0)))\n\treturn np.array(vertices), np.array(triangles)\ntfce_mediation/pyfunc.py\ndef save_fs(v,f, outname):\n\tif not outname.endswith('srf'):\n\t\toutname += '.srf'\n\toutname=check_outname(outname)\n\tnib.freesurfer.io.write_geometry(outname, v, f)\n", "answers": ["\t\t\t\tout_color_array = convert_bluetolightblue(np.array(( float(opts.paintsurface[1]),float(opts.paintsurface[2]) )), img_data)"], "length": 2032, "dataset": "repobench-p_e", "language": "python", "all_classes": null, "_id": "131139402cad5bfc8744b35012d307b9976737a536cf7a0c"}
{"input": "import os.path\nimport time\nimport logging\nfrom shutil import copyfileobj\nfrom requests.structures import CaseInsensitiveDict\nfrom dateutil.parser import parse\nfrom slugify import slugify\nfrom blinkpy import api\nfrom blinkpy.sync_module import BlinkSyncModule, BlinkOwl, BlinkLotus\nfrom blinkpy.helpers import util\nfrom blinkpy.helpers.constants import (\n    DEFAULT_MOTION_INTERVAL,\n    DEFAULT_REFRESH,\n    MIN_THROTTLE_TIME,\n    TIMEOUT_MEDIA,\n)\nfrom blinkpy.helpers.constants import __version__\nfrom blinkpy.auth import Auth, TokenRefreshFailed, LoginError\n# -*- coding: utf-8 -*-\n\"\"\"\nblinkpy is an unofficial api for the Blink security camera system.\n\nrepo url: https://github.com/fronzbot/blinkpy\n\nOriginal protocol hacking by MattTW :\nhttps://github.com/MattTW/BlinkMonitorProtocol\n\nPublished under the MIT license - See LICENSE file for more details.\n\"Blink Wire-Free HS Home Monitoring & Alert Systems\" is a trademark\nowned by Immedia Inc., see www.blinkforhome.com for more information.\nblinkpy is in no way affiliated with Blink, nor Immedia Inc.\n\"\"\"\n\n\n\n\n_LOGGER = logging.getLogger(__name__)\n\n\nclass Blink:\n    \"\"\"Class to initialize communication.\"\"\"\n\n    def __init__(\n        self,\n        refresh_rate=DEFAULT_REFRESH,\n        motion_interval=DEFAULT_MOTION_INTERVAL,\n        no_owls=False,\n    ):\n        \"\"\"\n        Initialize Blink system.\n\n        :param refresh_rate: Refresh rate of blink information.\n                             Defaults to 15 (seconds)\n        :param motion_interval: How far back to register motion in minutes.\n                                Defaults to last refresh time.\n                                Useful for preventing motion_detected property\n                                from de-asserting too quickly.\n        :param no_owls: Disable searching for owl entries (blink mini cameras only known entity).  Prevents an uneccessary API call if you don't have these in your network.\n        \"\"\"\n", "context": "blinkpy/auth.py\nclass TokenRefreshFailed(Exception):\n    \"\"\"Class to throw failed refresh exception.\"\"\"\nblinkpy/helpers/constants.py\nDEFAULT_MOTION_INTERVAL = 1\nblinkpy/auth.py\nclass LoginError(Exception):\n    \"\"\"Class to throw failed login exception.\"\"\"\nblinkpy/helpers/constants.py\nMIN_THROTTLE_TIME = 2\nblinkpy/helpers/util.py\n_LOGGER = logging.getLogger(__name__)\ndef json_load(file_name):\ndef json_save(data, file_name):\ndef gen_uid(size, uid_format=False):\ndef time_to_seconds(timestamp):\ndef get_time(time_to_convert=None):\ndef merge_dicts(dict_a, dict_b):\ndef prompt_login_data(data):\ndef validate_login_data(data):\n    def __init__(self, errcode):\n    def __init__(self, region_id):\n    def __init__(self, seconds=10):\n    def __call__(self, method):\n        def throttle_method():\n        def wrapper(*args, **kwargs):\nclass BlinkException(Exception):\nclass BlinkAuthenticationException(BlinkException):\nclass BlinkURLHandler:\nclass Throttle:\nblinkpy/helpers/constants.py\nDEFAULT_REFRESH = 30\nblinkpy/helpers/constants.py\nTIMEOUT_MEDIA = 90\nblinkpy/sync_module.py\nclass BlinkSyncModule:\n    \"\"\"Class to initialize sync module.\"\"\"\n\n    def __init__(self, blink, network_name, network_id, camera_list):\n        \"\"\"\n        Initialize Blink sync module.\n\n        :param blink: Blink class instantiation\n        \"\"\"\n        self.blink = blink\n        self.network_id = network_id\n        self.region_id = blink.auth.region_id\n        self.name = network_name\n        self.serial = None\n        self.status = \"offline\"\n        self.sync_id = None\n        self.host = None\n        self.summary = None\n        self.network_info = None\n        self.events = []\n        self.cameras = CaseInsensitiveDict({})\n        self.motion_interval = blink.motion_interval\n        self.motion = {}\n        self.last_record = {}\n        self.camera_list = camera_list\n        self.available = False\n\n    @property\n    def attributes(self):\n        \"\"\"Return sync attributes.\"\"\"\n        attr = {\n            \"name\": self.name,\n            \"id\": self.sync_id,\n            \"network_id\": self.network_id,\n            \"serial\": self.serial,\n            \"status\": self.status,\n            \"region_id\": self.region_id,\n        }\n        return attr\n\n    @property\n    def urls(self):\n        \"\"\"Return device urls.\"\"\"\n        return self.blink.urls\n\n    @property\n    def online(self):\n        \"\"\"Return boolean system online status.\"\"\"\n        try:\n            return ONLINE[self.status]\n        except KeyError:\n            _LOGGER.error(\"Unknown sync module status %s\", self.status)\n            self.available = False\n            return False\n\n    @property\n    def arm(self):\n        \"\"\"Return status of sync module: armed/disarmed.\"\"\"\n        try:\n            return self.network_info[\"network\"][\"armed\"]\n        except (KeyError, TypeError):\n            self.available = False\n            return None\n\n    @arm.setter\n    def arm(self, value):\n        \"\"\"Arm or disarm camera.\"\"\"\n        if value:\n            return api.request_system_arm(self.blink, self.network_id)\n        return api.request_system_disarm(self.blink, self.network_id)\n\n    def start(self):\n        \"\"\"Initialize the system.\"\"\"\n        response = self.sync_initialize()\n        if not response:\n            return False\n\n        try:\n            self.sync_id = self.summary[\"id\"]\n            self.serial = self.summary[\"serial\"]\n            self.status = self.summary[\"status\"]\n        except KeyError:\n            _LOGGER.error(\"Could not extract some sync module info: %s\", response)\n\n        is_ok = self.get_network_info()\n        self.check_new_videos()\n\n        if not is_ok or not self.update_cameras():\n            return False\n        self.available = True\n        return True\n\n    def sync_initialize(self):\n        \"\"\"Initialize a sync module.\"\"\"\n        response = api.request_syncmodule(self.blink, self.network_id)\n        try:\n            self.summary = response[\"syncmodule\"]\n            self.network_id = self.summary[\"network_id\"]\n        except (TypeError, KeyError):\n            _LOGGER.error(\n                \"Could not retrieve sync module information with response: %s\", response\n            )\n            return False\n        return response\n\n    def update_cameras(self, camera_type=BlinkCamera):\n        \"\"\"Update cameras from server.\"\"\"\n        try:\n            for camera_config in self.camera_list:\n                if \"name\" not in camera_config:\n                    break\n                blink_camera_type = camera_config.get(\"type\", \"\")\n                name = camera_config[\"name\"]\n                self.motion[name] = False\n                owl_info = self.get_owl_info(name)\n                lotus_info = self.get_lotus_info(name)\n                if blink_camera_type == \"mini\":\n                    camera_type = BlinkCameraMini\n                if blink_camera_type == \"lotus\":\n                    camera_type = BlinkDoorbell\n                self.cameras[name] = camera_type(self)\n                camera_info = self.get_camera_info(\n                    camera_config[\"id\"], owl_info=owl_info, lotus_info=lotus_info\n                )\n                self.cameras[name].update(camera_info, force_cache=True, force=True)\n\n        except KeyError:\n            _LOGGER.error(\"Could not create camera instances for %s\", self.name)\n            return False\n        return True\n\n    def get_owl_info(self, name):\n        \"\"\"Extract owl information.\"\"\"\n        try:\n            for owl in self.blink.homescreen[\"owls\"]:\n                if owl[\"name\"] == name:\n                    return owl\n        except (TypeError, KeyError):\n            pass\n        return None\n\n    def get_lotus_info(self, name):\n        \"\"\"Extract lotus information.\"\"\"\n        try:\n            for doorbell in self.blink.homescreen[\"doorbells\"]:\n                if doorbell[\"name\"] == name:\n                    return doorbell\n        except (TypeError, KeyError):\n            pass\n        return None\n\n    def get_events(self, **kwargs):\n        \"\"\"Retrieve events from server.\"\"\"\n        force = kwargs.pop(\"force\", False)\n        response = api.request_sync_events(self.blink, self.network_id, force=force)\n        try:\n            return response[\"event\"]\n        except (TypeError, KeyError):\n            _LOGGER.error(\"Could not extract events: %s\", response)\n            return False\n\n    def get_camera_info(self, camera_id, **kwargs):\n        \"\"\"Retrieve camera information.\"\"\"\n        owl = kwargs.get(\"owl_info\", None)\n        if owl is not None:\n            return owl\n        lotus = kwargs.get(\"lotus_info\", None)\n        if lotus is not None:\n            return lotus\n        response = api.request_camera_info(self.blink, self.network_id, camera_id)\n        try:\n            return response[\"camera\"][0]\n        except (TypeError, KeyError):\n            _LOGGER.error(\"Could not extract camera info: %s\", response)\n            return {}\n\n    def get_network_info(self):\n        \"\"\"Retrieve network status.\"\"\"\n        self.network_info = api.request_network_update(self.blink, self.network_id)\n        try:\n            if self.network_info[\"network\"][\"sync_module_error\"]:\n                raise KeyError\n        except (TypeError, KeyError):\n            self.available = False\n            return False\n        return True\n\n    def refresh(self, force_cache=False):\n        \"\"\"Get all blink cameras and pulls their most recent status.\"\"\"\n        if not self.get_network_info():\n            return\n        self.check_new_videos()\n        for camera_name in self.cameras.keys():\n            camera_id = self.cameras[camera_name].camera_id\n            camera_info = self.get_camera_info(\n                camera_id,\n                owl_info=self.get_owl_info(camera_name),\n                lotus_info=self.get_lotus_info(camera_name),\n            )\n            self.cameras[camera_name].update(camera_info, force_cache=force_cache)\n        self.available = True\n\n    def check_new_videos(self):\n        \"\"\"Check if new videos since last refresh.\"\"\"\n        try:\n            interval = self.blink.last_refresh - self.motion_interval * 60\n        except TypeError:\n            # This is the first start, so refresh hasn't happened yet.\n            # No need to check for motion.\n            return False\n\n        resp = api.request_videos(self.blink, time=interval, page=1)\n\n        for camera in self.cameras.keys():\n            self.motion[camera] = False\n\n        try:\n            info = resp[\"media\"]\n        except (KeyError, TypeError):\n            _LOGGER.warning(\"Could not check for motion. Response: %s\", resp)\n            return False\n\n        for entry in info:\n            try:\n                name = entry[\"device_name\"]\n                clip = entry[\"media\"]\n                timestamp = entry[\"created_at\"]\n                if self.check_new_video_time(timestamp):\n                    self.motion[name] = True and self.arm\n                    self.last_record[name] = {\"clip\": clip, \"time\": timestamp}\n            except KeyError:\n                _LOGGER.debug(\"No new videos since last refresh.\")\n\n        return True\n\n    def check_new_video_time(self, timestamp):\n        \"\"\"Check if video has timestamp since last refresh.\"\"\"\n        return time_to_seconds(timestamp) > self.blink.last_refresh\nblinkpy/api.py\n_LOGGER = logging.getLogger(__name__)\nMIN_THROTTLE_TIME = 5\ndef request_login(\n    auth, url, login_data, is_retry=False,\n):\ndef request_verify(auth, blink, verify_key):\ndef request_logout(blink):\ndef request_networks(blink):\ndef request_network_update(blink, network):\ndef request_user(blink):\ndef request_network_status(blink, network):\ndef request_syncmodule(blink, network):\ndef request_system_arm(blink, network):\ndef request_system_disarm(blink, network):\ndef request_command_status(blink, network, command_id):\ndef request_homescreen(blink):\ndef request_sync_events(blink, network):\ndef request_new_image(blink, network, camera_id):\ndef request_new_video(blink, network, camera_id):\ndef request_video_count(blink):\ndef request_videos(blink, time=None, page=0):\ndef request_cameras(blink, network):\ndef request_camera_info(blink, network, camera_id):\ndef request_camera_usage(blink):\ndef request_camera_liveview(blink, network, camera_id):\ndef request_camera_sensors(blink, network, camera_id):\ndef request_motion_detection_enable(blink, network, camera_id):\ndef request_motion_detection_disable(blink, network, camera_id):\ndef http_get(blink, url, stream=False, json=True, is_retry=False, timeout=TIMEOUT):\ndef http_post(blink, url, is_retry=False, data=None, json=True, timeout=TIMEOUT):\nblinkpy/auth.py\nclass Auth:\n    \"\"\"Class to handle login communication.\"\"\"\n\n    def __init__(self, login_data=None, no_prompt=False):\n        \"\"\"\n        Initialize auth handler.\n\n        :param login_data: dictionary for login data\n                           must contain the following:\n                             - username\n                             - password\n        :param no_prompt: Should any user input prompts\n                          be supressed? True/FALSE\n        \"\"\"\n        if login_data is None:\n            login_data = {}\n        self.data = login_data\n        self.token = login_data.get(\"token\", None)\n        self.host = login_data.get(\"host\", None)\n        self.region_id = login_data.get(\"region_id\", None)\n        self.client_id = login_data.get(\"client_id\", None)\n        self.account_id = login_data.get(\"account_id\", None)\n        self.login_response = None\n        self.is_errored = False\n        self.no_prompt = no_prompt\n        self.session = self.create_session()\n\n    @property\n    def login_attributes(self):\n        \"\"\"Return a dictionary of login attributes.\"\"\"\n        self.data[\"token\"] = self.token\n        self.data[\"host\"] = self.host\n        self.data[\"region_id\"] = self.region_id\n        self.data[\"client_id\"] = self.client_id\n        self.data[\"account_id\"] = self.account_id\n        return self.data\n\n    @property\n    def header(self):\n        \"\"\"Return authorization header.\"\"\"\n        if self.token is None:\n            return None\n        return {\n            \"TOKEN_AUTH\": self.token,\n            \"user-agent\": DEFAULT_USER_AGENT,\n            \"content-type\": \"application/json\",\n        }\n\n    def create_session(self, opts=None):\n        \"\"\"Create a session for blink communication.\"\"\"\n        if opts is None:\n            opts = {}\n        backoff = opts.get(\"backoff\", 1)\n        retries = opts.get(\"retries\", 3)\n        retry_list = opts.get(\"retry_list\", [429, 500, 502, 503, 504])\n        sess = Session()\n        assert_status_hook = [\n            lambda response, *args, **kwargs: response.raise_for_status()\n        ]\n        sess.hooks[\"response\"] = assert_status_hook\n        retry = Retry(\n            total=retries, backoff_factor=backoff, status_forcelist=retry_list\n        )\n        adapter = HTTPAdapter(max_retries=retry)\n        sess.mount(\"https://\", adapter)\n        sess.mount(\"http://\", adapter)\n        sess.get = partial(sess.get, timeout=TIMEOUT)\n        return sess\n\n    def prepare_request(self, url, headers, data, reqtype):\n        \"\"\"Prepare a request.\"\"\"\n        req = Request(reqtype.upper(), url, headers=headers, data=data)\n        return req.prepare()\n\n    def validate_login(self):\n        \"\"\"Check login information and prompt if not available.\"\"\"\n        self.data[\"username\"] = self.data.get(\"username\", None)\n        self.data[\"password\"] = self.data.get(\"password\", None)\n        if not self.no_prompt:\n            self.data = util.prompt_login_data(self.data)\n\n        self.data = util.validate_login_data(self.data)\n\n    def login(self, login_url=LOGIN_ENDPOINT):\n        \"\"\"Attempt login to blink servers.\"\"\"\n        self.validate_login()\n        _LOGGER.info(\"Attempting login with %s\", login_url)\n        response = api.request_login(self, login_url, self.data, is_retry=False,)\n        try:\n            if response.status_code == 200:\n                return response.json()\n            raise LoginError\n        except AttributeError as error:\n            raise LoginError from error\n\n    def logout(self, blink):\n        \"\"\"Log out.\"\"\"\n        return api.request_logout(blink)\n\n    def refresh_token(self):\n        \"\"\"Refresh auth token.\"\"\"\n        self.is_errored = True\n        try:\n            _LOGGER.info(\"Token expired, attempting automatic refresh.\")\n            self.login_response = self.login()\n            self.extract_login_info()\n            self.is_errored = False\n        except LoginError as error:\n            _LOGGER.error(\"Login endpoint failed. Try again later.\")\n            raise TokenRefreshFailed from error\n        except (TypeError, KeyError) as error:\n            _LOGGER.error(\"Malformed login response: %s\", self.login_response)\n            raise TokenRefreshFailed from error\n        return True\n\n    def extract_login_info(self):\n        \"\"\"Extract login info from login response.\"\"\"\n        self.region_id = self.login_response[\"account\"][\"tier\"]\n        self.host = f\"{self.region_id}.{BLINK_URL}\"\n        self.token = self.login_response[\"auth\"][\"token\"]\n        self.client_id = self.login_response[\"account\"][\"client_id\"]\n        self.account_id = self.login_response[\"account\"][\"account_id\"]\n\n    def startup(self):\n        \"\"\"Initialize tokens for communication.\"\"\"\n        self.validate_login()\n        if None in self.login_attributes.values():\n            self.refresh_token()\n\n    def validate_response(self, response, json_resp):\n        \"\"\"Check for valid response.\"\"\"\n        if not json_resp:\n            self.is_errored = False\n            return response\n        self.is_errored = True\n        try:\n            if response.status_code in [101, 401]:\n                raise UnauthorizedError\n            if response.status_code == 404:\n                raise exceptions.ConnectionError\n            json_data = response.json()\n        except KeyError:\n            pass\n        except (AttributeError, ValueError) as error:\n            raise BlinkBadResponse from error\n\n        self.is_errored = False\n        return json_data\n\n    def query(\n        self,\n        url=None,\n        data=None,\n        headers=None,\n        reqtype=\"get\",\n        stream=False,\n        json_resp=True,\n        is_retry=False,\n        timeout=TIMEOUT,\n    ):\n        \"\"\"\n        Perform server requests.\n\n        :param url: URL to perform request\n        :param data: Data to send\n        :param headers: Headers to send\n        :param reqtype: Can be 'get' or 'post' (default: 'get')\n        :param stream: Stream response? True/FALSE\n        :param json_resp: Return JSON response? TRUE/False\n        :param is_retry: Is this part of a re-auth attempt? True/FALSE\n        \"\"\"\n        req = self.prepare_request(url, headers, data, reqtype)\n        try:\n            response = self.session.send(req, stream=stream, timeout=timeout)\n            return self.validate_response(response, json_resp)\n        except (exceptions.ConnectionError, exceptions.Timeout):\n            _LOGGER.error(\n                \"Connection error. Endpoint %s possibly down or throttled.\", url,\n            )\n        except BlinkBadResponse:\n            code = None\n            reason = None\n            try:\n                code = response.status_code\n                reason = response.reason\n            except AttributeError:\n                pass\n            _LOGGER.error(\n                \"Expected json response from %s, but received: %s: %s\",\n                url,\n                code,\n                reason,\n            )\n        except UnauthorizedError:\n            try:\n                if not is_retry:\n                    self.refresh_token()\n                    return self.query(\n                        url=url,\n                        data=data,\n                        headers=self.header,\n                        reqtype=reqtype,\n                        stream=stream,\n                        json_resp=json_resp,\n                        is_retry=True,\n                        timeout=timeout,\n                    )\n                _LOGGER.error(\"Unable to access %s after token refresh.\", url)\n            except TokenRefreshFailed:\n                _LOGGER.error(\"Unable to refresh token.\")\n        return None\n\n    def send_auth_key(self, blink, key):\n        \"\"\"Send 2FA key to blink servers.\"\"\"\n        if key is not None:\n            response = api.request_verify(self, blink, key)\n            try:\n                json_resp = response.json()\n                blink.available = json_resp[\"valid\"]\n                if not json_resp[\"valid\"]:\n                    _LOGGER.error(\"%s\", json_resp[\"message\"])\n                    return False\n            except (KeyError, TypeError):\n                _LOGGER.error(\"Did not receive valid response from server.\")\n                return False\n        return True\n\n    def check_key_required(self):\n        \"\"\"Check if 2FA key is required.\"\"\"\n        try:\n            if self.login_response[\"account\"][\"client_verification_required\"]:\n                return True\n        except (KeyError, TypeError):\n            pass\n        return False\nblinkpy/sync_module.py\nclass BlinkOwl(BlinkSyncModule):\n    \"\"\"Representation of a sync-less device.\"\"\"\n\n    def __init__(self, blink, name, network_id, response):\n        \"\"\"Initialize a sync-less object.\"\"\"\n        cameras = [{\"name\": name, \"id\": response[\"id\"]}]\n        super().__init__(blink, name, network_id, cameras)\n        self.sync_id = response[\"id\"]\n        self.serial = response[\"serial\"]\n        self.status = response[\"enabled\"]\n        if not self.serial:\n            self.serial = f\"{network_id}-{self.sync_id}\"\n\n    def sync_initialize(self):\n        \"\"\"Initialize a sync-less module.\"\"\"\n        self.summary = {\n            \"id\": self.sync_id,\n            \"name\": self.name,\n            \"serial\": self.serial,\n            \"status\": self.status,\n            \"onboarded\": True,\n            \"account_id\": self.blink.account_id,\n            \"network_id\": self.network_id,\n        }\n        return self.summary\n\n    def update_cameras(self, camera_type=BlinkCameraMini):\n        \"\"\"Update sync-less cameras.\"\"\"\n        return super().update_cameras(camera_type=BlinkCameraMini)\n\n    def get_camera_info(self, camera_id, **kwargs):\n        \"\"\"Retrieve camera information.\"\"\"\n        try:\n            for owl in self.blink.homescreen[\"owls\"]:\n                if owl[\"name\"] == self.name:\n                    self.status = owl[\"enabled\"]\n                    return owl\n        except (TypeError, KeyError):\n            pass\n        return None\n\n    def get_network_info(self):\n        \"\"\"Get network info for sync-less module.\"\"\"\n        return True\n\n    @property\n    def network_info(self):\n        \"\"\"Format owl response to resemble sync module.\"\"\"\n        return {\n            \"network\": {\n                \"id\": self.network_id,\n                \"name\": self.name,\n                \"armed\": self.status,\n                \"sync_module_error\": False,\n                \"account_id\": self.blink.account_id,\n            }\n        }\n\n    @network_info.setter\n    def network_info(self, value):\n        \"\"\"Set network_info property.\"\"\"\nblinkpy/helpers/constants.py\nMAJOR_VERSION = 0\nMINOR_VERSION = 19\nPATCH_VERSION = \"0.rc0\"\nREQUIRED_PYTHON_VER = (3, 6, 0)\nPROJECT_NAME = \"blinkpy\"\nPROJECT_PACKAGE_NAME = \"blinkpy\"\nPROJECT_LICENSE = \"MIT\"\nPROJECT_AUTHOR = \"Kevin Fronczak\"\nPROJECT_COPYRIGHT = f\" 2017, {PROJECT_AUTHOR}\"\nPROJECT_URL = \"https://github.com/fronzbot/blinkpy\"\nPROJECT_EMAIL = \"kfronczak@gmail.com\"\nPROJECT_DESCRIPTION = \"A Blink camera Python library \" \"running on Python 3.\"\nPROJECT_LONG_DESCRIPTION = (\n    \"blinkpy is an open-source \"\n    \"unofficial API for the Blink Camera \"\n    \"system with the intention for easy \"\n    \"integration into various home \"\n    \"automation platforms.\"\n)\n    PROJECT_LONG_DESCRIPTION = open(\"README.rst\").read()\nPROJECT_CLASSIFIERS = [\n    \"Intended Audience :: Developers\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Operating System :: OS Independent\",\n    \"Programming Language :: Python :: 3.6\",\n    \"Programming Language :: Python :: 3.7\",\n    \"Programming Language :: Python :: 3.8\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Topic :: Home Automation\",\n]\nPROJECT_GITHUB_USERNAME = \"fronzbot\"\nPROJECT_GITHUB_REPOSITORY = \"blinkpy\"\nPYPI_URL = f\"https://pypi.python.org/pypi/{PROJECT_PACKAGE_NAME}\"\nBLINK_URL = \"immedia-semi.com\"\nDEFAULT_URL = f\"rest-prod.{BLINK_URL}\"\nBASE_URL = f\"https://{DEFAULT_URL}\"\nLOGIN_ENDPOINT = f\"{BASE_URL}/api/v5/account/login\"\nONLINE = {\"online\": True, \"offline\": False}\nDEFAULT_USER_AGENT = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\"\nDEVICE_ID = \"Blinkpy\"\nTIMESTAMP_FORMAT = \"%Y-%m-%dT%H:%M:%S%z\"\nDEFAULT_MOTION_INTERVAL = 1\nDEFAULT_REFRESH = 30\nMIN_THROTTLE_TIME = 2\nSIZE_NOTIFICATION_KEY = 152\nSIZE_UID = 16\nTIMEOUT = 10\nTIMEOUT_MEDIA = 90\nblinkpy/sync_module.py\nclass BlinkLotus(BlinkSyncModule):\n    \"\"\"Representation of a sync-less device.\"\"\"\n\n    def __init__(self, blink, name, network_id, response):\n        \"\"\"Initialize a sync-less object.\"\"\"\n        cameras = [{\"name\": name, \"id\": response[\"id\"]}]\n        super().__init__(blink, name, network_id, cameras)\n        self.sync_id = response[\"id\"]\n        self.serial = response[\"serial\"]\n        self.status = response[\"enabled\"]\n        if not self.serial:\n            self.serial = f\"{network_id}-{self.sync_id}\"\n\n    def sync_initialize(self):\n        \"\"\"Initialize a sync-less module.\"\"\"\n        self.summary = {\n            \"id\": self.sync_id,\n            \"name\": self.name,\n            \"serial\": self.serial,\n            \"status\": self.status,\n            \"onboarded\": True,\n            \"account_id\": self.blink.account_id,\n            \"network_id\": self.network_id,\n        }\n        return self.summary\n\n    def update_cameras(self, camera_type=BlinkDoorbell):\n        \"\"\"Update sync-less cameras.\"\"\"\n        return super().update_cameras(camera_type=BlinkDoorbell)\n\n    def get_camera_info(self, camera_id, **kwargs):\n        \"\"\"Retrieve camera information.\"\"\"\n        try:\n            for doorbell in self.blink.homescreen[\"doorbells\"]:\n                if doorbell[\"name\"] == self.name:\n                    self.status = doorbell[\"enabled\"]\n                    return doorbell\n        except (TypeError, KeyError):\n            pass\n        return None\n\n    def get_network_info(self):\n        \"\"\"Get network info for sync-less module.\"\"\"\n        return True\n\n    @property\n    def network_info(self):\n        \"\"\"Format lotus response to resemble sync module.\"\"\"\n        return {\n            \"network\": {\n                \"id\": self.network_id,\n                \"name\": self.name,\n                \"armed\": self.status,\n                \"sync_module_error\": False,\n                \"account_id\": self.blink.account_id,\n            }\n        }\n\n    @network_info.setter\n    def network_info(self, value):\n        \"\"\"Set network_info property.\"\"\"\n", "answers": ["        self.auth = Auth()"], "length": 2174, "dataset": "repobench-p_e", "language": "python", "all_classes": null, "_id": "b974e05782cc4c69c850e9554fcfe817a8cc14530563f3b4"}
{"input": "from itertools import chain\nfrom contextlib import contextmanager\nfrom collections import namedtuple, deque, defaultdict\nfrom .refs import ArgRef, FieldRef, ItemRef, Reference, is_from_arg, CtxRef\nfrom .nodes import Tuple, Number, Keyword, String, List, Symbol, Placeholder\nfrom .nodes import NodeVisitor, NodeTransformer\nfrom .types import IntType, NamedArgMeta, StringType, ListType, VarArgsMeta\nfrom .types import TypeVarMeta, TypeVar, Func, NamedArg, Record, TypeRefMeta\nfrom .types import RecordMeta, BoolType, Union, ListTypeMeta, DictTypeMeta\nfrom .types import TypingMeta, UnionMeta, Nothing, Option, VarArgs, FuncMeta\nfrom .types import TypeTransformer, Markup, VarNamedArgs, VarNamedArgsMeta\nfrom .types import MarkupMeta\nfrom .utils import VarsGen\nfrom .errors import Errors, UserError\nfrom .compat import zip_longest\nfrom .constant import HTML_ELEMENTS\n\n\n\nclass SyntaxError(UserError):\n    pass\n\n\nclass TypeCheckError(UserError):\n    pass\n\n\nclass SignatureMismatch(UserError):\n    pass\n\n\nclass Environ(object):\n\n    def __init__(self, defs=None, errors=None):\n        self.defs = defs or {}\n        self.vars = deque([{}])\n", "context": "kinko/refs.py\nclass ArgRef(Reference):\n\n    def __init__(self, name):\n        super(ArgRef, self).__init__(None)\n        self.name = name\n\n    def __repr__(self):\n        return \"#{}\".format(self.name)\nkinko/errors.py\nclass UserError(Exception):\n    pass\nkinko/refs.py\nclass CtxRef(Reference):\n\n    def __init__(self, name):\n        super(CtxRef, self).__init__(None)\n        self.name = name\n\n    def __repr__(self):\n        return 'ctx[{!r}]'.format(self.name)\nkinko/nodes.py\nclass Number(Node):\n\n    def __init__(self, value, **kw):\n        self.value = value\n        super(Number, self).__init__(**kw)\n\n    def __repr__(self):\n        return repr(self.value)\n\n    def accept(self, visitor):\n        return visitor.visit_number(self)\nkinko/types.py\nclass TypeVar(with_metaclass(TypeVarMeta, object)):\n    pass\nkinko/utils.py\nclass VarsGen(object):\n\n    def __init__(self):\n        self.vars = {}\n\n    def __getattr__(self, name):\n        if name not in self.vars:\n            self.vars[name] = TypeVar[None]\n        return self.vars[name]\nkinko/types.py\nclass StringType(with_metaclass(StringTypeMeta, object)):\n    pass\nkinko/types.py\nclass RecordMeta(TypingMeta):\n\n    def __cls_init__(cls, items):\n        cls.__items__ = dict(items)\n\n    def __repr__(cls):\n        return '{}{{{}}}'.format(\n            cls.__name__,\n            ' '.join(':{} {!r}'.format(*i) for i in cls.__items__.items()),\n        )\n\n    def accept(cls, visitor):\n        return visitor.visit_record(cls)\nkinko/types.py\nclass DictTypeMeta(TypingMeta):\n\n    def __cls_init__(cls, params):\n        cls.__key_type__, cls.__value_type__ = params\n\n    def __repr__(cls):\n        return '{{:{!r} {!r}}}'.format(cls.__key_type__, cls.__value_type__)\n\n    def accept(cls, visitor):\n        return visitor.visit_dict(cls)\nkinko/types.py\nclass MarkupMeta(GenericMeta):\n\n    def __repr__(cls):\n        return 'markup'\n\n    def accept(cls, visitor):\n        return visitor.visit_markup(cls)\nkinko/refs.py\ndef is_from_arg(ref):\n    return isinstance(get_origin(ref), ArgRef)\nkinko/types.py\nclass NamedArgMeta(TypingMeta):\n\n    def __cls_init__(cls, params):\n        cls.__arg_name__, cls.__arg_type__ = params\n\n    def __repr__(cls):\n        return ':{} {!r}'.format(cls.__arg_name__, cls.__arg_type__)\n\n    def accept(cls, visitor):\n        return visitor.visit_namedarg(cls)\nkinko/types.py\nclass NamedArg(with_metaclass(NamedArgMeta, object)):\n    pass\nkinko/types.py\nclass TypeTransformer(object):\n\n    def visit(self, type_):\n        return type_.accept(self)\n\n    def visit_bool(self, type_):\n        return type_\n\n    def visit_nothing(self, type_):\n        return type_\n\n    def visit_string(self, type_):\n        return type_\n\n    def visit_int(self, type_):\n        return type_\n\n    def visit_markup(self, type_):\n        return type_\n\n    def visit_typevar(self, type_):\n        return TypeVar[self.visit(type_.__instance__)\n                       if type_.__instance__ is not None else None]\n\n    def visit_typeref(self, type_):\n        return TypeRef[type_.__ref_name__]\n\n    def visit_union(self, type_):\n        return Union[(self.visit(t) for t in type_.__types__)]\n\n    def visit_option(self, type_):\n        t = (type_.__types__ - {Nothing}).pop()\n        return Option[self.visit(t)]\n\n    def visit_func(self, type_):\n        return Func[[self.visit(t) for t in type_.__args__],\n                    self.visit(type_.__result__)]\n\n    def visit_varargs(self, type_):\n        return VarArgs[self.visit(type_.__arg_type__)]\n\n    def visit_namedarg(self, type_):\n        return NamedArg[type_.__arg_name__, self.visit(type_.__arg_type__)]\n\n    def visit_varnamedargs(self, type_):\n        return VarNamedArgs[self.visit(type_.__arg_type__)]\n\n    def visit_list(self, type_):\n        return ListType[self.visit(type_.__item_type__)]\n\n    def visit_dict(self, type_):\n        return DictType[self.visit(type_.__key_type__),\n                        self.visit(type_.__value_type__)]\n\n    def visit_record(self, type_):\n        return Record[{key: self.visit(value)\n                       for key, value in type_.__items__.items()}]\nkinko/refs.py\nclass ItemRef(Reference):\n\n    def __repr__(self):\n        return '{!r} > []'.format(self.backref)\nkinko/compat.py\nPY3 = sys.version_info[0] == 3\nPY35 = sys.version_info >= (3, 5)\ndef with_metaclass(meta, *bases):\n        def __new__(cls, name, this_bases, d):\n    def _exec_in(source, globals_dict):\n    def _exec_in(source, globals_dict):\n    class metaclass(meta):\nkinko/types.py\nclass VarArgs(with_metaclass(VarArgsMeta, object)):\n    pass\nkinko/constant.py\nHTML_ELEMENTS = frozenset((\n    # root\n    'html',\n\n    # metadata\n    'head', 'title', 'base', 'link', 'meta', 'style',\n\n    # sections\n    'body', 'article', 'section', 'nav', 'aside', 'h1', 'h2', 'h3', 'h4', 'h5',\n    'h6', 'hgroup', 'headers', 'footer', 'address',\n\n    # grouping content\n    'p', 'hr', 'pre', 'blockquote', 'ol', 'ul', 'li', 'dl', 'dt', 'dd',\n    'figure', 'figcaption', 'main', 'div',\n\n    # text-level semantics\n    'a', 'em', 'strong', 'small', 's', 'cite', 'q', 'dfn', 'abbr', 'ruby', 'rt',\n    'rp', 'data', 'time', 'code', 'var', 'samp', 'kbd', 'sub', 'sup', 'i', 'b',\n    'u', 'mark', 'bdi', 'bdo', 'span', 'br', 'wbr',\n\n    # links\n    'a', 'area',\n\n    # edits\n    'ins', 'del',\n\n    # embedded content\n    'picture', 'source', 'img', 'iframe', 'embed', 'object', 'param', 'video',\n    'audio', 'track', 'map',\n\n    # tabular data\n    'table', 'caption', 'colgroup', 'col', 'tbody', 'thead', 'tfoot', 'tr',\n    'td', 'th',\n\n    # forms\n    'form', 'label', 'input', 'button', 'select', 'datalist', 'optgroup',\n    'option', 'textarea', 'keygen', 'output', 'progress', 'meter', 'fieldset',\n    'legend',\n\n    # interactive elements\n    'details', 'summary', 'menu', 'menuitem', 'dialog',\n\n    # scripting\n    'script', 'noscript', 'template', 'canvas',\n))\nkinko/types.py\nclass TypingMeta(GenericMeta):\n\n    def __cls_init__(cls, *args):\n        raise NotImplementedError\n\n    def __getitem__(cls, parameters):\n        type_ = cls.__class__(cls.__name__, cls.__bases__, dict(cls.__dict__))\n        type_.__cls_init__(parameters)\n        return type_\nkinko/types.py\nclass VarArgsMeta(TypingMeta):\n\n    def __cls_init__(cls, arg_type):\n        cls.__arg_type__ = arg_type\n\n    def __repr__(cls):\n        return '*{!r}'.format(cls.__arg_type__)\n\n    def accept(cls, visitor):\n        return visitor.visit_varargs(cls)\nkinko/types.py\nclass Nothing(with_metaclass(NothingMeta, object)):\n    pass\nkinko/types.py\nclass UnionMeta(TypingMeta):\n\n    def __cls_init__(cls, types):\n        cls.__types__ = set(types)\n\n    def __repr__(cls):\n        return '|'.join(map(repr, cls.__types__))\n\n    def accept(cls, visitor):\n        return visitor.visit_union(cls)\nkinko/types.py\nclass ListTypeMeta(TypingMeta):\n\n    def __cls_init__(cls, item_type):\n        cls.__item_type__ = item_type\n\n    def __repr__(cls):\n        return '[{!r}]'.format(cls.__item_type__)\n\n    def accept(cls, visitor):\n        return visitor.visit_list(cls)\nkinko/nodes.py\nclass Placeholder(Node):\n\n    def __init__(self, name, **kw):\n        self.name = name\n        super(Placeholder, self).__init__(**kw)\n\n    def __repr__(self):\n        return '#{}'.format(self.name)\n\n    def accept(self, visitor):\n        return visitor.visit_placeholder(self)\nkinko/types.py\nclass TypeRefMeta(TypingMeta):\n    __ref__ = None\n\n    def __cls_init__(cls, name):\n        cls.__ref_name__ = name\n\n    def __repr__(cls):\n        if cls.__ref__ is None:\n            return '<{}[-]>'.format(cls.__ref_name__)\n        else:\n            return '<{}[*]>'.format(cls.__ref_name__)\n\n    def accept(cls, visitor):\n        return visitor.visit_typeref(cls)\nkinko/types.py\nclass Record(with_metaclass(RecordMeta, object)):\n    pass\nkinko/nodes.py\nclass NodeVisitor(object):\n\n    def visit(self, node):\n        node.accept(self)\n\n    def visit_tuple(self, node):\n        for value in node.values:\n            self.visit(value)\n\n    def visit_list(self, node):\n        for value in node.values:\n            self.visit(value)\n\n    def visit_dict(self, node):\n        for value in node.values:\n            self.visit(value)\n\n    def visit_symbol(self, node):\n        pass\n\n    def visit_keyword(self, node):\n        pass\n\n    def visit_placeholder(self, node):\n        pass\n\n    def visit_number(self, node):\n        pass\n\n    def visit_string(self, node):\n        pass\nkinko/types.py\nclass Union(with_metaclass(UnionMeta, object)):\n    pass\nkinko/types.py\nclass IntType(with_metaclass(IntTypeMeta, object)):\n    pass\nkinko/types.py\nclass Markup(with_metaclass(MarkupMeta, object)):\n    pass\nkinko/types.py\nclass BoolType(with_metaclass(BoolTypeMeta, object)):\n    pass\nkinko/types.py\nclass ListType(with_metaclass(ListTypeMeta, object)):\n    pass\nkinko/errors.py\nclass Errors(object):\n\n    def __init__(self):\n        self.list = []\n        self._stack = [None]\n\n    @contextmanager\n    def module_ctx(self, module):\n        self._stack.append(Func(module, None))\n        try:\n            yield\n        finally:\n            self._stack.pop()\n\n    @contextmanager\n    def func_ctx(self, module, name):\n        self._stack.append(Func(module, name))\n        try:\n            yield\n        finally:\n            self._stack.pop()\n\n    @contextmanager\n    def location(self, location):\n        try:\n            yield\n        except UserError as e:\n            self.error(location, text_type(e))\n            raise\n\n    def warn(self, location, message):\n        self.list.append(Error(self._stack[-1], location, message, WARNING))\n\n    def error(self, location, message):\n        self.list.append(Error(self._stack[-1], location, message, ERROR))\nkinko/nodes.py\nclass NodeTransformer(object):\n\n    def visit(self, node):\n        return node.accept(self)\n\n    def visit_tuple(self, node):\n        return node.clone_with(self.visit(i) for i in node.values)\n\n    def visit_list(self, node):\n        return node.clone_with(self.visit(i) for i in node.values)\n\n    def visit_dict(self, node):\n        return node.clone_with(self.visit(i) for i in node.values)\n\n    def visit_symbol(self, node):\n        return node.clone()\n\n    def visit_keyword(self, node):\n        return node.clone()\n\n    def visit_placeholder(self, node):\n        return node.clone()\n\n    def visit_number(self, node):\n        return node.clone()\n\n    def visit_string(self, node):\n        return node.clone()\nkinko/types.py\nclass VarNamedArgs(with_metaclass(VarNamedArgsMeta, object)):\n    pass\nkinko/types.py\nclass VarNamedArgsMeta(TypingMeta):\n\n    def __cls_init__(cls, arg_type):\n        cls.__arg_type__ = arg_type\n\n    def __repr__(cls):\n        return '**{!r}'.format(cls.__arg_type__)\n\n    def accept(cls, visitor):\n        return visitor.visit_varnamedargs(cls)\nkinko/types.py\nclass TypeVarMeta(TypingMeta):\n    __backref__ = None\n\n    def __cls_init__(cls, instance):\n        cls.__instance__ = instance\n\n    def __repr__(cls):\n        return '<{}:{}>'.format(\n            hex(id(cls))[-3:].upper(),\n            repr(cls.__instance__) if cls.__instance__ is not None else '?',\n        )\n\n    def accept(cls, visitor):\n        return visitor.visit_typevar(cls)\nkinko/types.py\nclass FuncMeta(TypingMeta):\n\n    def __cls_init__(cls, params):\n        cls.__args__, cls.__result__ = params\n\n    def __repr__(cls):\n        return '({} -> {!r})'.format(\n            ' '.join(map(repr, cls.__args__)),\n            cls.__result__,\n        )\n\n    def accept(cls, visitor):\n        return visitor.visit_func(cls)\nkinko/nodes.py\nclass String(Node):\n\n    def __init__(self, value, **kw):\n        self.value = text_type(value)\n        super(String, self).__init__(**kw)\n\n    def __repr__(self):\n        return encode_basestring_ascii(self.value)\n\n    def accept(self, visitor):\n        return visitor.visit_string(self)\nkinko/nodes.py\nclass List(Node):\n\n    def __init__(self, values, **kw):\n        self.values = tuple(values)\n        super(List, self).__init__(**kw)\n\n    def __repr__(self):\n        return '[{}]'.format(' '.join(map(repr, self.values)))\n\n    def accept(self, visitor):\n        return visitor.visit_list(self)\nkinko/refs.py\nclass FieldRef(Reference):\n\n    def __init__(self, backref, name):\n        super(FieldRef, self).__init__(backref)\n        self.name = name\n\n    def __repr__(self):\n        return '{!r} > [{!r}]'.format(self.backref, self.name)\nkinko/nodes.py\nclass Tuple(Node):\n\n    def __init__(self, values, **kw):\n        self.values = tuple(values)\n        super(Tuple, self).__init__(**kw)\n\n    def __repr__(self):\n        return '({})'.format(' '.join(map(repr, self.values)))\n\n    def accept(self, visitor):\n        return visitor.visit_tuple(self)\nkinko/nodes.py\nclass Symbol(Node):\n\n    def __init__(self, name, **kw):\n        self.name = name\n        head, sep, tail = name.partition('/')\n        if sep:\n            self.ns, self.rel = head, tail\n        else:\n            self.ns, self.rel = None, name\n        super(Symbol, self).__init__(**kw)\n\n    def __repr__(self):\n        return self.name\n\n    def accept(self, visitor):\n        return visitor.visit_symbol(self)\nkinko/types.py\nclass Option(with_metaclass(OptionMeta, object)):\n    pass\nkinko/nodes.py\nclass Keyword(Node):\n\n    def __init__(self, name, **kw):\n        self.name = name\n        super(Keyword, self).__init__(**kw)\n\n    def __repr__(self):\n        return ':{}'.format(self.name)\n\n    def accept(self, visitor):\n        return visitor.visit_keyword(self)\nkinko/types.py\nclass Func(with_metaclass(FuncMeta, object)):\n    pass\nkinko/refs.py\nclass Reference(object):\n\n    def __init__(self, backref):\n        self.backref = backref\n", "answers": ["        self._root = TypeVar[None]"], "length": 1186, "dataset": "repobench-p_e", "language": "python", "all_classes": null, "_id": "96d3a6a8d00b1872187fb51f60ee407ddf78fa068d259332"}
{"input": "import static org.junit.Assert.assertThrows;\nimport com.google.common.base.Splitter;\nimport com.google.common.collect.Lists;\nimport com.google.gson.JsonObject;\nimport com.google.gson.JsonParser;\nimport java.security.KeyFactory;\nimport java.security.interfaces.RSAPrivateKey;\nimport java.security.spec.EncodedKeySpec;\nimport java.security.spec.PKCS8EncodedKeySpec;\nimport java.time.Duration;\nimport java.time.Instant;\nimport java.util.List;\nimport java.util.regex.Pattern;\nimport junit.framework.TestCase;\nimport net.oauth.jsontoken.crypto.HmacSHA256Verifier;\nimport net.oauth.jsontoken.crypto.SignatureAlgorithm;\nimport net.oauth.jsontoken.crypto.Verifier;\nimport net.oauth.jsontoken.discovery.DefaultPublicKeyLocator;\nimport net.oauth.jsontoken.discovery.IdentityServerDescriptorProvider;\nimport net.oauth.jsontoken.discovery.JsonServerInfo;\nimport net.oauth.jsontoken.discovery.VerifierProvider;\nimport net.oauth.jsontoken.discovery.VerifierProviders;\nimport net.oauth.jsontoken.exceptions.ErrorCode;\nimport net.oauth.jsontoken.exceptions.InvalidJsonTokenException;\nimport org.apache.commons.codec.binary.Base64;\nimport org.junit.function.ThrowingRunnable;\n/*\n * Copyright 2010 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage net.oauth.jsontoken;\n\n\n\npublic abstract class JsonTokenTestBase extends TestCase {\n\n  protected static final byte[] SYMMETRIC_KEY = \"kjdhasdkjhaskdjhaskdjhaskdjh\".getBytes();\n\n  protected static final String PRIVATE_KEY =\n      \"MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC6nMEXFuxTnM5+yM4Afngybf5Z\"\n          + \"89JxlchBA3Ni//Gm1/25MetzfId2Jg8NkthmRDzH6sFaoNS7n6Z6JyNJFszb2PXKBkZdem219F5k\"\n          + \"jawoHrfA1Lu8fBmGQYG/aG70aPft2eEZbY+XqW5WUlMk7vFW7BDikwBXyv/5rrFasBfPWd13xozQ\"\n          + \"9612IErWGlGMgxmB64jcTbGWMzDgzE/scSmyeQ0vQQMW8J+Nnb/yDpY7loXrVrAgZx8IBv1f9Fv3\"\n          + \"p7tirTD/vFgzxE2rIAauM/aU8zBHEyXL1NSNq0I62OAF4DLiDlcEFOvYjqoiCPQIh0NXnQy8Dcs5\"\n          + \"xHCj0e1b3X/LAgMBAAECggEBAJ9G5iQQA7xF7ZYXTITtbSgV6+/ZBTi/kEG1cUoBjL9MQZpNOlrC\"\n          + \"4lf8mgKK4LtA6OP1wfzZo1lVJeHDAAIFPOs0nm1Ft89XjMauAdcveM5xkYM7z9VL0vlddiHqQDHK\"\n          + \"WjsgKVnrwpC/I5b4A1FVxJXdPXg14igM8zioW2Y9QMVPxeUmRJxeGfvlotRlD1At1KNKg7Q2bPoi\"\n          + \"1IlRzdae6ky18x/o6FRbTo2WGRehqIAjqmwqNib3u4k/1QfEbKGShVjMtraxdlFBM7kXb/pTfhhU\"\n          + \"xlsf4xraVy2LWBLen+BAOYScd0P7vD+5oET+e4YVqILoz/WQqI9BYmTHkzj+LLECgYEA9bVjRrXq\"\n          + \"5NtO94w0T2BI9yGnZNRFbCcSocUlc6lgX7lFa6N5JvaoWF5p9CmUPPm7lxGOeSzvLKB4qv3uP/Px\"\n          + \"RQzWvAT/isKnSJ2FuKcFYGA527uJ5BlOJAtTKViYhQdYlE2g9KsjLkxJ27aF49jrkhKWqueIdJpF\"\n          + \"VfF9w+KYvVkCgYEAwm205fCRH3WEBzii2TrHqm/nVRWZ7Kxis4JppwxUslLKp33bzbHn9uOKFGfN\"\n          + \"rtXpSq9hvAcnJlJAEyVFtVNFcazE/+GbUfnrKaC3UeomjYxBk45Lcutt441gOO2SFcra7GHiNgVv\"\n          + \"fELNMo/Rr7tk8djcUcYXuDk4Kz/T2AttzcMCgYBg/Z8YtIrqmB+N3Exx4OIsm55GUPyueqYCMZ5d\"\n          + \"D8k5QBtFKByU4t0FNQ/CD/+yKiqAsa956eDnztiTNvWrTRI6XZ0OTzLIhZofMf8tKtEWgCWWtWrz\"\n          + \"HYIY/FdxhMWADaxLrnEQ49VZW0f0cRJdJK2o1amgARF+Zb9k85TflD0S0QKBgBYFlQrCT72vcs/a\"\n          + \"k19lb/4XBK23b6LF97v7VnosRF+aTwffkoje0LY/GYGsLDjUU8M40Coa6U1G3akNfLLIBsKUXg/Z\"\n          + \"ft0vIHqrkHf/vHQl4buTz2npzp2Kgs6P4g8D1f4WLCgQP4tkiZdjgM2VvR5DgNjmRgOAv6LubNE4\"\n          + \"oiw/AoGAXKfOSrbgx8JQUE7Lt6mhGvP9oTj3uiV16GgxjOtkpP3TfjsdRcmivAuekKMMKufQvGxh\"\n          + \"nX9eCYvLqJqZZwPy/002H7So3Yd1/d9ORkKetDKGjXHPDYyEPQQ+ss9OGm53XlViklXb+i9wsdDz\"\n          + \"R7tAFexSjyVKnWSDBh52t6lBtHo=\";\n\n  private static final String SERVER_INFO_DOCUMENT =\n      \"{ \\\"verification_keys\\\": {\"\n          +\n          // this is the public key that goes with the above private key\n          \"\\\"key1\\\":\\\"RSA.ALqcwRcW7FOczn7IzgB-eDJt_lnz0nGVyEEDc2L_8abX_bkx63N8h3YmDw2S2GZEPMfqwVqg1LufpnonI0kWzNvY9coGRl16bbX0XmSNrCget8DUu7x8GYZBgb9obvRo9-3Z4Rltj5epblZSUyTu8VbsEOKTAFfK__musVqwF89Z3XfGjND3rXYgStYaUYyDGYHriNxNsZYzMODMT-xxKbJ5DS9BAxbwn42dv_IOljuWhetWsCBnHwgG_V_0W_enu2KtMP-8WDPETasgBq4z9pTzMEcTJcvU1I2rQjrY4AXgMuIOVwQU69iOqiII9AiHQ1edDLwNyznEcKPR7Vvdf8s.AQAB\\\"},\"\n        + \" \"\n          +\n          // some other information that might be in the server info document.\n          \"\\\"foo\\\": \\\"bar\\\"}\";\n\n  protected VerifierProviders locators;\n  protected VerifierProviders locatorsFromRuby;\n  protected RSAPrivateKey privateKey;\n\n  protected static final String TOKEN_STRING =\n      \"eyJhbGciOiJIUzI1NiIsImtpZCI6ImtleTIifQ.eyJpc3MiOiJnb29nbGUuY29tIiwiYmFyIjoxNSwiZm9vIjoic29tZSB2YWx1ZSIsImF1ZCI6Imh0dHA6Ly93d3cuZ29vZ2xlLmNvbSIsImlhdCI6MTI3NjY2OTcyMiwiZXhwIjoxMjc2NjY5NzIzfQ.Xugb4nb5kLV3NTpOLaz9er5PhAI5mFehHst_33EUFHs\";\n  protected static final String TOKEN_STRING_BAD_SIG =\n      \"eyJhbGciOiJIUzI1NiIsImtpZCI6ImtleTIifQ.eyJpc3MiOiJnb29nbGUuY29tIiwiYmFyIjoxNSwiZm9vIjoic29tZSB2YWx1ZSIsImF1ZCI6Imh0dHA6Ly93d3cuZ29vZ2xlLmNvbSIsImlhdCI6MTI3NjY2OTcyMiwiZXhwIjoxMjc2NjY5NzIzfQ.Wugb4nb5kLV3NTpOLaz9er5PhAI5mFehHst_33EUFHs\";\n  protected static final String TOKEN_STRING_2PARTS =\n      \"eyJhbGciOiJIUzI1NiIsImtpZCI6ImtleTIifQ.eyJpc3MiOiJnb29nbGUuY29tIiwiYmFyIjoxNSwiZm9vIjoic29tZSB2YWx1ZSIsImF1ZCI6Imh0dHA6Ly93d3cuZ29vZ2xlLmNvbSIsImlhdCI6MTI3NjY2OTcyMiwiZXhwIjoxMjc2NjY5NzIzfQ\";\n  protected static final String TOKEN_STRING_UNSUPPORTED_SIGNATURE_ALGORITHM =\n      \"eyJhbGciOiJIUzUxMiIsImtpZCI6ImtleTIifQ.eyJpc3MiOiJnb29nbGUuY29tIiwiYmFyIjoxNSwiZm9vIjoic29tZSB2YWx1ZSIsImF1ZCI6Imh0dHA6Ly93d3cuZ29vZ2xlLmNvbSIsImlhdCI6MTI3NjY2OTcyMiwiZXhwIjoxMjc2NjY5NzIzfQ.44qsiZg1Hnf95N-2wNqd1htgDlE7X0BSUMMkboMcZ5QLKbmVATozMuzdoE0MAhU-IdWUuICFbzu_wcDEXDTLug\";\n  protected static final String TOKEN_STRING_CORRUPT_HEADER =\n      \"fyJhbGciOiJIUzI1NiIsImtpZCI6ImtleTIifQ.eyJpc3MiOiJnb29nbGUuY29tIiwiYmFyIjoxNSwiZm9vIjoic29tZSB2YWx1ZSIsImF1ZCI6Imh0dHA6Ly93d3cuZ29vZ2xlLmNvbSIsImlhdCI6MTI3NjY2OTcyMiwiZXhwIjoxMjc2NjY5NzIzfQ.Xugb4nb5kLV3NTpOLaz9er5PhAI5mFehHst_33EUFHs\";\n  protected static final String TOKEN_STRING_HEADER_MISSING_ALG =\n      \"eyJ3cm9uZ1BhcmFtIjoiSFMyNTYifQ.eyJpc3MiOiJnb29nbGUuY29tIiwiYmFyIjoxNSwiZm9vIjoic29tZSB2YWx1ZSIsImF1ZCI6Imh0dHA6Ly93d3cuZ29vZ2xlLmNvbSIsImlhdCI6MTI3NjY2OTcyMiwiZXhwIjoxMjc2NjY5NzIzfQ.mBtVwsgT2uZJqu2zyUzbCXF4tfo8jSSlOeRI0Tv222o\";\n  protected static final String TOKEN_FROM_RUBY =\n      \"eyJhbGciOiAiSFMyNTYiLCAidHlwIjogIkpXVCJ9.eyJoZWxsbyI6ICJ3b3JsZCJ9.tvagLDLoaiJKxOKqpBXSEGy7SYSifZhjntgm9ctpyj8\";\n\n  protected static final Duration SKEW = Duration.ofMinutes(1);\n  protected FakeClock clock = new FakeClock(SKEW);\n\n  /**\n   * Convert encoded tokens into a more human-readable form without verifying. Useful for logging.\n   */\n  protected static String decodeTokenForHumans(String encodedToken) {\n    String[] pieces = encodedToken.split(Pattern.quote(\".\"));\n    if (pieces.length != 3) {\n      return \"invalid token (3 segments expected): \" + encodedToken;\n    }\n    for (int i = 0; i < 3; i++) {\n      pieces[i] = new String(Base64.decodeBase64(pieces[i].getBytes()));\n    }\n    return pieces[0] + \".\" + pieces[1] + \".\" + pieces[2];\n  }\n\n  @Override\n  protected void setUp() throws Exception {", "context": "src/main/java/net/oauth/jsontoken/exceptions/InvalidJsonTokenException.java\npublic final class InvalidJsonTokenException extends Exception {\n  private final ErrorCode errorCode;\n\n  public InvalidJsonTokenException(ErrorCode errorCode) {\n    this.errorCode = errorCode;\n  }\n\n  public InvalidJsonTokenException(ErrorCode errorCode, String message) {\n    super(message);\n    this.errorCode = errorCode;\n  }\n\n  public InvalidJsonTokenException(ErrorCode errorCode, Throwable cause) {\n    super(cause);\n    this.errorCode = errorCode;\n  }\n\n  public ErrorCode getErrorCode() {\n    return errorCode;\n  }\n}\nsrc/main/java/net/oauth/jsontoken/discovery/IdentityServerDescriptorProvider.java\npublic class IdentityServerDescriptorProvider implements ServerDescriptorProvider {\n\n  /*\n   * (non-Javadoc)\n   * @see net.oauth.jsontoken.discovery.ServerDescriptorProvider#getServerDescriptor(java.lang.String)\n   */\n  @Override\n  public URI getServerDescriptor(String issuer) {\n    return URI.create(issuer);\n  }\n}\nsrc/main/java/net/oauth/jsontoken/discovery/DefaultPublicKeyLocator.java\npublic class DefaultPublicKeyLocator implements VerifierProvider {\n\n  private final ServerDescriptorProvider descriptorProvider;\n  private final ServerInfoResolver descriptorResolver;\n\n  /**\n   * Public constructor.\n   *\n   * @param descriptorProvider A {@link ServerDescriptorProvider} that maps issuer ids to server\n   *     descriptors (URLs).\n   * @param resolver A {@link ServerInfoResolver}, i.e., an object that can fetch and parse a server\n   *     info document, given a server descriptor.\n   */\n  public DefaultPublicKeyLocator(\n      ServerDescriptorProvider descriptorProvider, ServerInfoResolver resolver) {\n    this.descriptorProvider = descriptorProvider;\n    this.descriptorResolver = resolver;\n  }\n\n  /*\n   * (non-Javadoc)\n   * @see net.oauth.jsontoken.discovery.VerifierProvider#findVerifier(java.lang.String, java.lang.String)\n   */\n  @Override\n  public List<Verifier> findVerifier(String issuer, String keyId) {\n    URI serverDescriptor = descriptorProvider.getServerDescriptor(issuer);\n    Verifier rsaVerifier =\n        new RsaSHA256Verifier(\n            descriptorResolver.resolve(serverDescriptor).getVerificationKey(keyId));\n    return Lists.newArrayList(rsaVerifier);\n  }\n}\nsrc/main/java/net/oauth/jsontoken/discovery/VerifierProviders.java\npublic class VerifierProviders {\n\n  private final Map<SignatureAlgorithm, VerifierProvider> map = Maps.newHashMap();\n\n  /** Sets a new {@link VerifierProvider} for the given {@link SignatureAlgorithm}. */\n  public void setVerifierProvider(SignatureAlgorithm alg, VerifierProvider provider) {\n    map.put(alg, provider);\n  }\n\n  /** Returns the {@link VerifierProvider} for the given {@link SignatureAlgorithm}. */\n  @Nullable\n  public VerifierProvider getVerifierProvider(SignatureAlgorithm alg) {\n    return map.get(alg);\n  }\n}\nsrc/main/java/net/oauth/jsontoken/crypto/SignatureAlgorithm.java\npublic enum SignatureAlgorithm {\n  HS256(\"SHA256\"),\n  HS1(\"SHA1\"),\n  RS256(\"SHA256\"),\n  RS1(\"SHA1\");\n\n  private final String hashAlg;\n\n  private SignatureAlgorithm(String hashAlg) {\n    this.hashAlg = hashAlg;\n  }\n\n  /** What the signature algorithm is named in the \"alg\" parameter in a JSON Token's envelope. */\n  public String getNameForJson() {\n    return name();\n  }\n\n  /**\n   * Returns the hash algorithm that should be used when hashing data. When large pieces of data are\n   * to be included in a JSON Token's payload, it sometimes might make sense to include the hash of\n   * the data instead. If an issuer wants to do that, they should use this hash algorithm to hash\n   * the data.\n   */\n  public String getHashAlgorithm() {\n    return hashAlg;\n  }\n\n  /** Given the name of the algorithm in the envelope, returns the corresponding enum instance. */\n  public static SignatureAlgorithm getFromJsonName(String name) {\n    return SignatureAlgorithm.valueOf(name);\n  }\n}\nsrc/main/java/net/oauth/jsontoken/exceptions/ErrorCode.java\npublic enum ErrorCode {\n\n  /** The header is missing required parameters. */\n  BAD_HEADER,\n\n  /** Signature failed verification. */\n  BAD_SIGNATURE,\n\n  /** IAT is after EXP or IAT is in the future */\n  BAD_TIME_RANGE,\n\n  /** IAT and EXP are both in the past. */\n  EXPIRED_TOKEN,\n\n  /**\n   * The token is in an illegal state because of incorrect use of the library. If this error code\n   * appears, immediately rethrow as an {@link IllegalStateException}.\n   */\n  ILLEGAL_STATE,\n\n  /** Token string is corrupted and/or does not contain three components. */\n  MALFORMED_TOKEN_STRING,\n\n  /** There are no verifiers available for a given issuer and keyId. */\n  NO_VERIFIER,\n\n  /** Generic catch-all for exceptions with scenarios that are not pre-defined. */\n  UNKNOWN,\n\n  /** The signature algorithm is not supported or is unknown. */\n  UNSUPPORTED_ALGORITHM\n}\nsrc/main/java/net/oauth/jsontoken/discovery/JsonServerInfo.java\npublic class JsonServerInfo implements ServerInfo {\n\n  @SerializedName(\"verification_keys\")\n  private final Map<String, String> verificationKeys = Maps.newHashMap();\n\n  /**\n   * Parses a JSON-formatted server info document and returns it as a {@link JsonServerInfo} object.\n   *\n   * @param json the contents of the JSON-formatted server info document.\n   */\n  public static JsonServerInfo getDocument(String json) {\n    return new Gson().fromJson(json, JsonServerInfo.class);\n  }\n\n  /*\n   * (non-Javadoc)\n   * @see net.oauth.jsontoken.discovery.ServerInfo#getVerificationKey(java.lang.String)\n   */\n  @Override\n  public PublicKey getVerificationKey(String keyId) {\n    String magicKey = verificationKeys.get(keyId);\n    if (magicKey == null) {\n      return null;\n    } else {\n      return new MagicRsaPublicKey(magicKey).getKey();\n    }\n  }\n}\nsrc/main/java/net/oauth/jsontoken/crypto/HmacSHA256Verifier.java\npublic class HmacSHA256Verifier implements Verifier {\n\n  private final HmacSHA256Signer signer;\n\n  /**\n   * Public constructor.\n   *\n   * @param verificationKey the HMAC verification key to be used for signature verification.\n   * @throws InvalidKeyException if the verificationKey cannot be used as an HMAC key.\n   */\n  public HmacSHA256Verifier(byte[] verificationKey) throws InvalidKeyException {\n    signer = new HmacSHA256Signer(\"verifier\", null, verificationKey);\n  }\n\n  /*\n   * (non-Javadoc)\n   * @see net.oauth.jsontoken.crypto.Verifier#verifySignature(byte[], byte[])\n   */\n  @Override\n  public void verifySignature(byte[] source, byte[] signature) throws SignatureException {\n    byte[] comparison = signer.sign(source);\n    if (!compareBytes(signature, comparison)) {\n      throw new SignatureException(\"signature did not verify\");\n    }\n  }\n\n  /**\n   * Performs a byte-by-byte comparison of {@code first} and {@code second} parameters. This method\n   * will \"NOT\" short-circuit the comparison once it has detected a byte difference in order to\n   * defend against a \"timing attack\".\n   *\n   * @param first the first byte array used in the comparison\n   * @param second the second byte array used in the comparison\n   * @return {@code true} if the {@code first} and {@code second} byte arrays are equal otherwise\n   *     {@code false}\n   */\n  private boolean compareBytes(byte[] first, byte[] second) {\n    if (first == null || second == null) {\n      return (first == second);\n    } else if (first.length != second.length) {\n      return false;\n    } else {\n      byte result = 0;\n      for (int i = 0; i < first.length; i++) {\n        result |= first[i] ^ second[i];\n      }\n      return (result == 0);\n    }\n  }\n}\nsrc/main/java/net/oauth/jsontoken/crypto/Verifier.java\npublic interface Verifier {\n\n  /**\n   * Verifies a signature on an array of bytes.\n   *\n   * @param source The bytes that were signed.\n   * @param signature The signature on the bytes.\n   * @throws SignatureException If the signature doesn't match, or if some other error occurred.\n   */\n  void verifySignature(byte[] source, byte[] signature) throws SignatureException;\n}\nsrc/main/java/net/oauth/jsontoken/discovery/VerifierProvider.java\npublic interface VerifierProvider {\n\n  /**\n   * Returns the {@code List<Verifier>} that represents a certain verification key, given the key's\n   * id and its issuer.\n   *\n   * @param issuer the id of the issuer that's using the key.\n   * @param keyId the id of the key, if keyId mismatches, return a list of possible verification\n   *     keys.\n   * @return a {@code List<Verifier>} object that represents the verification key.\n   */\n  List<Verifier> findVerifier(String issuer, String keyId);\n}\n", "answers": ["    final Verifier hmacVerifier = new HmacSHA256Verifier(SYMMETRIC_KEY);"], "length": 1353, "dataset": "repobench-p_e", "language": "java", "all_classes": null, "_id": "f81c4342fdf64aab3c695157e30a0a0879ff2586d042f718"}
{"input": "package io.github.duckasteroid.progress.slf4j;\nimport io.github.duckasteroid.progress.Configuration;\nimport io.github.duckasteroid.progress.ProgressMonitor;\nimport io.github.duckasteroid.progress.base.BaseProgressMonitor;\nimport io.github.duckasteroid.progress.base.event.ProgressMonitorListener;\nimport io.github.duckasteroid.progress.base.event.ProgressMonitorListenerFactory;\nimport io.github.duckasteroid.progress.base.event.ProgressUpdateType;\nimport io.github.duckasteroid.progress.base.format.CompoundFormat;\nimport io.github.duckasteroid.progress.base.format.ProgressFormat;\nimport io.github.duckasteroid.progress.base.format.SimpleProgressFormat;\nimport io.github.duckasteroid.progress.slf4j.util.LruCache;\nimport io.github.duckasteroid.progress.slf4j.util.SingleLevelMap;\nimport java.util.Collections;\nimport java.util.Map;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n\n/**\n * Factory for monitors that route updates to SLF4J logger instances.\n * The factory uses the following configuration parameters:\n * <ul>\n *   <li><pre>io.github.duckasteroid.progress.slf4j.root.logger.name</pre>The name of the root SLF4J\n *   logger used to receive messages when monitor name begins with <pre>#</pre>. Default is\n *   <pre>io.github.duckasteroid.progress.ProgressMonitor</pre></li>\n *   <li><pre>io.github.duckasteroid.progress.slf4j.cacheSize</pre>. How may instances of listeners\n *   to cache (rather than re-create when requested). Default 100.</li>\n *   <li><pre>io.github.duckasteroid.progress.slf4j.format</pre>A format string that is parsed to\n *   create a {@link ProgressFormat} instance used to format monitor updates being sent to loggers.\n *   Default is {@link SimpleProgressFormat#DEFAULT}</li>\n * </ul>\n */\npublic class Slf4jFactory implements ProgressMonitorListenerFactory {\n  private static final Logger LOG = LoggerFactory.getLogger(Slf4jFactory.class);\n\n  private static final String NS = \"io.github.duckasteroid.progress.slf4j.\";\n  public static final String ROOT_LOGGER_NAME = NS + \"root.logger.name\";\n  public static final String CACHE_SIZE = NS + \".cacheSize\";\n  public static final String FORMAT = NS + \".format\"; //NOPMD - case is different\n  private static final int DEFAULT_CACHE_SIZE = 100;", "context": "api/src/main/java/io/github/duckasteroid/progress/ProgressMonitor.java\npublic interface ProgressMonitor extends Comparable<ProgressMonitor>, Closeable {\n  /**\n   * The parent monitor of this monitor (if any).\n   * @return the parent monitor or <code>null</code> if this is the root\n   */\n  ProgressMonitor getParent();\n\n  /**\n   * The path from this monitor to the root through all parent contexts in order leaf to root.\n   * @return a list of parent contexts (all the way to the root); an empty list if this is the\n   *         root.\n   */\n  List<ProgressMonitor> getContext();\n\n  /**\n   * The name of this task set by the {@link #getParent() parent} (if any).\n   * @return the task name (or empty String)\n   */\n  String getTaskName();\n\n  /**\n   * The last {@link #setStatus(String) notified} status.\n   * @return the task status (or empty String)\n   */\n  String getStatus();\n\n  /**\n   * Notify users (if possible) of the status of this task (without changing the fraction done).\n   * @param status The task status notification message\n   */\n  void setStatus(String status);\n\n  /**\n   * The \"size\" of this task - by default this is <code>1</code>.\n   * @return the \"relative\" size of this task\n   */\n  long getSize();\n\n  /**\n   * Modify the overall size of this task. If the current {@link #getWorkDone()} is already more\n   * than this then the monitor will also be marked {@link #isDone() done}.\n   * Changing the size of a \"done\" monitor has no effect, it remains done...\n   * The size cannot be modified to less than 1; the value defaults to this if out of range.\n   * Note: modifying the size of a subtask - does not influence the work it contributes to the\n   * parent task\n   * @param size the new size of this task\n   */\n  void setSize(long size);\n\n  /**\n   * The unit of work in this monitor (default empty).\n   * Sub tasks will inherit this unit.\n   * @return the unit as set in {@link #setUnit(String)}\n   */\n  String getUnit();\n\n  /**\n   * Modify the unit of this task, only new subtasks will pick up this unit.\n   * @param unit the new unit\n   */\n  void setUnit(String unit);\n\n  /**\n   * The current amount of work done. This is always a positive number or zero.\n   * It may be more than the size of this task (which simply means the task is done)\n   * @return the current amount of work done\n   */\n  long getWorkDone();\n\n  /**\n   * Log an amount of work and (optionally) update the status in a single operation.\n   * This operation has no effect if the monitor is {@linkplain #isDone() done} or\n   * {@linkplain #isCancelled() cancelled}.\n   * If this work takes the {@linkplain #getWorkDone() total work done} past the {@linkplain\n   * #getSize() size} then the monitor will be marked done.\n   * If the amount of work is less than zero, it will default to zero\n   * @param amount the amount of work done\n   * @param status a status message to set (if not <code>null</code>)\n   * @return the value of {@link #getWorkDone()} as a result of this new work\n   */\n  long worked(long amount, String status);\n\n  /**\n   * Log an amount of work with no (null) corresponding status update.\n   * This is the same as calling <code>worked(amount, null)</code>\n   * @see #worked(long, String)\n   * @param amount the amount of work done\n   * @return the value of {@link #getWorkDone()} as a result of this new work\n   */\n  long worked(long amount);\n\n  /**\n   * Called to set the work done to the size of the task (at this point in time).\n   * Subsequent modifications to {@link #getSize()} may alter this, but the progress will only be\n   * done once\n   */\n  void done();\n\n  /**\n   * Has the work reported completed (i.e. {@link #getWorkDone()} &gt;= {@link #getSize()}} )\n   * NOTE: This is a one time event, if the work done is ever &gt;= the size - subsequent\n   * increases in the size don't \"undo\" the monitor, it remains done.\n   * @return true if the work was complete/done\n   */\n  boolean isDone();\n\n  @Override\n  default void close() {\n    done();\n  }\n\n  /**\n   * Has the task being reported been cancelled. This is used as a signal between the class doing\n   * progress and the outside world that may wish it to stop before it is complete.\n   * The cancelled state has no appreciable impact on the rest of the workings of the monitor -\n   * it's for the receiver and the publisher of the progress to decide what to do with it.\n   * @return true if cancelled\n   */\n  boolean isCancelled();\n\n  /**\n   * Change the cancelled state of the task being monitored (and that of all sub-tasks).\n   * @see #isCancelled()\n   * @param cancelled the new cancelled state\n   */\n  void setCancelled(boolean cancelled);\n\n  /**\n   * Create a sub task of this - which when {@linkplain #isDone() done} equates to the given\n   * amount of work in this monitor.\n   * NOTE: The progress of the subtask is not reflected in this monitor until it is\n   * {@link #done()}, only then is the total work posted on this monitor.\n   * @param name the name of the sub task\n   * @param work the amount of work the new sub task will contribute to this monitor when done\n   * @return a new progress monitor for the sub task\n   * @throws IllegalStateException If this task is already {@link #isDone() done} or is\n   *                 {@link #isCancelled() cancelled}\n   */\n  ProgressMonitor newSubTask(String name, long work);\n\n  /**\n   * Create a sub task of this - which when {@link #isDone()} equates to a single (1) unit of work\n   * in this task.\n   * Equivalent to calling {@link #newSubTask(String, long)} with the given <code>name</code> and\n   * a <code>size = 1</code>\n   * @param name the name of the sub task\n   * @return a new progress monitor for the sub task\n   * @throws IllegalStateException If this task is already {@link #isDone() done} or is\n   *                 {@link #isCancelled() cancelled}\n   */\n  ProgressMonitor newSubTask(String name);\n\n  /**\n   * The {@link #getWorkDone()} as a fraction of the {@link #getSize()} for this task in the range\n   * 0 to 1.\n   * NOTE: This value can never be less than 0 or more than 1\n   * @return the fraction done\n   */\n  double getFractionDone();\n\n\n}\nclient/src/main/java/io/github/duckasteroid/progress/Configuration.java\npublic class Configuration {\n  private static final Configuration singleton = load();\n  private final transient Properties source;\n\n  public Configuration(Properties source) {\n    this.source = source;\n  }\n\n  private static Configuration load() {\n    // FIXME Attempt to load config files from classpath a la logging frameworks\n    return new Configuration(System.getProperties());\n  }\n\n  public static Configuration getInstance() {\n    return singleton;\n  }\n\n  public Optional<String> getStringValue(String prop) {\n    return Optional.ofNullable(source.getProperty(prop));\n  }\n\n  public boolean hasValueFor(String name) {\n    return getStringValue(name).isPresent();\n  }\n\n  public <T> T getValue(String name, Function<String, T> parser, T defaultValue) {\n    return getStringValue(name).map(parser::apply).orElse(defaultValue);\n  }\n\n  public Integer getInteger(String name, Integer defaultValue) {\n    return getValue(name, Integer::parseInt, defaultValue);\n  }\n\n  public Long getLong(String name, Long defaultValue) {\n    return getValue(name, Long::parseLong, defaultValue);\n  }\n\n  public Boolean getBoolean(String name, Boolean defaultValue) {\n    return getValue(name, Boolean::parseBoolean, defaultValue);\n  }\n\n  public String getString(String name, String defaultValue) {\n    return getValue(name, Function.identity(), defaultValue);\n  }\n\n}\nslf4j/src/main/java/io/github/duckasteroid/progress/slf4j/util/SingleLevelMap.java\npublic class SingleLevelMap implements Map<ProgressUpdateType, Slf4JProgress.Level> {\n  private final transient Slf4JProgress.Level value;\n\n  public SingleLevelMap(Slf4JProgress.Level value) {\n    this.value = value;\n  }\n\n  @Override\n  public int size() {\n    return ProgressUpdateType.values().length;\n  }\n\n  @Override\n  public boolean isEmpty() {\n    return false;\n  }\n\n  @Override\n  public boolean containsKey(Object key) {\n    return key != null && key instanceof ProgressUpdateType;\n  }\n\n  @Override\n  public boolean containsValue(Object value) {\n    return this.value.equals(value);\n  }\n\n  @Override\n  public Slf4JProgress.Level get(Object key) {\n    return value;\n  }\n\n  @Override\n  public Slf4JProgress.Level put(ProgressUpdateType key, Slf4JProgress.Level value) {\n    throw new UnsupportedOperationException();\n  }\n\n  @Override\n  public Slf4JProgress.Level remove(Object key) {\n    throw new UnsupportedOperationException();\n  }\n\n  @Override\n  public void putAll(Map<? extends ProgressUpdateType, ? extends Slf4JProgress.Level> m) {\n    throw new UnsupportedOperationException();\n  }\n\n  @Override\n  public void clear() {\n    throw new UnsupportedOperationException();\n  }\n\n  @Override\n  public Set<ProgressUpdateType> keySet() {\n    return Set.of(ProgressUpdateType.values());\n  }\n\n  @Override\n  public Collection<Slf4JProgress.Level> values() {\n    return Collections.singleton(value);\n  }\n\n  @Override\n  public Set<Entry<ProgressUpdateType, Slf4JProgress.Level>> entrySet() {\n    return keySet().stream().map(k -> new Entry<ProgressUpdateType, Slf4JProgress.Level>() {\n      @Override\n      public ProgressUpdateType getKey() {\n        return k;\n      }\n\n      @Override\n      public Slf4JProgress.Level getValue() {\n        return value;\n      }\n\n      @Override\n      public Slf4JProgress.Level setValue(Slf4JProgress.Level value) {\n        throw new UnsupportedOperationException();\n      }\n    }).collect(Collectors.toSet());\n  }\n}\nclient/src/main/java/io/github/duckasteroid/progress/base/BaseProgressMonitor.java\npublic final class BaseProgressMonitor extends AbstractProgressMonitor implements ProgressMonitor {\n\n  /**\n   * A set of listeners.\n   */\n  protected final transient List<ProgressMonitorListener> listeners = new CopyOnWriteArrayList<>();\n  /**\n   * Has cancellation been requested.\n   */\n  protected AtomicBoolean cancelled = new AtomicBoolean(false);\n\n  public BaseProgressMonitor() {\n    this(\"\");\n  }\n\n  public BaseProgressMonitor(final String name) {\n    super(0, name);\n  }\n\n  public BaseProgressMonitor(final String name, final long size) {\n    this(name);\n    this.setSize(size);\n  }\n\n  /**\n   * Create a new progress monitor.\n   * @param name the name of the monitor\n   * @param size the size (units of work)\n   * @param listeners any listeners for the monitor\n   */\n  public BaseProgressMonitor(final String name, final long size,\n                             Collection<ProgressMonitorListener> listeners) {\n    this(name);\n    this.setSize(size);\n    this.listeners.addAll(listeners);\n  }\n\n  public BaseProgressMonitor(final long size) {\n    this();\n    this.setSize(size);\n  }\n\n  public void addProgressMonitorListener(ProgressMonitorListener listener) {\n    listeners.add(listener);\n  }\n\n  public void removeProgressMonitorListener(ProgressMonitorListener listener) {\n    listeners.remove(listener);\n  }\n\n  @Override\n  @SuppressWarnings(\"PMD.DataflowAnomalyAnalysis\")\n  public void notifyListeners(final ProgressMonitor source, final ProgressUpdateType updateType) {\n    final ProgressMonitorEvent event = new ProgressMonitorEvent(this, source, updateType);\n    // iterate the listeners (if any)\n    for (ProgressMonitorListener listener : listeners) {\n      listener.logUpdate(event);\n    }\n  }\n\n  @Override\n  protected void onDone() {\n    // nothing to do!\n  }\n\n  /**\n   * A list of all the active monitors in this monitor and all children that are active.\n   * As soon as a monitor is marked done - all children are removed from this list.\n   *\n   * @return the list of all active monitors (in the order they were created - with their children\n   *         after them)\n   */\n  public List<ProgressMonitor> getAllActive() {\n    ArrayList<ProgressMonitor> active = new ArrayList<>(children.size() + 1);\n    appendActive(active);\n    return active;\n  }\n\n  /**\n   * Always returns <code>null</code>.\n   */\n  public ProgressMonitor getParent() {\n    return null;\n  }\n\n  /**\n   * Always returns an empty list.\n   */\n  public List<ProgressMonitor> getContext() {\n    return Collections.emptyList();\n  }\n\n  public boolean isCancelled() {\n    return cancelled.get();\n  }\n\n  @Override\n  public void setCancelled(boolean cancelled) {\n    this.cancelled.set(cancelled);\n    if (cancelled) {\n      notifyListeners(this, ProgressUpdateType.CANCELLED);\n    }\n  }\n\n\n}\nclient/src/main/java/io/github/duckasteroid/progress/base/event/ProgressMonitorListenerFactory.java\npublic interface ProgressMonitorListenerFactory {\n  /**\n   * Create a {@link ProgressMonitorListener} for the named progress.\n   *\n   * @param name the name of the progress\n   * @return a {@link ProgressMonitorListener} instance for the name\n   */\n  ProgressMonitorListener createProgressMonitorListener(String name);\n}\nslf4j/src/main/java/io/github/duckasteroid/progress/slf4j/util/LruCache.java\npublic class LruCache extends LinkedHashMap<String, ProgressMonitorListener> {\n  private static final long serialVersionUID = 71308712074L;\n\n  private final transient int cacheSize;\n\n  public LruCache(int size) {\n    super(size, 0.75f, true);\n    this.cacheSize = size;\n  }\n\n  @Override\n  protected boolean removeEldestEntry(Map.Entry<String, ProgressMonitorListener> eldest) {\n    return size() >= cacheSize;\n  }\n}\nformat/src/main/java/io/github/duckasteroid/progress/base/format/ProgressFormat.java\npublic interface ProgressFormat {\n  /**\n   * Given the monitor produce a string form.\n   *\n   * @param monitor the monitor to format\n   * @return the string format\n   */\n  String format(ProgressMonitor monitor);\n}\nclient/src/main/java/io/github/duckasteroid/progress/base/event/ProgressUpdateType.java\npublic enum ProgressUpdateType {\n  /**\n   * A status change.\n   */\n  STATUS,\n  /**\n   * Work done (the most common event).\n   */\n  WORK,\n  /**\n   * Progress cancelled.\n   */\n  CANCELLED,\n  /**\n   * Progress done.\n   */\n  DONE;\n\n  /**\n   * Parse the string form into an enum.\n   *\n   * @param s the string form\n   * @return corresponding enum or null\n   * @see #name()\n   */\n  public static ProgressUpdateType parse(String s) {\n    for (ProgressUpdateType type : values()) {\n      if (type.name().equals(s)) {\n        return type;\n      }\n    }\n    return null;\n  }\n}\nformat/src/main/java/io/github/duckasteroid/progress/base/format/SimpleProgressFormat.java\npublic class SimpleProgressFormat implements ProgressFormat {\n  /**\n   * A default (everything on) format for monitors.\n   */\n  public static final ProgressFormat DEFAULT =\n      createSimpleProgressFormat(true, true, true, true, true, true);\n\n  /**\n   * include a <code>&gt;</code> separated list of parents.\n   */\n  private final transient boolean showParents;\n\n  private final transient ProgressFormat format; //NOPMD\n\n  private SimpleProgressFormat(boolean showParents, ProgressFormat format) {\n    this.showParents = showParents;\n    this.format = format;\n  }\n\n  /**\n   * Create a simple progress format.\n   * @param showParents show parent monitors\n   * @param showTask show task name\n   * @param showWork show work done absolute\n   * @param showUnit show work unit\n   * @param showPercent show percent work done\n   * @param showStatus show status message\n   * @return a new {@link SimpleProgressFormat}\n   */\n  public static SimpleProgressFormat createSimpleProgressFormat(boolean showParents,\n                                                                boolean showTask, boolean showWork,\n                                                                boolean showUnit,\n                                                                boolean showPercent,\n                                                                boolean showStatus) {\n    ArrayList<FormatElement> elements = new ArrayList<>(8);\n    if (showTask) {\n      elements.add(new TaskName());\n    }\n    if (showWork) {\n      if (!elements.isEmpty()) {\n        elements.add(StaticString.CONDITIONAL_WHITESPACE);\n      }\n      elements.add(new StringWrapper(\"[\", new Fraction(), \"]\"));\n    }\n    if (showUnit) {\n      if (!elements.isEmpty()) {\n        elements.add(StaticString.CONDITIONAL_WHITESPACE);\n      }\n      elements.add(new Unit());\n    }\n    if (showPercent) {\n      if (!elements.isEmpty()) {\n        elements.add(StaticString.CONDITIONAL_WHITESPACE);\n      }\n      elements.add(new StringWrapper(\"(\", new Percentage(), \")\"));\n    }\n    if (showStatus) {\n      if (!elements.isEmpty()) {\n        elements.add(new StaticString(\" - \", false));\n      }\n      elements.add(new Status());\n    }\n    return new SimpleProgressFormat(showParents, new CompoundFormat(elements));\n  }\n\n  @Override\n  public String format(ProgressMonitor monitor) {\n    final StringBuilder sb = new StringBuilder();\n\n    if (showParents && monitor.getParent() != null) {\n      // reurse\n      String parent = format(monitor.getParent());\n      sb.append(parent).append(\"> \");\n    }\n\n    sb.append(format.format(monitor));\n\n    return sb.toString();\n  }\n}\nformat/src/main/java/io/github/duckasteroid/progress/base/format/CompoundFormat.java\npublic class CompoundFormat implements ProgressFormat {\n\n  public static CompoundFormat MAXIMAL = new CompoundFormat(new FormatElement[] {\n      new TaskName(), StaticString.CONDITIONAL_WHITESPACE,\n      new Spinner(Spinner.SPINNER_SLASHES), StaticString.WHITESPACE,\n      new Percentage(), StaticString.WHITESPACE,\n      StringWrapper.wrap(\"[\", new ProgressBar(50, BAR_EQUALS), \"]\"), StaticString.WHITESPACE,\n      new Fraction(), StaticString.WHITESPACE,\n      StringWrapper.prefix(\"- \", new Status())\n  });\n  private final List<FormatElement> elements = new ArrayList<>(7); //NOPMD - method clash\n\n  public CompoundFormat(FormatElement... elements) {\n    this(Arrays.asList(elements));\n  }\n\n  public CompoundFormat(Collection<FormatElement> elements) {\n    this.elements.addAll(elements);\n  }\n\n  @SuppressWarnings(\"PMD.DataflowAnomalyAnalysis\")\n  static List<FormatElement> parse(String config,\n                                   Map<String, Function<String, FormatElement>> helpers) {\n    LinkedList<FormatElement> result = new LinkedList<>();\n    StringTokenizer segments = new StringTokenizer(config, \"%\", true);\n\n    ParseState state = ParseState.TEXT;\n    int pos = 0;\n    while (segments.hasMoreTokens()) {\n      String s = segments.nextToken();\n      pos += s.length();\n      if (\"%\".equals(s)) {\n        if (state == ParseState.TEXT) {\n          state = ParseState.ENTITY;\n        } else {\n          state = ParseState.TEXT;\n        }\n        continue; // skip...\n      }\n\n      switch (state) {\n        default:\n        case TEXT:\n          FormatElement string = new StaticString(s);\n          add(string, result);\n          break;\n        case ENTITY:\n          String[] split = s.split(\":\");\n          Function<String, FormatElement> elementBuilder = helpers.get(split[0]);\n          FormatElement formatElement = elementBuilder.apply(split.length > 1 ? split[1] : \"\");\n          add(formatElement, result);\n          break;\n      }\n    }\n    return result;\n  }\n\n  public static CompoundFormat parse(String s) {\n    return new CompoundFormat(parse(s, FormatParser.loadParsers()));\n  }\n\n  private static void add(FormatElement formatElement, LinkedList<FormatElement> result) {\n    if (formatElement != null) {\n      if (result.size() > 0) {\n        FormatElement previous = result.getLast();\n        if (previous instanceof FormatElement.Wrapping) {\n          // already wrapping?\n          if (!((FormatElement.Wrapping) previous).isWrapping()) {\n            ((FormatElement.Wrapping) previous).setWrapped(formatElement);\n            return;\n          }\n        }\n      }\n      // add the\n      result.add(formatElement);\n    }\n  }\n\n  @Override\n  public String format(ProgressMonitor source) {\n    StringBuilder string = new StringBuilder();\n    for (FormatElement fe : elements) {\n      fe.appendTo(string, source);\n    }\n    return string.toString();\n  }\n\n  public List<FormatElement> elements() {\n    return Collections.unmodifiableList(elements);\n  }\n\n  private enum ParseState {\n    TEXT, ENTITY\n  }\n\n}\nclient/src/main/java/io/github/duckasteroid/progress/base/event/ProgressMonitorListener.java\npublic interface ProgressMonitorListener {\n  /**\n   * Called when the supplied progress monitor has an update.\n   *\n   * @param event The event object\n   */\n  default void logUpdate(ProgressMonitorEvent event) {\n  }\n\n  ;\n}\n", "answers": ["  private final transient Map<String, ProgressMonitorListener> cache = new LruCache(getCacheSize());"], "length": 2514, "dataset": "repobench-p_e", "language": "java", "all_classes": null, "_id": "b17456c5d586663c22b6d7f85f55ea076645fbc26390d24f"}
{"input": "import logging\nimport operator\nimport pytest\nfrom bloop.conditions import ConditionRenderer\nfrom bloop.exceptions import InvalidModel, InvalidStream\nfrom bloop.models import (\n    BaseModel,\n    Column,\n    GlobalSecondaryIndex,\n    IMeta,\n    Index,\n    LocalSecondaryIndex,\n    bind_column,\n    bind_index,\n    model_created,\n    object_modified,\n    unbind,\n    unpack_from_dynamodb,\n)\nfrom bloop.types import (\n    UUID,\n    Boolean,\n    DateTime,\n    Integer,\n    String,\n    Timestamp,\n    Type,\n)\nfrom ..helpers.models import User, VectorModel\n    (operator.ne, \"!=\"),\n    (operator.eq, \"==\"),\n    (operator.lt, \"<\"),\n    (operator.le, \"<=\"),\n    (operator.gt, \">\"),\n    (operator.ge, \">=\")\n]\n\n\n@pytest.fixture\ndef unpack_kwargs(engine):\n    return {\n        \"attrs\": {\"name\": {\"S\": \"numberoverzero\"}},\n        \"expected\": {User.name, User.joined},\n        \"model\": User,\n        \"engine\": engine,\n        \"context\": {\"engine\": engine, \"extra\": \"foo\"},\n    }\n\n\n# BASE MODEL =============================================================================================== BASE MODEL\n\n\ndef test_default_model_init():\n    \"\"\"Missing attributes are set to `None`\"\"\"\n    user = User(id=\"user_id\", email=\"user@domain.com\")\n    assert user.id == \"user_id\"\n    assert user.email == \"user@domain.com\"\n    assert not hasattr(user, \"name\")\n\n\ndef test_load_default_init(engine):\n    \"\"\"The default model loader uses cls.__new__(cls) method\"\"\"\n    init_called = False\n\n    class Blob(BaseModel):\n        def __init__(self, *args, **kwargs):\n            nonlocal init_called\n            # No args, kwargs provided to custom init function\n            assert not args\n            assert not kwargs\n            init_called = True\n            super().__init__(**kwargs)\n        id = Column(String, hash_key=True)\n        data = Column(String)\n    engine.bind(Blob)\n\n    assert isinstance(Blob.Meta.init(), Blob)\n\n    blob = {\n        \"id\": {\"S\": \"foo\"},\n        \"data\": {\"S\": \"data\"},\n        \"extra_field\": {\"N\": \"0.125\"}\n    }\n\n    unpack_from_dynamodb(attrs=blob, expected=Blob.Meta.columns, model=Blob, engine=engine)\n\n    assert init_called is False\n\n\ndef test_meta_read_write_units():\n    \"\"\"If `read_units` or `write_units` is missing from a model's Meta, it defaults to None until bound\"\"\"\n    class Model(BaseModel):\n        id = Column(UUID, hash_key=True)\n\n    assert Model.Meta.write_units is None\n    assert Model.Meta.read_units is None\n\n    class Other(BaseModel):\n        class Meta:\n            read_units = 2\n            write_units = 3\n        id = Column(UUID, hash_key=True)\n\n    assert Other.Meta.write_units == 3\n    assert Other.Meta.read_units == 2\n\n\ndef test_meta_indexes_columns():\n    \"\"\"An index should not be considered a Column, even if it subclasses\"\"\"\n    assert User.by_email not in set(User.Meta.columns)\n    assert User.by_email in set(User.Meta.indexes)\n\n\ndef test_invalid_model_keys():\n    with pytest.raises(InvalidModel):\n        class DoubleHash(BaseModel):\n            hash1 = Column(UUID, hash_key=True)\n            hash2 = Column(UUID, hash_key=True)\n\n    with pytest.raises(InvalidModel):\n        class DoubleRange(BaseModel):\n            id = Column(UUID, hash_key=True)\n            range1 = Column(UUID, range_key=True)\n            range2 = Column(UUID, range_key=True)\n\n    with pytest.raises(InvalidModel):\n        class NoHash(BaseModel):\n            other = Column(UUID, range_key=True)\n\n    with pytest.raises(InvalidModel):\n        class SharedHashRange(BaseModel):\n            both = Column(UUID, hash_key=True, range_key=True)\n\n\ndef test_invalid_model_duplicate_dynamo_name():\n    \"\"\"Two columns have the same dynamo_name, which is ambiguous\"\"\"\n    with pytest.raises(InvalidModel):\n        class SharedDynamoName(BaseModel):\n            class Meta:\n                abstract = True\n            id = Column(UUID, hash_key=True)\n            first = Column(String, dynamo_name=\"shared\")\n            second = Column(Integer, dynamo_name=\"shared\")\n\n\ndef test_invalid_local_index():\n    with pytest.raises(InvalidModel):\n        class InvalidLSI(BaseModel):\n            id = Column(UUID, hash_key=True)\n", "context": "tests/helpers/models.py\nclass User(BaseModel):\n    id = Column(String, hash_key=True)\n    age = Column(Integer)\n    name = Column(String)\n    email = Column(String)\n    joined = Column(DateTime, dynamo_name=\"j\")\n    by_email = GlobalSecondaryIndex(hash_key=\"email\", projection=\"all\")\nbloop/types.py\nclass UUID(String):\n    python_type = uuid.UUID\n\n    def dynamo_load(self, value, *, context, **kwargs):\n        if value is None:\n            return None\n        return uuid.UUID(value)\n\n    def dynamo_dump(self, value, *, context, **kwargs):\n        if value is None:\n            return None\n        return str(value)\nbloop/exceptions.py\nclass InvalidModel(BloopException, ValueError):\n    \"\"\"This is not a valid Model.\"\"\"\nbloop/types.py\nclass Type:\n    \"\"\"Abstract base type.\"\"\"\n\n    python_type = None\n    backing_type = None\n\n    def supports_operation(self, operation: str) -> bool:\n        \"\"\"\n        Used to ensure a conditional operation is supported by this type.\n\n        By default, uses a hardcoded table of operations that maps to each backing DynamoDB type.\n\n        You can override this method to implement your own conditional operators, or to dynamically\n        adjust which operations your type supports.\n        \"\"\"\n        return operation in OPERATION_SUPPORT_BY_TYPE[self.backing_type]\n\n    def __init__(self):\n        if not hasattr(self, \"inner_typedef\"):\n            self.inner_typedef = self\n        super().__init__()\n\n    def __getitem__(self, key):\n        raise RuntimeError(f\"{self!r} does not support document paths\")\n\n    def dynamo_dump(self, value, *, context, **kwargs):\n        \"\"\"Converts a local value into a DynamoDB value.\n\n        For example, to store a string enum as an integer:\n\n        .. code-block:: python\n\n            def dynamo_dump(self, value, *, context, **kwargs):\n                colors = [\"red\", \"blue\", \"green\"]\n                return colors.index(value.lower())\n        \"\"\"\n        raise NotImplementedError\n\n    def dynamo_load(self, value, *, context, **kwargs):\n        \"\"\"Converts a DynamoDB value into a local value.\n\n        For example, to load a string enum from an integer:\n\n        .. code-block:: python\n\n            def dynamo_dump(self, value, *, context, **kwargs):\n                colors = [\"red\", \"blue\", \"green\"]\n                return colors[value]\n        \"\"\"\n        raise NotImplementedError\n\n    def _dump(self, value, **kwargs):\n        \"\"\"Entry point for serializing values.  Most custom types should use :func:`~bloop.types.Type.dynamo_dump`.\n\n        This wraps the return value of :func:`~bloop.types.Type.dynamo_dump` in DynamoDB's wire format.\n        For example, serializing a string enum to an int:\n\n        .. code-block:: python\n\n            value = \"green\"\n            # dynamo_dump(\"green\") = 2\n            _dump(value) == {\"N\": 2}\n\n        If a complex type calls this function with ``None``, it will forward ``None`` to\n        :func:`~bloop.types.Type.dynamo_dump`.  This can happen when dumping eg. a sparse\n        :class:`~.bloop.types.Map`, or a missing (not set) value.\n        \"\"\"\n        wrapped = actions.wrap(value)\n        value = self.dynamo_dump(wrapped.value, **kwargs)\n        if value is None:\n            return actions.wrap(None)\n        else:\n            value = {self.backing_type: value}\n            return wrapped.type.new_action(value)\n\n    def _load(self, value, **kwargs):\n        \"\"\"Entry point for deserializing values.  Most custom types should use :func:`~bloop.types.Type.dynamo_load`.\n\n        This unpacks DynamoDB's wire format and calls :func:`~bloop.types.Type.dynamo_load` on the inner value.\n        For example, deserializing an int to a string enum:\n\n        .. code-block:: python\n\n            value = {\"N\": 2}\n            # dynamo_load(2) = \"green\"\n            _load(value) == \"green\"\n\n        If a complex type calls this function with ``None``, it will forward ``None`` to\n        :func:`~bloop.types.Type.dynamo_load`.  This can happen when loading eg. a sparse :class:`~bloop.types.Map`.\n        \"\"\"\n        if value is not None:\n            value = next(iter(value.values()))\n        return self.dynamo_load(value, **kwargs)\n\n    def __repr__(self):\n        # Render class python types by name\n        python_type = self.python_type\n        if isinstance(python_type, type):\n            python_type = python_type.__name__\n\n        return \"<{}[{}:{}]>\".format(\n            self.__class__.__name__,\n            self.backing_type, python_type\n        )\nbloop/types.py\nclass DateTime(String):\n    \"\"\"Always stored in DynamoDB using the :data:`~bloop.types.FIXED_ISO8601_FORMAT` format.\n\n    Naive datetimes (``tzinfo is None``) are not supported, and trying to use one will raise ``ValueError``.\n\n    .. code-block:: python\n\n        from datetime import datetime, timedelta, timezone\n\n        class Model(Base):\n            id = Column(Integer, hash_key=True)\n            date = Column(DateTime)\n        engine.bind()\n\n        obj = Model(id=1, date=datetime.now(timezone.utc))\n        engine.save(obj)\n\n        one_day_ago = datetime.now(timezone.utc) - timedelta(days=1)\n\n        query = engine.query(\n            Model,\n            key=Model.id==1,\n            filter=Model.date >= one_day_ago)\n\n        query.first().date\n\n    .. note::\n\n        To use common datetime libraries such as `arrow`_, `delorean`_, or `pendulum`_,\n        see :ref:`DateTime and Timestamp Extensions <user-extensions-datetime>` in the user guide.  These\n        are drop-in replacements and support non-utc timezones:\n\n        .. code-block:: python\n\n            from bloop import DateTime  # becomes:\n            from bloop.ext.pendulum import DateTime\n\n    .. _arrow: http://crsmithdev.com/arrow\n    .. _delorean: https://delorean.readthedocs.io/en/latest/\n    .. _pendulum: https://pendulum.eustace.io\n    \"\"\"\n    python_type = datetime.datetime\n\n    def dynamo_load(self, value, *, context, **kwargs):\n        if value is None:\n            return None\n        dt = datetime.datetime.strptime(value, FIXED_ISO8601_FORMAT)\n        # we assume all stored values are utc, so we simply force timezone to utc\n        # without changing the day/time values\n        return dt.replace(tzinfo=datetime.timezone.utc)\n\n    def dynamo_dump(self, value, *, context, **kwargs):\n        if value is None:\n            return None\n        if value.tzinfo is None:\n            raise ValueError(\n                \"naive datetime instances are not supported.  You can set a timezone with either \"\n                \"your_dt.replace(tzinfo=) or your_dt.astimezone(tz=).  WARNING: calling astimezone on a naive \"\n                \"datetime will assume the naive datetime is in the system's timezone, even though \"\n                \"datetime.utcnow() creates a naive object!  You almost certainly don't want to do that.\"\n            )\n        dt = value.astimezone(tz=datetime.timezone.utc)\n        return dt.strftime(FIXED_ISO8601_FORMAT)\nbloop/conditions.py\nclass ConditionRenderer:\n    # noinspection PyUnresolvedReferences\n    \"\"\"Renders collections of :class:`~bloop.conditions.BaseCondition` into DynamoDB's wire format for expressions,\n    including:\n\n    * ``\"ConditionExpression\"`` -- used in conditional operations\n    * ``\"FilterExpression\"`` -- used in queries and scans to ignore results that don't match the filter\n    * ``\"KeyConditionExpressions\"`` -- used to describe a query's hash (and range) key(s)\n    * ``\"ProjectionExpression\"`` -- used to include a subset of possible columns in the results of a query or scan\n    * ``\"UpdateExpression\"`` -- used to save objects\n\n    Normally, you will only need to call :func:`~bloop.conditions.ConditionRenderer.render` to handle any combination\n    of conditions.  You can also call each individual ``render_*`` function to control how multiple conditions of\n    each type are applied.\n\n    You can collect the rendered condition at any time through :attr:`~bloop.conditions.ConditionRenderer.rendered`.\n\n    .. code-block:: python\n\n        >>> renderer.render(obj=user, atomic=True)\n        >>> renderer.output\n        {'ConditionExpression': '((#n0 = :v1) AND (attribute_not_exists(#n2)) AND (#n4 = :v5))',\n         'ExpressionAttributeNames': {'#n0': 'age', '#n2': 'email', '#n4': 'id'},\n         'ExpressionAttributeValues': {':v1': {'N': '3'}, ':v5': {'S': 'some-user-id'}}}\n\n\n    :param engine: Used to dump values in conditions into the appropriate wire format.\n    :type engine: :class:`~bloop.engine.Engine`\n    \"\"\"\n    def __init__(self, engine):\n        self.refs = ReferenceTracker(engine)\n        self.engine = engine\n        self.expressions = {}\n\n    def render(self, obj=None, condition=None, update=False, filter=None, projection=None, key=None):\n        \"\"\"Main entry point for rendering multiple expressions.  All parameters are optional, except obj when\n        atomic or update are True.\n\n        :param obj: *(Optional)* An object to render an atomic condition or update expression for.  Required if\n            update or atomic are true.  Default is False.\n        :param condition: *(Optional)* Rendered as a \"ConditionExpression\" for a conditional operation.\n            If atomic is True, the two are rendered in an AND condition.  Default is None.\n        :type condition: :class:`~bloop.conditions.BaseCondition`\n        :param bool update: *(Optional)*  True if an \"UpdateExpression\" should be rendered for ``obj``.\n            Default is False.\n        :param filter: *(Optional)* A filter condition for a query or scan, rendered as a \"FilterExpression\".\n            Default is None.\n        :type filter: :class:`~bloop.conditions.BaseCondition`\n        :param projection: *(Optional)* A set of Columns to include in a query or scan, rendered as a\n            \"ProjectionExpression\".  Default is None.\n        :type projection: set :class:`~bloop.models.Column`\n        :param key: *(Optional)* A key condition for queries, rendered as a \"KeyConditionExpression\".  Default is None.\n        :type key: :class:`~bloop.conditions.BaseCondition`\n        \"\"\"\n        if update and not obj:\n            raise InvalidCondition(\"An object is required to render updates.\")\n\n        if filter:\n            self.filter_expression(filter)\n\n        if projection:\n            self.projection_expression(projection)\n\n        if key:\n            self.key_expression(key)\n\n        # Condition requires a bit of work, because either one can be empty/false\n        if condition:\n            self.condition_expression(condition)\n\n        if update:\n            self.update_expression(obj)\n\n    def condition_expression(self, condition):\n        self.expressions[\"ConditionExpression\"] = condition.render(self)\n\n    def filter_expression(self, condition):\n        self.expressions[\"FilterExpression\"] = condition.render(self)\n\n    def key_expression(self, condition):\n        self.expressions[\"KeyConditionExpression\"] = condition.render(self)\n\n    def projection_expression(self, columns):\n        included = set()\n        ref_names = []\n        for column in columns:\n            if column in included:\n                continue\n            included.add(column)\n            ref = self.refs.any_ref(column=column)\n            ref_names.append(ref.name)\n        self.expressions[\"ProjectionExpression\"] = \", \".join(ref_names)\n\n    def update_expression(self, obj):\n        updates = {\n            ActionType.Add: [],\n            ActionType.Delete: [],\n            ActionType.Remove: [],\n            ActionType.Set: [],\n        }\n        for column in sorted(\n                # Don't include key columns in an UpdateExpression\n                filter(lambda c: c not in obj.Meta.keys, global_tracking[obj]),\n                key=lambda c: c.dynamo_name):\n            name_ref = self.refs.any_ref(column=column)\n            value_ref = self.refs.any_ref(column=column, value=getattr(obj, column.name, None))\n            update_type = value_ref.action.type\n            # Can't set to an empty value, force to a Remove\n            if is_empty(value_ref) or update_type is ActionType.Remove:\n                self.refs.pop_refs(value_ref)\n                update_type = ActionType.Remove\n                value_ref = None\n            updates[update_type].append((name_ref, value_ref))\n\n        expressions = []\n        for update_type, refs in updates.items():\n            if not refs:\n                continue\n            k = update_type.wire_key.upper()\n            r = update_type.render\n            expressions.append(f\"{k} \" + \", \".join(r(*ref) for ref in refs))\n        if expressions:\n            self.expressions[\"UpdateExpression\"] = \" \".join(e.strip() for e in expressions)\n\n    @property\n    def output(self):\n        \"\"\"The wire format for all conditions that have been rendered.\n        A new :class:`~bloop.conditions.ConditionRenderer` should be used for each operation.\"\"\"\n        expressions = {k: v for (k, v) in self.expressions.items() if v is not None}\n        if self.refs.attr_names:\n            expressions[\"ExpressionAttributeNames\"] = self.refs.attr_names\n        if self.refs.attr_values:\n            expressions[\"ExpressionAttributeValues\"] = self.refs.attr_values\n        return expressions\nbloop/types.py\nclass Boolean(Type):\n    python_type = bool\n    backing_type = BOOLEAN\n\n    def dynamo_load(self, value, *, context, **kwargs):\n        if value is None:\n            return None\n        return bool(value)\n\n    def dynamo_dump(self, value, *, context, **kwargs):\n        if value is None:\n            return None\n        return bool(value)\nbloop/types.py\nclass Timestamp(Integer):\n    \"\"\"Stores the unix (epoch) time in seconds.  Milliseconds are truncated to 0 on load and save.\n\n    Naive datetimes (``tzinfo is None``) are not supported, and trying to use one will raise ``ValueError``.\n\n    .. code-block:: python\n\n        from datetime import datetime, timedelta, timezone\n\n        class Model(Base):\n            id = Column(Integer, hash_key=True)\n            date = Column(Timestamp)\n        engine.bind()\n\n        obj = Model(id=1, date=datetime.now(timezone.utc))\n        engine.save(obj)\n\n        one_day_ago = datetime.now(timezone.utc) - timedelta(days=1)\n\n        query = engine.query(\n            Model,\n            key=Model.id==1,\n            filter=Model.date >= one_day_ago)\n\n        query.first().date\n\n    .. note::\n\n        To use common datetime libraries such as `arrow`_, `delorean`_, or `pendulum`_,\n        see :ref:`DateTime and Timestamp Extensions <user-extensions-datetime>` in the user guide.  These\n        are drop-in replacements and support non-utc timezones:\n\n        .. code-block:: python\n\n            from bloop import Timestamp  # becomes:\n            from bloop.ext.pendulum import Timestamp\n\n    .. _arrow: http://crsmithdev.com/arrow\n    .. _delorean: https://delorean.readthedocs.io/en/latest/\n    .. _pendulum: https://pendulum.eustace.io\n    \"\"\"\n    python_type = datetime.datetime\n\n    def dynamo_load(self, value, *, context, **kwargs):\n        if value is None:\n            return None\n        value = super().dynamo_load(value, context=context, **kwargs)\n        # we assume all stored values are utc, so we simply force timezone to utc\n        # without changing the day/time values\n        return datetime.datetime.fromtimestamp(value, tz=datetime.timezone.utc)\n\n    def dynamo_dump(self, value, *, context, **kwargs):\n        if value is None:\n            return None\n        if value.tzinfo is None:\n            raise ValueError(\n                \"naive datetime instances are not supported.  You can set a timezone with either \"\n                \"your_dt.replace(tzinfo=) or your_dt.astimezone(tz=).  WARNING: calling astimezone on a naive \"\n                \"datetime will assume the naive datetime is in the system's timezone, even though \"\n                \"datetime.utcnow() creates a naive object!  You almost certainly don't want to do that.\"\n            )\n        value = value.timestamp()\n        return super().dynamo_dump(value, context=context, **kwargs)\nbloop/types.py\nclass Integer(Number):\n    \"\"\"Truncates values when loading or dumping.\n\n    For example, ``3.14`` in DynamoDB is loaded as ``3``. If a value is ``7.5``\n    locally, it's stored in DynamoDB as ``7``.\n    \"\"\"\n    python_type = int\n\n    def dynamo_load(self, value, *, context, **kwargs):\n        if value is None:\n            return None\n        number = super().dynamo_load(value, context=context, **kwargs)\n        return int(number)\n\n    def dynamo_dump(self, value, *, context, **kwargs):\n        if value is None:\n            return None\n        value = int(value)\n        return super().dynamo_dump(value, context=context, **kwargs)\ntests/helpers/models.py\nclass VectorModel(BaseModel):\n    name = Column(String, hash_key=True)\n    list_str = Column(List(String))\n    set_str = Column(Set(String))\n    map_nested = Column(Map(**{\n        \"bytes\": Binary,\n        \"str\": String,\n        \"map\": Map(**{\n            \"int\": Integer,\n            \"str\": String\n        })\n    }))\n    some_int = Column(Integer)\n    some_bytes = Column(Binary)\nbloop/exceptions.py\nclass InvalidStream(BloopException, ValueError):\n    \"\"\"This is not a valid stream definition.\"\"\"\nbloop/types.py\nclass String(Type):\n    python_type = str\n    backing_type = STRING\n\n    def dynamo_load(self, value, *, context, **kwargs):\n        if not value:\n            return \"\"\n        return value\n\n    def dynamo_dump(self, value, *, context, **kwargs):\n        if not value:\n            return None\n        return value\nbloop/models.py\nclass IMeta:\nclass BaseModel:\n    class Meta(IMeta):\nclass Index:\nclass GlobalSecondaryIndex(Index):\nclass LocalSecondaryIndex(Index):\nclass Column(ComparisonMixin):\n    class UNBOUND:\n        class Meta(IMeta):\n    def __init__(self, **attrs):\n    def __init_subclass__(cls: type, **kwargs):\n    def __repr__(self):\n    def __init__(self, *, projection, hash_key=None, range_key=None, dynamo_name=None, **kwargs):\n    def __copy__(self):\n    def __set_name__(self, owner, name):\n    def __repr__(self):\n    def name(self):\n    def dynamo_name(self):\n    def hash_key(self):\n    def range_key(self):\n    def keys(self):\n    def __set__(self, obj, value):\n    def __get__(self, obj, type=None):\n    def __delete__(self, obj):\n    def __init__(\n            self, *, projection,\n            hash_key, range_key=None,\n            read_units=None, write_units=None,\n            dynamo_name=None, **kwargs):\n    def __init__(self, *, projection, range_key, dynamo_name=None, strict=True, **kwargs):\n    def hash_key(self):\n    def read_units(self):\n    def read_units(self, value):\n    def write_units(self):\n    def write_units(self, value):\n    def __init__(self, typedef, hash_key=False, range_key=False, dynamo_name=None, default=missing):\n    def __copy__(self):\n    def __set_name__(self, owner, name):\n    def __set__(self, obj, value):\n    def __get__(self, obj, type=None):\n    def __delete__(self, obj):\n    def __repr__(self):\n    def name(self):\n    def dynamo_name(self):\ndef subclassof(obj, classinfo):\ndef instanceof(obj, classinfo):\ndef loaded_columns(obj: BaseModel):\ndef unpack_from_dynamodb(*, attrs, expected, model=None, obj=None, engine=None, context=None, **kwargs):\ndef validate_projection(projection):\ndef validate_stream(meta):\ndef validate_encryption(meta):\ndef validate_backups(meta):\ndef validate_billing(meta):\ndef validate_ttl(meta):\ndef unbound_repr(obj):\ndef setdefault(obj, field, default):\ndef ensure_hash(cls) -> None:\ndef initialize_meta(cls: type):\ndef bind_column(model, name, column, force=False, recursive=False, copy=False) -> Column:\ndef bind_index(model, name, index, force=False, recursive=True, copy=False) -> Index:\ndef refresh_index(meta, index) -> None:\ndef unbind(meta, name=None, dynamo_name=None) -> None:\n", "answers": ["            index = LocalSecondaryIndex(range_key=\"id\", projection=\"keys\")"], "length": 2269, "dataset": "repobench-p_e", "language": "python", "all_classes": null, "_id": "1dc911da7eb7c3c6b8336d06b4112a69343babb1b98f436a"}
{"input": "import csv\nimport logging\nimport multiprocessing as mp\nimport os\nimport time\nimport tqdm\nfrom abc import abstractmethod\nfrom queue import Empty\nfrom typing import TYPE_CHECKING, Dict, List\nfrom montreal_forced_aligner.alignment.multiprocessing import (\n    AlignArguments,\n    AlignFunction,\n    CompileInformationArguments,\n    CompileTrainGraphsArguments,\n    CompileTrainGraphsFunction,\n    compile_information_func,\n)\nfrom montreal_forced_aligner.dictionary.mixins import DictionaryMixin\nfrom montreal_forced_aligner.utils import KaldiProcessWorker, Stopped, run_mp, run_non_mp\n    from montreal_forced_aligner.abc import MetaDict\n    from montreal_forced_aligner.corpus.multiprocessing import Job\n                                break\n                        else:\n                            break\n                        continue\n                    pbar.update(done + errors)\n                    error_sum += errors\n                for p in procs:\n                    p.join()\n                if error_dict:\n                    for v in error_dict.values():\n                        raise v\n            else:\n                self.logger.debug(\"Not using multiprocessing...\")\n                for args in arguments:\n                    function = CompileTrainGraphsFunction(args)\n                    for done, errors in function.run():\n                        pbar.update(done + errors)\n                        error_sum += errors\n        if error_sum:\n            self.logger.warning(\n                f\"Compilation of training graphs failed for {error_sum} utterances.\"\n            )\n        self.logger.debug(f\"Compiling training graphs took {time.time() - begin}\")\n\n    def align_utterances(self) -> None:\n        \"\"\"\n        Multiprocessing function that aligns based on the current model.\n\n        See Also\n        --------\n        :class:`~montreal_forced_aligner.alignment.multiprocessing.AlignFunction`\n            Multiprocessing helper function for each job\n        :meth:`.AlignMixin.align_arguments`\n            Job method for generating arguments for the helper function\n        :kaldi_steps:`align_si`\n            Reference Kaldi script\n        :kaldi_steps:`align_fmllr`\n            Reference Kaldi script\n        \"\"\"\n        begin = time.time()\n        self.unaligned_files = set()\n        self.logger.info(\"Generating alignments...\")\n        with tqdm.tqdm(total=self.num_utterances) as pbar:\n            if self.use_mp:\n                manager = mp.Manager()\n                error_dict = manager.dict()\n                return_queue = manager.Queue()\n                stopped = Stopped()\n                procs = []\n                for i, args in enumerate(self.align_arguments()):\n                    function = AlignFunction(args)\n                    p = KaldiProcessWorker(i, return_queue, function, error_dict, stopped)\n                    procs.append(p)\n                    p.start()\n                while True:\n                    try:\n                        utterance, log_likelihood = return_queue.get(timeout=1)\n                        if stopped.stop_check():\n                            continue\n                    except Empty:\n                        for proc in procs:\n                            if not proc.finished.stop_check():\n                                break\n                        else:\n                            break\n                        continue\n                    if hasattr(self, \"utterances\"):\n                        if hasattr(self, \"frame_shift\"):\n                            num_frames = int(\n                                self.utterances[utterance].duration * self.frame_shift\n                            )\n                        else:\n                            num_frames = self.utterances[utterance].duration\n                        self.utterances[utterance].alignment_log_likelihood = (\n                            log_likelihood / num_frames\n                        )\n                    pbar.update(1)\n                for p in procs:\n                    p.join()\n                if error_dict:\n                    for v in error_dict.values():\n                        raise v\n            else:\n                self.logger.debug(\"Not using multiprocessing...\")\n                for args in self.align_arguments():\n                    function = AlignFunction(args)\n                    for utterance, log_likelihood in function.run():\n                        if hasattr(self, \"utterances\"):\n                            if hasattr(self, \"frame_shift\"):\n                                num_frames = int(\n                                    self.utterances[utterance].duration * self.frame_shift\n                                )\n                            else:\n                                num_frames = self.utterances[utterance].duration\n                            self.utterances[utterance].alignment_log_likelihood = (\n                                log_likelihood / num_frames\n                            )\n                        pbar.update(1)\n\n        self.compile_information()\n        self.logger.debug(f\"Alignment round took {time.time() - begin}\")\n\n    def compile_information(self):\n        \"\"\"\n        Compiles information about alignment, namely what the overall log-likelihood was\n        and how many files were unaligned.\n\n        See Also\n        --------\n        :func:`~montreal_forced_aligner.alignment.multiprocessing.compile_information_func`\n            Multiprocessing helper function for each job\n        :meth:`.AlignMixin.compile_information_arguments`\n            Job method for generating arguments for the helper function\n        \"\"\"\n        compile_info_begin = time.time()\n\n        jobs = self.compile_information_arguments()\n\n        if self.use_mp:\n            alignment_info = run_mp(\n", "context": "montreal_forced_aligner/alignment/multiprocessing.py\nclass AlignFunction(KaldiFunction):\n    \"\"\"\n    Multiprocessing function for alignment.\n\n    See Also\n    --------\n    :meth:`.AlignMixin.align_utterances`\n        Main function that calls this function in parallel\n    :meth:`.AlignMixin.align_arguments`\n        Job method for generating arguments for this function\n    :kaldi_src:`align-equal-compiled`\n        Relevant Kaldi binary\n    :kaldi_src:`gmm-boost-silence`\n        Relevant Kaldi binary\n\n    Parameters\n    ----------\n    args: :class:`~montreal_forced_aligner.alignment.multiprocessing.AlignArguments`\n        Arguments for the function\n    \"\"\"\n\n    def __init__(self, args: AlignArguments):\n        self.log_path = args.log_path\n        self.dictionaries = args.dictionaries\n        self.fst_scp_paths = args.fst_scp_paths\n        self.feature_strings = args.feature_strings\n        self.model_path = args.model_path\n        self.ali_paths = args.ali_paths\n        self.align_options = args.align_options\n\n    def run(self):\n        \"\"\"Run the function\"\"\"\n        with open(self.log_path, \"w\", encoding=\"utf8\") as log_file:\n            for dict_name in self.dictionaries:\n                feature_string = self.feature_strings[dict_name]\n                fst_path = self.fst_scp_paths[dict_name]\n                ali_path = self.ali_paths[dict_name]\n                com = [\n                    thirdparty_binary(\"gmm-align-compiled\"),\n                    f\"--transition-scale={self.align_options['transition_scale']}\",\n                    f\"--acoustic-scale={self.align_options['acoustic_scale']}\",\n                    f\"--self-loop-scale={self.align_options['self_loop_scale']}\",\n                    f\"--beam={self.align_options['beam']}\",\n                    f\"--retry-beam={self.align_options['retry_beam']}\",\n                    \"--careful=false\",\n                    \"-\",\n                    f\"scp:{fst_path}\",\n                    feature_string,\n                    f\"ark:{ali_path}\",\n                    \"ark,t:-\",\n                ]\n\n                boost_proc = subprocess.Popen(\n                    [\n                        thirdparty_binary(\"gmm-boost-silence\"),\n                        f\"--boost={self.align_options['boost_silence']}\",\n                        self.align_options[\"optional_silence_csl\"],\n                        self.model_path,\n                        \"-\",\n                    ],\n                    stderr=log_file,\n                    stdout=subprocess.PIPE,\n                    env=os.environ,\n                )\n                align_proc = subprocess.Popen(\n                    com,\n                    stdout=subprocess.PIPE,\n                    stderr=log_file,\n                    encoding=\"utf8\",\n                    stdin=boost_proc.stdout,\n                    env=os.environ,\n                )\n                for line in align_proc.stdout:\n                    line = line.strip()\n                    utterance, log_likelihood = line.split()\n                    yield utterance, float(log_likelihood)\nmontreal_forced_aligner/dictionary/mixins.py\nclass DictionaryMixin:\n    \"\"\"\n    Abstract class for MFA classes that use acoustic models\n\n    Parameters\n    ----------\n    oov_code : str\n        What to label words not in the dictionary, defaults to ``'<unk>'``\n    position_dependent_phones : bool\n        Specifies whether phones should be represented as dependent on their\n        position in the word (beginning, middle or end), defaults to True\n    num_silence_states : int\n        Number of states to use for silence phones, defaults to 5\n    num_non_silence_states : int\n        Number of states to use for non-silence phones, defaults to 3\n    shared_silence_phones : bool\n        Specify whether to share states across all silence phones, defaults\n        to True\n    silence_probability : float\n        Probability of optional silences following words, defaults to 0.5\n    punctuation: str, optional\n        Punctuation to use when parsing text\n    clitic_markers: str, optional\n        Clitic markers to use when parsing text\n    compound_markers: str, optional\n        Compound markers to use when parsing text\n    brackets: list[tuple[str, str], optional\n        Character tuples to treat as full brackets around words\n    clitic_set: set[str]\n        Set of clitic words\n    disambiguation_symbols: set[str]\n        Set of disambiguation symbols\n    max_disambiguation_symbol: int\n        Maximum number of disambiguation symbols required, defaults to 0\n    \"\"\"\n\n    positions: List[str] = [\"_B\", \"_E\", \"_I\", \"_S\"]\n\n    def __init__(\n        self,\n        oov_word: str = \"<unk>\",\n        silence_word: str = \"<sil>\",\n        noise_word: str = \"<noise>\",\n        optional_silence_phone: str = \"sil\",\n        oov_phone: str = \"spn\",\n        other_noise_phone: str = \"noi\",\n        position_dependent_phones: bool = True,\n        num_silence_states: int = 5,\n        num_noise_states: int = 5,\n        num_non_silence_states: int = 3,\n        shared_silence_phones: bool = False,\n        ignore_case: bool = True,\n        silence_probability: float = 0.5,\n        punctuation: List[str] = None,\n        clitic_markers: List[str] = None,\n        compound_markers: List[str] = None,\n        brackets: List[Tuple[str, str]] = None,\n        non_silence_phones: Set[str] = None,\n        disambiguation_symbols: Set[str] = None,\n        clitic_set: Set[str] = None,\n        max_disambiguation_symbol: int = 0,\n        phone_set_type: typing.Union[str, PhoneSetType] = \"UNKNOWN\",\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.punctuation = DEFAULT_PUNCTUATION\n        self.clitic_markers = DEFAULT_CLITIC_MARKERS\n        self.compound_markers = DEFAULT_COMPOUND_MARKERS\n        self.brackets = DEFAULT_BRACKETS\n        if punctuation is not None:\n            self.punctuation = punctuation\n        if clitic_markers is not None:\n            self.clitic_markers = clitic_markers\n        if compound_markers is not None:\n            self.compound_markers = compound_markers\n        if brackets is not None:\n            self.brackets = brackets\n\n        self.num_silence_states = num_silence_states\n        self.num_noise_states = num_noise_states\n        self.num_non_silence_states = num_non_silence_states\n        self.shared_silence_phones = shared_silence_phones\n        self.silence_probability = silence_probability\n        self.ignore_case = ignore_case\n        self.oov_word = oov_word\n        self.silence_word = silence_word\n        self.noise_word = noise_word\n        self.position_dependent_phones = position_dependent_phones\n        self.optional_silence_phone = optional_silence_phone\n        self.oov_phone = oov_phone\n        self.oovs_found = Counter()\n        self.other_noise_phone = other_noise_phone\n        if non_silence_phones is None:\n            non_silence_phones = set()\n        self.non_silence_phones = non_silence_phones\n        self.excluded_phones = set()\n        self.excluded_pronunciation_count = 0\n        self.max_disambiguation_symbol = max_disambiguation_symbol\n        if disambiguation_symbols is None:\n            disambiguation_symbols = set()\n        self.disambiguation_symbols = disambiguation_symbols\n        if clitic_set is None:\n            clitic_set = set()\n        self.clitic_set = clitic_set\n        if not isinstance(phone_set_type, PhoneSetType):\n            phone_set_type = PhoneSetType[phone_set_type]\n        self.phone_set_type = phone_set_type\n\n    @property\n    def base_phones(self) -> Dict[str, Set[str]]:\n        base_phones = {}\n        for p in self.non_silence_phones:\n            b = self.phone_set_type.get_base_phone(p)\n            if b not in base_phones:\n                base_phones[b] = set()\n            base_phones[b].add(p)\n\n        return base_phones\n\n    @property\n    def split_regex(self) -> re.Pattern:\n        \"\"\"Pattern for splitting arbitrary text\"\"\"\n        markers = self.compound_markers\n        if \"-\" in markers:\n            markers = [\"-\"] + [x for x in self.compound_markers if x != \"-\"]\n        return re.compile(rf'[{\"\".join(markers)} ]')\n\n    @property\n    def extra_questions_mapping(self) -> Dict[str, List[str]]:\n        \"\"\"Mapping of extra questions for the given phone set type\"\"\"\n        mapping = {}\n        mapping[\"silence_question\"] = []\n        for p in sorted(self.silence_phones):\n            mapping[\"silence_question\"].append(p)\n            if self.position_dependent_phones:\n                mapping[\"silence_question\"].extend([p + x for x in self.positions])\n        for k, v in self.phone_set_type.extra_questions.items():\n            if k not in mapping:\n                mapping[k] = []\n            if self.phone_set_type is PhoneSetType.ARPA:\n                if self.position_dependent_phones:\n                    for x in sorted(v):\n                        mapping[k].extend([x + pos for pos in self.positions])\n                else:\n                    mapping[k] = sorted(v)\n            elif self.phone_set_type == PhoneSetType.IPA:\n                filtered_v = set()\n                for x in self.non_silence_phones:\n                    base_phone = self.phone_set_type.get_base_phone(x)\n                    if base_phone in v:\n                        filtered_v.add(x)\n                if len(filtered_v) < 2:\n                    del mapping[k]\n                    continue\n                if self.position_dependent_phones:\n                    for x in sorted(filtered_v):\n                        mapping[k].extend([x + pos for pos in self.positions])\n                else:\n                    mapping[k] = sorted(filtered_v)\n            elif self.phone_set_type is PhoneSetType.PINYIN:\n                filtered_v = set()\n                for x in self.non_silence_phones:\n                    base_phone = self.phone_set_type.get_base_phone(x)\n                    if base_phone in v or x in v:\n                        filtered_v.add(x)\n                    elif x in v:\n                        filtered_v.add(x)\n                if len(filtered_v) < 2:\n                    del mapping[k]\n                    continue\n                if self.position_dependent_phones:\n                    for x in sorted(filtered_v):\n                        mapping[k].extend([x + pos for pos in self.positions])\n                else:\n                    mapping[k] = sorted(filtered_v)\n        if self.position_dependent_phones:\n            phones = sorted(self.non_silence_phones)\n            for pos in self.positions:\n                mapping[f\"non_silence{pos}\"] = [x + pos for x in phones]\n            silence_phones = sorted(self.silence_phones)\n            for pos in [\"\"] + self.positions:\n                mapping[f\"silence{pos}\"] = [x + pos for x in silence_phones]\n        return mapping\n\n    @property\n    def dictionary_options(self) -> MetaDict:\n        \"\"\"Dictionary options\"\"\"\n        return {\n            \"punctuation\": self.punctuation,\n            \"clitic_markers\": self.clitic_markers,\n            \"clitic_set\": self.clitic_set,\n            \"compound_markers\": self.compound_markers,\n            \"brackets\": self.brackets,\n            \"num_silence_states\": self.num_silence_states,\n            \"num_non_silence_states\": self.num_non_silence_states,\n            \"shared_silence_phones\": self.shared_silence_phones,\n            \"silence_probability\": self.silence_probability,\n            \"oov_word\": self.oov_word,\n            \"silence_word\": self.silence_word,\n            \"position_dependent_phones\": self.position_dependent_phones,\n            \"optional_silence_phone\": self.optional_silence_phone,\n            \"oov_phone\": self.oov_phone,\n            \"other_noise_phone\": self.other_noise_phone,\n            \"non_silence_phones\": self.non_silence_phones,\n            \"max_disambiguation_symbol\": self.max_disambiguation_symbol,\n            \"disambiguation_symbols\": self.disambiguation_symbols,\n            \"phone_set_type\": str(self.phone_set_type),\n        }\n\n    @property\n    def silence_phones(self):\n        \"\"\"Silence phones\"\"\"\n        return {\n            self.optional_silence_phone,\n            self.oov_phone,\n            self.other_noise_phone,\n        }\n\n    @property\n    def context_independent_csl(self):\n        \"\"\"Context independent colon-separated list\"\"\"\n        return \":\".join(str(self.phone_mapping[x]) for x in self.silence_phones)\n\n    @property\n    def specials_set(self):\n        \"\"\"Special words, like the ``oov_word`` ``silence_word``, ``<eps>``, ``<s>``, and ``</s>``\"\"\"\n        return {self.oov_word, self.silence_word, self.noise_word, \"<eps>\", \"<s>\", \"</s>\"}\n\n    @property\n    def phone_mapping(self) -> Dict[str, int]:\n        \"\"\"Mapping of phones to integer IDs\"\"\"\n        phone_mapping = {}\n        i = 0\n        phone_mapping[\"<eps>\"] = i\n        for p in self.kaldi_silence_phones:\n            i += 1\n            phone_mapping[p] = i\n        for p in self.kaldi_non_silence_phones:\n            i += 1\n            phone_mapping[p] = i\n        i = max(phone_mapping.values())\n        for x in range(self.max_disambiguation_symbol + 2):\n            p = f\"#{x}\"\n            self.disambiguation_symbols.add(p)\n            i += 1\n            phone_mapping[p] = i\n        return phone_mapping\n\n    @property\n    def reversed_phone_mapping(self) -> ReversedMappingType:\n        \"\"\"\n        A mapping of integer ids to phones\n        \"\"\"\n        mapping = {}\n        for k, v in self.phone_mapping.items():\n            mapping[v] = k\n        return mapping\n\n    @property\n    def positional_silence_phones(self) -> List[str]:\n        \"\"\"\n        List of silence phones with positions\n        \"\"\"\n        silence_phones = []\n        for p in sorted(self.silence_phones):\n            silence_phones.append(p)\n            for pos in self.positions:\n                silence_phones.append(p + pos)\n        return silence_phones\n\n    def _generate_positional_list(self, phones: Set[str]) -> List[str]:\n        \"\"\"\n        Helper function to generate positional list for phones along with any base phones for the phone set\n\n        Parameters\n        ----------\n        phones: set[str]\n            Set of phones\n\n        Returns\n        -------\n        list[str]\n            List of positional phones, sorted by base phone\n        \"\"\"\n        positional_phones = []\n        for p in sorted(phones):\n            if p not in self.non_silence_phones:\n                continue\n            base_phone = self.phone_set_type.get_base_phone(p)\n            for pos in self.positions:\n                pos_p = base_phone + pos\n                if pos_p not in positional_phones:\n                    positional_phones.append(pos_p)\n            for pos in self.positions:\n                pos_p = p + pos\n                if pos_p not in positional_phones:\n                    positional_phones.append(pos_p)\n        return positional_phones\n\n    def _generate_non_positional_list(self, phones: Set[str]) -> List[str]:\n        \"\"\"\n        Helper function to generate non-positional list for phones with any base phones for the phone set\n\n        Parameters\n        ----------\n        phones: set[str]\n            Set of phones\n\n        Returns\n        -------\n        list[str]\n            List of non-positional phones, sorted by base phone\n        \"\"\"\n        base_phones = set()\n        for p in phones:\n            base_phone = self.phone_set_type.get_base_phone(p)\n            base_phones.add(base_phone)\n\n        return sorted(phones | base_phones)\n\n    def _generate_phone_list(self, phones: Set[str]) -> List[str]:\n        \"\"\"\n        Helper function to generate phone list\n\n        Parameters\n        ----------\n        phones: set[str]\n            Set of phones\n\n        Returns\n        -------\n        list[str]\n            List of positional or non-positional phones, sorted by base phone\n        \"\"\"\n        if self.position_dependent_phones:\n            return self._generate_positional_list(phones)\n        return self._generate_non_positional_list(phones)\n\n    @property\n    def positional_non_silence_phones(self) -> List[str]:\n        \"\"\"\n        List of non-silence phones with positions\n        \"\"\"\n        return self._generate_positional_list(self.non_silence_phones)\n\n    @property\n    def kaldi_non_silence_phones(self):\n        \"\"\"Non silence phones in Kaldi format\"\"\"\n        if self.position_dependent_phones:\n            return self.positional_non_silence_phones\n        return self._generate_non_positional_list(self.non_silence_phones)\n\n    @property\n    def kaldi_phones_for_topo(self):\n        \"\"\"Mappings of phones for generating topo file\"\"\"\n        mapping = {}\n        for p in sorted(self.non_silence_phones):\n            base_phone = self.phone_set_type.get_base_phone(p)\n            query_set = {p, base_phone}\n            if any(x in self.phone_set_type.extra_short_phones for x in query_set):\n                num_states = 1  # One state for extra short sounds\n            elif any(x in self.phone_set_type.diphthong_phones for x in query_set):\n                num_states = 5  # 5 states for diphthongs (onset of first target, steady state,\n                # transition to next target, steady state, offset of second target)\n            elif any(x in self.phone_set_type.triphthong_phones for x in query_set):\n                num_states = 6  # 5 states for diphthongs (onset of first target, steady state,\n                # transition to next target, steady state, offset of second target)\n            elif any(x in self.phone_set_type.affricate_phones for x in query_set):\n                num_states = 4  # 4 states for affricates (closure, burst, onset of frication, offset of frication)\n            elif any(x in self.phone_set_type.stop_phones for x in query_set):\n                num_states = 2  # Two states for stops (closure, burst), extra states added below for aspirated, ejectives\n            else:\n                num_states = self.num_non_silence_states\n            if self.phone_set_type is PhoneSetType.IPA:\n                if re.match(r\"^.*[ʱʼʰʲʷⁿˠ][ː]?$\", p):\n                    num_states += 1\n                if re.match(r\"^.*̚$\", p) and p not in self.phone_set_type.extra_short_phones:\n                    num_states -= 1\n            elif self.phone_set_type is PhoneSetType.PINYIN:\n                if p in {\"c\", \"ch\", \"q\"}:\n                    num_states += 1\n            if num_states not in mapping:\n                mapping[num_states] = []\n            mapping[num_states].extend(\n                [x for x in self._generate_phone_list({p}) if x not in mapping[num_states]]\n            )\n        if self.phone_set_type is PhoneSetType.ARPA:\n            mapping[1] = [x for x in mapping[1] if \"0\" in x]\n        return mapping\n\n    @property\n    def kaldi_grouped_phones(self) -> Dict[str, List[str]]:\n        \"\"\"Non silence phones in Kaldi format\"\"\"\n        groups = {}\n        for p in sorted(self.non_silence_phones):\n\n            base_phone = self.phone_set_type.get_base_phone(p)\n            if base_phone not in groups:\n                if self.position_dependent_phones:\n                    groups[base_phone] = [base_phone + pos for pos in self.positions]\n                else:\n                    groups[base_phone] = [base_phone]\n            if self.position_dependent_phones:\n                groups[base_phone].extend(\n                    [p + pos for pos in self.positions if p + pos not in groups[base_phone]]\n                )\n            else:\n                if p not in groups[base_phone]:\n                    groups[base_phone].append(p)\n        return groups\n\n    @property\n    def kaldi_silence_phones(self):\n        \"\"\"Silence phones in Kaldi format\"\"\"\n        if self.position_dependent_phones:\n            return self.positional_silence_phones\n        return sorted(self.silence_phones)\n\n    def save_oovs_found(self, directory: str) -> None:\n        \"\"\"\n        Save all out of vocabulary items to a file in the specified directory\n\n        Parameters\n        ----------\n        directory : str\n            Path to directory to save ``oovs_found.txt``\n        \"\"\"\n        with open(os.path.join(directory, \"oovs_found.txt\"), \"w\", encoding=\"utf8\") as f, open(\n            os.path.join(directory, \"oov_counts.txt\"), \"w\", encoding=\"utf8\"\n        ) as cf:\n            for oov in sorted(self.oovs_found.keys(), key=lambda x: (-self.oovs_found[x], x)):\n                f.write(oov + \"\\n\")\n                cf.write(f\"{oov}\\t{self.oovs_found[oov]}\\n\")\n\n    @property\n    def optional_silence_csl(self) -> str:\n        \"\"\"\n        Phone ID of the optional silence phone\n        \"\"\"\n        return str(self.phone_mapping[self.optional_silence_phone])\n\n    @property\n    def silence_csl(self) -> str:\n        \"\"\"\n        A colon-separated string of silence phone ids\n        \"\"\"\n        return \":\".join(map(str, (self.phone_mapping[x] for x in self.kaldi_silence_phones)))\n\n    @property\n    def non_silence_csl(self) -> str:\n        \"\"\"\n        A colon-separated string of non-silence phone ids\n        \"\"\"\n        return \":\".join(map(str, (self.phone_mapping[x] for x in self.kaldi_non_silence_phones)))\n\n    @property\n    def phones(self) -> set:\n        \"\"\"\n        The set of all phones (silence and non-silence)\n        \"\"\"\n        return self.silence_phones | self.non_silence_phones\n\n    def check_bracketed(self, word: str) -> bool:\n        \"\"\"\n        Checks whether a given string is surrounded by brackets.\n\n        Parameters\n        ----------\n        word : str\n            Text to check for final brackets\n\n        Returns\n        -------\n        bool\n            True if the word is fully bracketed, false otherwise\n        \"\"\"\n        for b in self.brackets:\n            if re.match(rf\"^{re.escape(b[0])}.*{re.escape(b[1])}$\", word):\n                return True\n        return False\n\n    def construct_sanitize_function(self) -> SanitizeFunction:\n        \"\"\"\n        Construct a :class:`~montreal_forced_aligner.dictionary.mixins.SanitizeFunction` to use in multiprocessing jobs\n\n        Returns\n        -------\n        :class:`~montreal_forced_aligner.dictionary.mixins.SanitizeFunction`\n            Function for sanitizing text\n        \"\"\"\n        f = SanitizeFunction(\n            self.punctuation, self.clitic_markers, self.compound_markers, self.brackets\n        )\n\n        return f\n\n    def sanitize(self, item: str) -> str:\n        \"\"\"\n        Sanitize an item according to punctuation and clitic markers\n\n        Parameters\n        ----------\n        item: str\n            Word to sanitize\n\n        Returns\n        -------\n        str\n            Sanitized form\n        \"\"\"\n        return self.construct_sanitize_function()(item)\nmontreal_forced_aligner/alignment/multiprocessing.py\nclass CompileTrainGraphsFunction(KaldiFunction):\n    \"\"\"\n    Multiprocessing function to compile training graphs\n\n    See Also\n    --------\n    :meth:`.AlignMixin.compile_train_graphs`\n        Main function that calls this function in parallel\n    :meth:`.AlignMixin.compile_train_graphs_arguments`\n        Job method for generating arguments for this function\n    :kaldi_src:`compile-train-graphs`\n        Relevant Kaldi binary\n\n    Parameters\n    ----------\n    args: :class:`~montreal_forced_aligner.alignment.multiprocessing.CompileTrainGraphsArguments`\n        Arguments for the function\n    \"\"\"\n\n    progress_pattern = re.compile(\n        r\"^LOG.*succeeded for (?P<succeeded>\\d+) graphs, failed for (?P<failed>\\d+)\"\n    )\n\n    def __init__(self, args: CompileTrainGraphsArguments):\n        self.log_path = args.log_path\n        self.dictionaries = args.dictionaries\n        self.tree_path = args.tree_path\n        self.model_path = args.model_path\n        self.text_int_paths = args.text_int_paths\n        self.disambig_path = args.disambig_path\n        self.lexicon_fst_paths = args.lexicon_fst_paths\n        self.fst_scp_paths = args.fst_scp_paths\n\n    def run(self):\n        with open(self.log_path, \"w\", encoding=\"utf8\") as log_file:\n            for dict_name in self.dictionaries:\n                fst_scp_path = self.fst_scp_paths[dict_name]\n                fst_ark_path = fst_scp_path.replace(\".scp\", \".ark\")\n                text_path = self.text_int_paths[dict_name]\n                proc = subprocess.Popen(\n                    [\n                        thirdparty_binary(\"compile-train-graphs\"),\n                        f\"--read-disambig-syms={self.disambig_path}\",\n                        self.tree_path,\n                        self.model_path,\n                        self.lexicon_fst_paths[dict_name],\n                        f\"ark:{text_path}\",\n                        f\"ark,scp:{fst_ark_path},{fst_scp_path}\",\n                    ],\n                    stderr=subprocess.PIPE,\n                    encoding=\"utf8\",\n                    env=os.environ,\n                )\n                for line in proc.stderr:\n                    log_file.write(line)\n                    m = self.progress_pattern.match(line.strip())\n                    if m:\n                        yield int(m.group(\"succeeded\")), int(m.group(\"failed\"))\nmontreal_forced_aligner/alignment/multiprocessing.py\nclass AlignArguments(NamedTuple):\n    \"\"\"Arguments for :class:`~montreal_forced_aligner.alignment.multiprocessing.AlignFunction`\"\"\"\n\n    log_path: str\n    dictionaries: List[str]\n    fst_scp_paths: Dict[str, str]\n    feature_strings: Dict[str, str]\n    model_path: str\n    ali_paths: Dict[str, str]\n    align_options: MetaDict\nmontreal_forced_aligner/alignment/multiprocessing.py\nclass CompileTrainGraphsArguments(NamedTuple):\n    \"\"\"Arguments for :class:`~montreal_forced_aligner.alignment.multiprocessing.CompileTrainGraphsFunction`\"\"\"\n\n    log_path: str\n    dictionaries: List[str]\n    tree_path: str\n    model_path: str\n    text_int_paths: Dict[str, str]\n    disambig_path: str\n    lexicon_fst_paths: Dict[str, str]\n    fst_scp_paths: Dict[str, str]\nmontreal_forced_aligner/utils.py\ndef run_mp(\n    function: Callable,\n    argument_list: List[Tuple[Any, ...]],\n    log_directory: str,\n    return_info: bool = False,\n) -> Optional[Dict[int, Any]]:\n    \"\"\"\n    Apply a function for each job in parallel\n\n    Parameters\n    ----------\n    function: Callable\n        Multiprocessing function to apply\n    argument_list: list\n        List of arguments for each job\n    log_directory: str\n        Directory that all log information from the processes goes to\n    return_info: dict, optional\n        If the function returns information, supply the return dict to populate\n    \"\"\"\n    from .config import BLAS_THREADS\n\n    os.environ[\"OPENBLAS_NUM_THREADS\"] = f\"{BLAS_THREADS}\"\n    os.environ[\"MKL_NUM_THREADS\"] = f\"{BLAS_THREADS}\"\n    stopped = Stopped()\n    manager = mp.Manager()\n    job_queue = manager.Queue()\n    return_dict = manager.dict()\n    info = None\n    if return_info:\n        info = manager.dict()\n    for a in argument_list:\n        job_queue.put(a)\n    procs = []\n    for i in range(len(argument_list)):\n        p = ProcessWorker(i, job_queue, function, return_dict, stopped, info)\n        procs.append(p)\n        p.start()\n\n    for p in procs:\n        p.join()\n    if \"error\" in return_dict:\n        _, exc = return_dict[\"error\"]\n        raise exc\n\n    parse_logs(log_directory)\n    if return_info:\n        return info\nmontreal_forced_aligner/alignment/multiprocessing.py\nclass CompileInformationArguments(NamedTuple):\n    \"\"\"Arguments for :func:`~montreal_forced_aligner.alignment.multiprocessing.compile_information_func`\"\"\"\n\n    align_log_paths: str\nmontreal_forced_aligner/utils.py\nclass Stopped(object):\n    \"\"\"\n    Multiprocessing class for detecting whether processes should stop processing and exit ASAP\n\n    Attributes\n    ----------\n    val: :func:`~multiprocessing.Value`\n        0 if not stopped, 1 if stopped\n    lock: :class:`~multiprocessing.Lock`\n        Lock for process safety\n    _source: multiprocessing.Value\n        1 if it was a Ctrl+C event that stopped it, 0 otherwise\n    \"\"\"\n\n    def __init__(self, initval: Union[bool, int] = False):\n        self.val = mp.Value(\"i\", initval)\n        self.lock = mp.Lock()\n        self._source = mp.Value(\"i\", 0)\n\n    def stop(self) -> None:\n        \"\"\"Signal that work should stop asap\"\"\"\n        with self.lock:\n            self.val.value = True\n\n    def stop_check(self) -> int:\n        \"\"\"Check whether a process should stop\"\"\"\n        with self.lock:\n            return self.val.value\n\n    def set_sigint_source(self) -> None:\n        \"\"\"Set the source as a ctrl+c\"\"\"\n        with self.lock:\n            self._source.value = True\n\n    def source(self) -> int:\n        \"\"\"Get the source value\"\"\"\n        with self.lock:\n            return self._source.value\nmontreal_forced_aligner/utils.py\ndef run_non_mp(\n    function: Callable,\n    argument_list: List[Tuple[Any, ...]],\n    log_directory: str,\n    return_info: bool = False,\n) -> Optional[Dict[Any, Any]]:\n    \"\"\"\n    Similar to :func:`run_mp`, but no additional processes are used and the jobs are evaluated in sequential order\n\n    Parameters\n    ----------\n    function: Callable\n        Multiprocessing function to evaluate\n    argument_list: list\n        List of arguments to process\n    log_directory: str\n        Directory that all log information from the processes goes to\n    return_info: dict, optional\n        If the function returns information, supply the return dict to populate\n\n    Returns\n    -------\n    dict, optional\n        If the function returns information, returns the dictionary it was supplied with\n    \"\"\"\n    if return_info:\n        info = {}\n        for i, args in enumerate(argument_list):\n            info[i] = function(*args)\n        parse_logs(log_directory)\n        return info\n\n    for args in argument_list:\n        function(*args)\n    parse_logs(log_directory)\nmontreal_forced_aligner/alignment/multiprocessing.py\ndef compile_information_func(align_log_path: str) -> Dict[str, Union[List[str], float, int]]:\n    \"\"\"\n    Multiprocessing function for compiling information about alignment\n\n    See Also\n    --------\n    :meth:`.AlignMixin.compile_information`\n        Main function that calls this function in parallel\n\n    Parameters\n    ----------\n    align_log_path: str\n        Log path for alignment\n\n    Returns\n    -------\n    dict[str, Union[list[str], float, int]]\n        Information about log-likelihood and number of unaligned files\n    \"\"\"\n    average_logdet_pattern = re.compile(\n        r\"Overall average logdet is (?P<logdet>[-.,\\d]+) over (?P<frames>[.\\d+e]+) frames\"\n    )\n    log_like_pattern = re.compile(\n        r\"^LOG .* Overall log-likelihood per frame is (?P<log_like>[-0-9.]+) over (?P<frames>\\d+) frames.*$\"\n    )\n\n    decode_error_pattern = re.compile(\n        r\"^WARNING .* Did not successfully decode file (?P<utt>.*?), .*$\"\n    )\n\n    data = {\"unaligned\": [], \"too_short\": [], \"log_like\": 0, \"total_frames\": 0}\n    with open(align_log_path, \"r\", encoding=\"utf8\") as f:\n        for line in f:\n            decode_error_match = re.match(decode_error_pattern, line)\n            if decode_error_match:\n                data[\"unaligned\"].append(decode_error_match.group(\"utt\"))\n                continue\n            log_like_match = re.match(log_like_pattern, line)\n            if log_like_match:\n                log_like = log_like_match.group(\"log_like\")\n                frames = log_like_match.group(\"frames\")\n                data[\"log_like\"] = float(log_like)\n                data[\"total_frames\"] = int(frames)\n            m = re.search(average_logdet_pattern, line)\n            if m:\n                logdet = float(m.group(\"logdet\"))\n                frames = float(m.group(\"frames\"))\n                data[\"logdet\"] = logdet\n                data[\"logdet_frames\"] = frames\n    return data\nmontreal_forced_aligner/utils.py\nclass KaldiProcessWorker(mp.Process):\n    \"\"\"\n    Multiprocessing function work\n\n    Parameters\n    ----------\n    job_name: int\n        Integer number of job\n    job_q: :class:`~multiprocessing.Queue`\n        Job queue to pull arguments from\n    function: KaldiFunction\n        Multiprocessing function to call on arguments from job_q\n    return_dict: dict\n        Dictionary for collecting errors\n    stopped: :class:`~montreal_forced_aligner.utils.Stopped`\n        Stop check\n    return_info: dict[int, Any], optional\n        Optional dictionary to fill if the function should return information to main thread\n    \"\"\"\n\n    def __init__(\n        self,\n        job_name: int,\n        return_q: mp.Queue,\n        function: KaldiFunction,\n        error_dict: dict,\n        stopped: Stopped,\n    ):\n        mp.Process.__init__(self)\n        self.job_name = job_name\n        self.function = function\n        self.return_q = return_q\n        self.error_dict = error_dict\n        self.stopped = stopped\n        self.finished = Stopped()\n\n    def run(self) -> None:\n        \"\"\"\n        Run through the arguments in the queue apply the function to them\n        \"\"\"\n        try:\n            for result in self.function.run():\n                self.return_q.put(result)\n        except Exception:\n            self.stopped.stop()\n            self.error_dict[self.job_name] = Exception(traceback.format_exception(*sys.exc_info()))\n        finally:\n            self.finished.stop()\n", "answers": ["                compile_information_func, jobs, self.working_log_directory, True"], "length": 3046, "dataset": "repobench-p_e", "language": "python", "all_classes": null, "_id": "0c717f4722b891e7a785825ab5c74a953f360b5849e9620b"}
{"input": "import logging\nimport abc\nimport functools\nfrom collections import namedtuple\nfrom six.moves import range, zip\nfrom pybufrkit.constants import (DEFAULT_TABLES_DIR,\n                                 UNITS_CODE_TABLE,\n                                 UNITS_FLAG_TABLE,\n                                 UNITS_STRING)\nfrom pybufrkit.errors import PyBufrKitError, UnknownDescriptor\nfrom pybufrkit.bufr import SectionConfigurer\nfrom pybufrkit.descriptors import (ElementDescriptor,\n                                   FixedReplicationDescriptor,\n                                   DelayedReplicationDescriptor,\n                                   OperatorDescriptor,\n                                   SequenceDescriptor,\n                                   AssociatedDescriptor,\n                                   MarkerDescriptor,\n                                   SkippedLocalDescriptor)\n        # Lastly, get all the descriptors that has a corresponding Zero bit value\n        self.bitmapped_descriptors = [\n            (idx, d) for bit, (idx, d) in zip(\n                bitmap,\n                self.back_referenced_descriptors\n            ) if bit == 0\n        ]\n        self.next_bitmapped_descriptor = functools.partial(next, iter(self.bitmapped_descriptors))\n\n    def _assert_equal_values_of_index(self, idx):\n        \"\"\"\n        Assert that the values of the specified index are identical for all\n        subsets. It is only used for compressed data. For an example, to ensure\n        the delayed replication factors are the same for all subsets.\n        \"\"\"\n        minv, maxv = CoderState.minmax([values[idx] for values in self.decoded_values_all_subsets])\n        assert minv == maxv, 'Values from all subsets are NOT identical'\n\n    @staticmethod\n    def minmax(values):\n        \"\"\"\n        Give a list of values, find out the minimum and maximum, ignore any Nones.\n        \"\"\"\n        mn, mx = None, None\n        for v in values:\n            if v is not None:\n                if mn is None or mn > v:\n                    mn = v\n                if mx is None or mx < v:\n                    mx = v\n        return mn, mx\n\n\nclass Coder(object):\n    \"\"\"\n    This class is an abstract superclass for Decoder and Encoder. By itself it\n    cannot do anything. But it provides common operations for subclasses.\n\n    :param definitions_dir: Where to find the BPCL definition files.\n    :param tables_root_dir: Where to find the BUFR table files.\n    \"\"\"\n\n    def __init__(self,\n                 definitions_dir=None,\n                 tables_root_dir=None):\n\n        self.section_configurer = SectionConfigurer(definitions_dir=definitions_dir)\n        self.tables_root_dir = tables_root_dir or DEFAULT_TABLES_DIR\n\n    @abc.abstractmethod\n    def process(self, *args, **kwargs):\n        \"\"\"Entry point of the class\"\"\"\n\n    @abc.abstractmethod\n    def process_section(self, bufr_message, bit_operator, section):\n        \"\"\"\n        Process the given section of a BUFR message\n\n        :param bufr_message: The BufrMessage object to process\n        :param bit_operator: The bit operator (reader or writer)\n        :param section:\n        \"\"\"\n\n    def process_template(self, state, bit_operator, template):\n        \"\"\"\n        Process the top level BUFR Template\n\n        :param state: The state of the processing.\n        :param bit_operator: The bit operator for read/write bits.\n        :param template: The BUFR Template of the message.\n        \"\"\"\n        self.process_members(state, bit_operator, template.members)\n\n    def process_members(self, state, bit_operator, members):\n        \"\"\"\n        Process a list of descriptors that are members of a composite descriptor.\n\n        :param state: The state of the processing.\n        :param bit_operator: The bit operator for read/write bits.\n        :param members: A list of descriptors.\n        \"\"\"\n        for member in members:\n            member_type = type(member)\n\n            log.debug('Processing {} {}'.format(member, member.name if hasattr(member, 'name') else ''))\n\n            # TODO: NOT using if-elif for following checks because they may co-exist???\n            #      It is highly unlikely if not impossible\n\n            # 221 YYY data not present for following YYY descriptors except class 0-9 and 31\n            if state.data_not_present_count:\n                state.data_not_present_count -= 1\n                log.debug('Data not present: {} to go'.format(state.data_not_present_count))\n\n                if member_type is ElementDescriptor:\n                    X = member.X\n                    if not (1 <= X <= 9 or X == 31):  # skipping\n                        continue\n                        # TODO: maybe the descriptor should still be kept and set its value to None?\n                        #       So it helps to keep the structure intact??\n\n            # Currently defining new reference values\n            # For ElementDescriptor only. This makes sense though not explicitly stated in the manual\n            if state.nbits_of_new_refval and member_type is ElementDescriptor:\n                self.process_define_new_refval(state, bit_operator, member)\n                continue\n\n            # 206 YYY signify data width for local descriptor\n            if state.nbits_of_skipped_local_descriptor:\n                self.process_skipped_local_descriptor(state, bit_operator, member)\n                continue\n\n            # Currently defining new bitmap\n            if state.bitmap_definition_state != BITMAP_NA:\n                self.process_bitmap_definition(state, bit_operator, member)\n\n            # Now process normally\n            if member_type is ElementDescriptor:\n                self.process_element_descriptor(state, bit_operator, member)\n\n", "context": "pybufrkit/descriptors.py\nclass ElementDescriptor(Descriptor):\n    \"\"\"\n    Element Descriptor 0XXYYY\n\n    :param int id_: The descriptor ID\n    :param str name: Name of the descriptor\n    :param str unit: Units of the descriptor\n    :param int scale: Scale factor of the descriptor value\n    :param int refval: Reference value of the descriptor value\n    :param int nbits: The number of bits used by the descriptor\n    :param str crex_unit: Units of the descriptor for CREX spec\n    :param int crex_scale: Scale factor of the descriptor value for CREX Spec\n    :param int crex_nchars: Number of characters used by the descriptor for CREX Spec\n    \"\"\"\n\n    def __init__(self, id_, name, unit, scale, refval, nbits,\n                 crex_unit, crex_scale, crex_nchars):\n        super(ElementDescriptor, self).__init__(id_)\n        self.name, self.unit, self.scale, self.refval, self.nbits = (\n            name, unit, scale, refval, nbits\n        )\n        self.crex_unit, self.crex_scale, self.crex_nchars = crex_unit, crex_scale, crex_nchars\n\n    def as_list(self):\n        return [self.id, self.name, self.unit, self.scale, self.refval, self.nbits]\n\n    def as_dict(self):\n        return {self.id: (self.name, self.unit, self.scale, self.refval, self.nbits)}\n\n    def accept(self, visitor):\n        visitor.visit_element_descriptor(self)\npybufrkit/descriptors.py\nclass FixedReplicationDescriptor(ReplicationDescriptor):\n    \"\"\"\n    Fixed replication Descriptor 1XXYYY\n    \"\"\"\n\n    def __init__(self, id_, members=None):\n        super(FixedReplicationDescriptor, self).__init__(id_, members)\n\n    @property\n    def n_repeats(self):\n        \"\"\"\n        Number of times to perform the replication. This value is decoded\n        directly from the descriptor ID.\n        \"\"\"\n        return self.id % 1000\n\n    def accept(self, visitor):\n        visitor.visit_replication_descriptor(self)\n        if not self.members:\n            visitor.visit_replication_descriptor_end(self)\n            return\n        for member in self.members:\n            member.accept(visitor)\n        visitor.visit_replication_descriptor_end(self)\npybufrkit/constants.py\nUNITS_CODE_TABLE = 'CODE TABLE'\npybufrkit/constants.py\nDEFAULT_TABLES_DIR = os.path.join(BASE_DIR, 'tables')\npybufrkit/descriptors.py\nclass SkippedLocalDescriptor(Descriptor):\n    \"\"\"\n    The skipped local descriptor is a placeholder for any descriptors followed by\n    operator descriptor 206YYY.\n    \"\"\"\n\n    def __init__(self, id_, nbits):\n        # TODO: If a local descriptor does not exist in tables, it will be\n        # created as an undefined descriptor. So how it should be converted\n        # later to SkippedLocalDescriptor if it has a preceding 206YYY? During\n        # template lint?\n        super(SkippedLocalDescriptor, self).__init__(id_)\n        self.nbits = nbits\n        # Dummy unit so it does not fail on unit checking in other functions\n        self.unit = 'SKIPPED'\n\n    def __str__(self):\n        return 'S{:05d}'.format(self.id)\npybufrkit/descriptors.py\nclass OperatorDescriptor(Descriptor):\n    \"\"\"\n    Operator Descriptor 2XXYYY\n    \"\"\"\n\n    def __init__(self, id_):\n        super(OperatorDescriptor, self).__init__(id_)\n\n    @property\n    def operator_code(self):\n        return self.id // 1000\n\n    @property\n    def operand_value(self):\n        return self.id % 1000\npybufrkit/descriptors.py\nclass DelayedReplicationDescriptor(ReplicationDescriptor):\n    \"\"\"\n    Delayed replication Descriptor 1XX000\n    \"\"\"\n\n    def __init__(self, id_, members=None, factor=None):\n        super(DelayedReplicationDescriptor, self).__init__(id_, members)\n        self.factor = factor\n\n    @property\n    def n_repeats(self):\n        raise PyBufrKitError('Cannot access n_repeats for Delayed Replication')\n\n    def accept(self, visitor):\n        visitor.visit_replication_descriptor(self)\n        visitor.visit_replication_factor(self.factor)\n        if not self.members:\n            visitor.visit_replication_descriptor_end(self)\n            return\n        for member in self.members:\n            member.accept(visitor)\n        visitor.visit_replication_descriptor_end(self)\npybufrkit/bufr.py\nclass SectionConfigurer(object):\n    \"\"\"\n    This class is responsible for loading the section config JSON files. It also\n    initialise and configure a requested Section.\n    \"\"\"\n\n    def __init__(self, definitions_dir=None):\n        definitions_dir = definitions_dir or os.path.join(BASE_DIR, 'definitions')\n        fnames = [fname for fname in os.listdir(definitions_dir)\n                  if fname.startswith('section') and fname.endswith('.json')]\n\n        log.debug(\"Reading definition json files from {}\".format(definitions_dir))\n\n        self.configurations = {}\n        for fname in fnames:\n            with open(os.path.join(definitions_dir, fname)) as ins:\n                index, edition = self.get_section_index_and_edition(fname)\n                data = json.load(ins)\n                if index not in self.configurations:\n                    self.configurations[index] = {}\n                config = self.configurations[index]\n                edition_key = DEFAULT_SECTION_EDITION if edition is None else edition\n                config[edition_key] = data\n                # If this config is default, also add it for the default key\n                if data.get('default', False) and edition_key != DEFAULT_SECTION_EDITION:\n                    config[DEFAULT_SECTION_EDITION] = data\n\n    def configure_section(self, bufr_message, section_index, configuration_transformers=()):\n        \"\"\"\n        Initialise and Configure a section for the give section index and\n        version.\n\n        :param BufrMessage bufr_message: The Bufr Message object to configure\n        :param int section_index: (Zero-based) index of the section\n        :param collection configuration_transformers: A collection of configuration\n            transformation functions. These functions make it possible to use\n            the same set of JSON files while still dynamically providing\n            different coder behaviours.\n        :return: The configured section or ``None`` if not present\n        \"\"\"\n\n        config = self.get_configuration(bufr_message, section_index)\n\n        for configuration_transformer in configuration_transformers:\n            config = configuration_transformer(config)\n\n        section = BufrSection()\n        section.set_metadata('index', config['index'])\n        section.set_metadata('description', config.get('description', ''))\n        section.set_metadata('optional', config.get('optional', False))\n        section.set_metadata('end_of_message', config.get('end_of_message', False))\n\n        for parameter in config['parameters']:\n            data_type = parameter['type']\n            nbits = parameter['nbits']\n            if data_type == 'bytes':\n                assert nbits % NBITS_PER_BYTE == 0, \\\n                    'nbits for bytes type must be integer multiple of 8: {}'.format(nbits)\n            section_parameter = SectionParameter(\n                parameter['name'],\n                nbits,\n                data_type,\n                parameter.get('expected', None),\n                parameter.get('as_property', False)\n            )\n            section.add_parameter(section_parameter)\n\n        # Check whether this section is optional. If it is optional, check whether it exists in the\n        # current message.\n        is_section_presents = (\n            not section.optional or\n            getattr(bufr_message, 'is_section{}_presents'.format(section_index)).value\n        )\n\n        # If the section exists, add the section for process other wise ignore it\n        if is_section_presents:\n            bufr_message.add_section(section)\n            return section\n        else:\n            log.info(\"Section {} is not present\".format(section_index))\n            return None\n\n    def get_configuration(self, bufr_message, section_index):\n        # When the bufr_message is first created, it has no edition information.\n        # So the default edition is used.\n        if bufr_message.edition is not None:\n            section_edition = bufr_message.edition.value or DEFAULT_SECTION_EDITION\n        else:\n            section_edition = DEFAULT_SECTION_EDITION\n\n        log.info(\"Configure Section {} of edition {}\".format(\n            section_index, section_edition if section_edition != DEFAULT_SECTION_EDITION else 'default'))\n\n        section_configs = self.configurations[section_index]\n\n        # Get the default config for the section if specific edition is not found\n        return section_configs.get(section_edition, section_configs[DEFAULT_SECTION_EDITION])\n\n    def configure_section_with_values(self, bufr_message, section_index, values, overrides=None):\n        \"\"\"\n        Initialise and Configure a section for the give section index and version\n        and also populate the value of each section parameter with the given list\n        of values. Used by the encoder.\n\n        :param BufrMessage bufr_message: The BUFR message object to configure\n        :param int section_index: The zero-based section index\n        :param list values: A list of values for the parameters.\n        :return: The configured section or ``None`` if the section is not present\n        \"\"\"\n        section = self.configure_section(bufr_message, section_index)\n        if section is not None:\n            assert len(section) == len(values), \\\n                'Number of Section parameters ({}) not equal to number of values to be encoded ({})'.format(\n                    len(section), len(values))\n            for idx, parameter in enumerate(section):\n                parameter.value = values[idx] if overrides is None else overrides.get(parameter.name, values[idx])\n\n        return section\n\n    @staticmethod\n    def get_section_index_and_edition(fname):\n        \"\"\"\n        Get Section Index and version from file name of a configuration file.\n\n        :param str fname: The base file name\n        :return: The index and edition numbers.\n        \"\"\"\n        expr = os.path.splitext(fname)[0][7:]\n        if '-' in expr:\n            fields = expr.split('-')\n            return int(fields[0]), int(fields[1])\n        else:\n            return int(expr), None\n\n    @staticmethod\n    def info_configuration(config):\n        \"\"\"\n        This is a configuration transformation function to make the decoder work\n        only for the part of message before the template data.\n\n        :param dict config: The config JSON object loaded from a configuration file.\n        \"\"\"\n        parameter_types = [parameter['type'] for parameter in config['parameters']]\n        if PARAMETER_TYPE_TEMPLATE_DATA in parameter_types:\n            new_config = deepcopy(config)\n            new_config['end_of_message'] = True\n            new_config['parameters'] = config['parameters'][:parameter_types.index(PARAMETER_TYPE_TEMPLATE_DATA)]\n            return new_config\n        else:\n            return config\n\n    @staticmethod\n    def ignore_value_expectation(config):\n        \"\"\"\n        Remove any expectation value check.\n\n        :param dict config: The config JSON object loaded from a configuration file.\n        \"\"\"\n        new_config = deepcopy(config)\n        for parameter in new_config['parameters']:\n            parameter['expected'] = None\n        return new_config\npybufrkit/constants.py\nUNITS_STRING = 'CCITT IA5'\npybufrkit/errors.py\nclass PyBufrKitError(Exception):\n    \"\"\"\n    This is the root exception object of the package.\n    \"\"\"\n\n    def __init__(self, message=''):\n        self.message = message\n\n    def __str__(self):\n        return 'Error: {}'.format(self.message)\npybufrkit/constants.py\nUNITS_FLAG_TABLE = 'FLAG TABLE'\npybufrkit/descriptors.py\nclass SequenceDescriptor(Descriptor):\n    \"\"\"\n    Sequence Descriptor 3XXYYY\n    \"\"\"\n\n    def __init__(self, id_, name, members=None):\n        super(SequenceDescriptor, self).__init__(id_)\n        self.members = members\n        self.name = name\n\n    def __iter__(self):\n        return iter(self.members)\n\n    def __getitem__(self, item):\n        return self.members.__getitem__(item)\n\n    def accept(self, visitor):\n        visitor.visit_sequence_descriptor(self)\n        if not self.members:\n            return\n        for member in self.members:\n            member.accept(visitor)\n        visitor.visit_sequence_descriptor_end(self)\npybufrkit/errors.py\nclass UnknownDescriptor(PyBufrKitError):\n    \"\"\"\n    An unknown, i.e. not found in BUFR tables, BUFR descriptor.\n    \"\"\"\npybufrkit/descriptors.py\nclass MarkerDescriptor(ElementDescriptor):\n    \"\"\"\n    A marker descriptor is useful in the case when marker operator descriptors\n    are used to signify a statistical value of an element descriptor. For an\n    example, 224255 and 225255.\n    \"\"\"\n\n    def __str__(self):\n        return '{}{:05d}'.format(\n            marker_descriptor_prefix.get(self.marker_id, 'M'),\n            self.id\n        )\n\n    @staticmethod\n    def from_element_descriptor(ed, marker_id,\n                                scale=None, refval=None, nbits=None):\n        \"\"\"\n        Create from a given element descriptor with the option to override its\n        scale, refval and nbits.\n\n        :param ElementDescriptor ed: The element descriptor\n        :param int marker_id: The marker operator ID\n        :param int scale: Overridden value for scale.\n        :param int refval: Overridden value for reference.\n        :param int nbits: Overridden value for number of bits.\n        :rtype: MarkerDescriptor\n        \"\"\"\n        md = MarkerDescriptor(\n            ed.id, ed.name, ed.unit,\n            ed.scale if scale is None else scale,\n            ed.refval if refval is None else refval,\n            ed.nbits if nbits is None else nbits,\n            ed.crex_unit, ed.crex_scale, ed.crex_nchars\n        )\n        md.marker_id = marker_id\n        return md\npybufrkit/descriptors.py\nclass AssociatedDescriptor(Descriptor):\n    \"\"\"\n    Associated field for element descriptor\n\n    :param int nbits: Number of bits used by this descriptor.\n    \"\"\"\n\n    def __init__(self, id_, nbits):\n        super(AssociatedDescriptor, self).__init__(id_)\n        self.nbits = nbits\n        # Dummy unit so it does not fail on unit checking in other functions\n        self.unit = 'ASSOCIATED'\n\n    def __str__(self):\n        return 'A{:05d}'.format(self.id)\n", "answers": ["            elif member_type is FixedReplicationDescriptor:"], "length": 1868, "dataset": "repobench-p_e", "language": "python", "all_classes": null, "_id": "5342b06bc9ef391b79807b10296251b1b02924ec256969ac"}
{"input": "import java.io.IOException;\nimport java.security.InvalidAlgorithmParameterException;\nimport java.security.KeyPair;\nimport java.security.NoSuchAlgorithmException;\nimport java.security.PrivateKey;\nimport java.security.PublicKey;\nimport java.security.spec.InvalidKeySpecException;\nimport java.util.UUID;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.TimeUnit;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.google.common.base.Optional;\nimport com.google.common.base.Preconditions;\nimport com.google.common.base.Stopwatch;\nimport com.kryptnostic.api.v1.KryptnosticConnection;\nimport com.kryptnostic.api.v1.KryptnosticCryptoManager;\nimport com.kryptnostic.api.v1.client.DefaultKryptnosticClient;\nimport com.kryptnostic.api.v1.client.DefaultKryptnosticCryptoManager;\nimport com.kryptnostic.api.v1.client.KryptnosticRestAdapter;\nimport com.kryptnostic.api.v1.security.loaders.rsa.FreshRsaKeyLoader;\nimport com.kryptnostic.api.v1.security.loaders.rsa.LocalRsaKeyLoader;\nimport com.kryptnostic.api.v1.security.loaders.rsa.NetworkRsaKeyLoader;\nimport com.kryptnostic.directory.v1.http.UserDirectoryApi;\nimport com.kryptnostic.kodex.v1.authentication.CredentialFactory;\nimport com.kryptnostic.kodex.v1.client.KryptnosticClient;\nimport com.kryptnostic.kodex.v1.crypto.ciphers.AesCryptoService;\nimport com.kryptnostic.kodex.v1.crypto.ciphers.BlockCiphertext;\nimport com.kryptnostic.kodex.v1.crypto.ciphers.CryptoService;\nimport com.kryptnostic.kodex.v1.crypto.ciphers.Cypher;\nimport com.kryptnostic.kodex.v1.crypto.ciphers.PasswordCryptoService;\nimport com.kryptnostic.kodex.v1.exceptions.types.IrisException;\nimport com.kryptnostic.kodex.v1.exceptions.types.KodexException;\nimport com.kryptnostic.kodex.v1.exceptions.types.ResourceNotFoundException;\nimport com.kryptnostic.kodex.v1.exceptions.types.SecurityConfigurationException;\nimport com.kryptnostic.kodex.v1.marshalling.DeflatingJacksonMarshaller;\nimport com.kryptnostic.kodex.v1.serialization.jackson.KodexObjectMapperFactory;\nimport com.kryptnostic.kodex.v1.storage.DataStore;\nimport com.kryptnostic.krypto.engine.KryptnosticEngine;\nimport com.kryptnostic.v2.constants.Names;\nimport com.kryptnostic.v2.crypto.CryptoServiceLoader;\nimport com.kryptnostic.v2.crypto.KryptnosticCryptoServiceLoader;\nimport com.kryptnostic.v2.search.SearchApi;\nimport com.kryptnostic.v2.sharing.api.SharingApi;\nimport com.kryptnostic.v2.storage.api.KeyStorageApi;\nimport com.kryptnostic.v2.storage.api.ObjectListingApi;\nimport com.kryptnostic.v2.storage.api.ObjectStorageApi;\nimport com.kryptnostic.v2.storage.api.TypesApi;\nimport retrofit.RestAdapter;\nimport retrofit.client.Client;\n            Optional<byte[]> maybePrivateKeyBytes = Optional.fromNullable( dataStore\n                    .get( Names.FHE_PRIVATE_KEY ) );\n            Optional<byte[]> maybeSearchPrivateKeyBytes = Optional.fromNullable( dataStore\n                    .get( Names.FHE_SEARCH_PRIVATE_KEY ) );\n            Optional<byte[]> maybeClientHashFunction = Optional.fromNullable( dataStore\n                    .get( Names.CLIENT_HASH_FUNCTION ) );\n            boolean privateKeyPresent = maybePrivateKeyBytes.isPresent();\n            boolean searchPrivateKeyPresent = maybeSearchPrivateKeyBytes.isPresent();\n            boolean clientHashPresent = maybeClientHashFunction.isPresent();\n            if ( !privateKeyPresent || !searchPrivateKeyPresent || !clientHashPresent ) {\n                // If some keys are absent locally let's try and pull from the network.\n                throw new IOException( \"Unable to load kryptnostic engine keys.\" );\n            }\n            privateKey = privateKeyCryptoService.decryptBytes( mapper.readValue( maybePrivateKeyBytes.get(),\n                    BlockCiphertext.class ) );\n            searchPrivateKey = privateKeyCryptoService.decryptBytes( mapper.readValue( maybeSearchPrivateKeyBytes\n                    .get(),\n                    BlockCiphertext.class ) );\n            engine.initClient( privateKey, searchPrivateKey );\n            holder.clientHashFunction = maybeClientHashFunction.get();\n            return holder;\n        } catch ( SecurityConfigurationException | IOException e ) {\n            try {\n                Optional<BlockCiphertext> maybeEncryptedPrivateKey = keyStorageApi\n                        .getFHEPrivateKeyForCurrentUser();\n                Optional<BlockCiphertext> maybeEncryptedSearchPrivateKey = keyStorageApi\n                        .getFHESearchPrivateKeyForUser();\n                byte[] maybeClientHashFunction = keyStorageApi.getHashFunctionForCurrentUser();\n                // TODO: Check that the length matches the expected length for the client hash function.\n                if ( maybeEncryptedPrivateKey.isPresent() && maybeEncryptedSearchPrivateKey.isPresent()\n                        && ( maybeClientHashFunction.length > 0 ) ) {\n                    encryptedPrivateKey = maybeEncryptedPrivateKey.get();\n                    encryptedSearchPrivateKey = maybeEncryptedSearchPrivateKey.get();\n                    privateKey = privateKeyCryptoService.decryptBytes( encryptedPrivateKey );\n                    searchPrivateKey = privateKeyCryptoService.decryptBytes( encryptedSearchPrivateKey );\n\n                    engine.initClient( privateKey, searchPrivateKey );\n                    holder.clientHashFunction = maybeClientHashFunction;\n                } else {\n                    throw new SecurityConfigurationException( \"Unable to load FHE keys from server.\", e );\n                }\n            } catch ( SecurityConfigurationException e1 ) {\n                // If have a problem retrieving data from the serve or decrypting keys, we regenerate.\n                engine.initClient();\n                privateKey = Preconditions.checkNotNull( engine.getPrivateKey(),\n                        \"Private key from engine cannot be null.\" );\n                searchPrivateKey = Preconditions.checkNotNull( engine.getSearchPrivateKey(),\n                        \"Search private key cannot be null.\" );\n                holder.clientHashFunction = engine.getClientHashFunction();\n                /*\n                 * Need to flush to network since we just generated.\n                 */\n                try {\n                    encryptedPrivateKey = privateKeyCryptoService.encrypt( privateKey );\n                    encryptedSearchPrivateKey = privateKeyCryptoService.encrypt( searchPrivateKey );\n                    keyStorageApi.setHashFunctionForCurrentUser( holder.clientHashFunction );\n                    keyStorageApi.setFHEPrivateKeyForCurrentUser( encryptedPrivateKey );\n                    keyStorageApi.setFHESearchPrivateKeyForCurrentUser( encryptedSearchPrivateKey );\n                } catch ( SecurityConfigurationException e2 ) {\n                    throw new IrisException( e2 );\n                }\n            }\n\n            /*\n             * If we got here then keys came from network or were freshly created and need to be flushed to disk.\n             */\n            try {\n                dataStore.put( Names.FHE_PRIVATE_KEY,\n                        mapper.writeValueAsBytes( encryptedPrivateKey ) );\n                dataStore.put( Names.FHE_SEARCH_PRIVATE_KEY,\n                        mapper.writeValueAsBytes( encryptedSearchPrivateKey ) );\n                dataStore.put( Names.CLIENT_HASH_FUNCTION,\n                        mapper.writeValueAsBytes( holder.clientHashFunction ) );\n\n            } catch ( IOException e1 ) {\n                logger.error( \"Unable to configure FHE keys.\", e1 );\n                throw new Error( \"Sad times.Freeze? I'm a robot. I'm not a refrigerator. \", e1 );\n            }\n\n        }\n\n        return holder;\n    }\n\n    @Override\n    public byte[] getClientHashFunction() {\n        return clientHashFunction;\n    }\n\n    @Override\n    public ObjectStorageApi getObject